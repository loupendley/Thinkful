{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2714,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "url = 'https://www.thinkful.com'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsupervised Learning Capstone: Build you own NLP model  \n",
    "\n",
    "For this challenge, you will need to choose a corpus of data from nltk or another source that includes categories you can predict and create an analysis pipeline that includes the following steps:\n",
    "\n",
    "For this project you'll dig into a large amount of text and apply most of what you've covered in this unit and in the course so far.\n",
    "\n",
    "1.  First, pick a set of texts. This can be either a series of novels, chapters, or articles. Anything you'd like. It just has to have multiple entries of varying characteristics. At least 100 should be good. There should also be at least 10 different authors, but try to keep the texts related (either all on the same topic of from the same branch of literature - something to make classification a bit more difficult than obviously different subjects).\n",
    "\n",
    "This capstone can be an extension of your NLP challenge if you wish to use the same corpus. If you found problems with that data set that limited your analysis, however, it may be worth using what you learned to choose a new corpus. Reserve 25% of your corpus as a test set.\n",
    "\n",
    "The first technique is to create a series of clusters. Try several techniques and pick the one you think best represents your data. Make sure there is a narrative and reasoning around why you have chosen the given clusters. Are authors consistently grouped into the same cluster?\n",
    "\n",
    "2.  Next, perform some unsupervised feature generation and selection using the techniques covered in this unit and elsewhere in the course. Using those features then build models to attempt to classify your texts by author. Try different permutations of unsupervised and supervised techniques to see which combinations have the best performance.\n",
    "\n",
    "3.  Lastly return to your holdout group. Does your clustering on those members perform as you'd expect? Have your clusters remained stable or changed dramatically? What about your model? Is it's performance consistent?\n",
    "\n",
    "If there is a divergence in the relative stability of your model and your clusters, delve into why.\n",
    "\n",
    "Your end result should be a write up of how clustering and modeling compare for classifying your texts. What are the advantages of each? Why would you want to use one over the other? Approximately 3-5 pages is a good length for your write up, and remember to include visuals to help tell your story!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My authors:\n",
    "\n",
    "1. John Buchan\n",
    "    2. The Thirty-nine Steps (10 chapters) URL: http://www.gutenberg.org/cache/epub/558/pg558.txt\n",
    "    2. Mr. Standfast (22 chapters) URL: https://www.gutenberg.org/ebooks/560.txt.utf-8\n",
    "    \n",
    "1. Edgar Rice Burroughs\n",
    "   2. Tarzan of the Apes (28 chapters) URL: https://www.gutenberg.org/ebooks/78.txt.utf-8\n",
    "   2. Son of Tarzan (27 chapters) URL: https://www.gutenberg.org/ebooks/90.txt.utf-8\n",
    "   \n",
    "1. Alexander Dumas\n",
    "   2. The Count of Monte Cristo (117 chapters) URL: https://www.gutenberg.org/files/1184/1184-0.txt\n",
    "   2. The Three Muskateers (66 chapters) URL: https://www.gutenberg.org/files/1257/1257-0.txt\n",
    "   \n",
    "1. H. Rider Haggard\n",
    "   2. King Solomon's Mines (20 chapters) URL: https://www.gutenberg.org/ebooks/2166.txt.utf-8\n",
    "   2. Finished (23 chapters) https://www.gutenberg.org/ebooks/1724.txt.utf-8\n",
    "\n",
    "1. Anthony Hope\n",
    "   2. The Prisoner of Zennda (22 chapters) URL: https://www.gutenberg.org/files/95/95-0.txt\n",
    "   2. Rupert of Hentzau (21 chapters) URL: https://www.gutenberg.org/files/1145/1145-0.txt\n",
    "   \n",
    "1. Rudyard Kipling\n",
    "   2. Kim (15 chapters) URL: https://www.gutenberg.org/ebooks/2226.txt.utf-8\n",
    "   2. Captains Courageous (10 chapters) URL: https://www.gutenberg.org/ebooks/2225.txt.utf-8\n",
    "   \n",
    "1. Talbot Mundy\n",
    "   2. King of the Kyber Rifles (28 chapters) URL: https://www.gutenberg.org/files/6066/6066-0.txt\n",
    "   2. Lion of Petra (13 chapters) URL: https://www.gutenberg.org/ebooks/19307.txt.utf-8\n",
    "   \n",
    "1. Baroness Orczy\n",
    "   2. The Scarlet Pimpernel (31 chapters) URL: https://www.gutenberg.org/ebooks/60.txt.utf-8\n",
    "   2. The Elusive Pimpernel (35 chapters) URL: https://www.gutenberg.org/files/2785/2785-0.txt\n",
    "   \n",
    "1. Jules Verne\n",
    "   2. Around the World in Eighty Days (37 chapters) URL: https://www.gutenberg.org/ebooks/103.txt.utf-8\n",
    "   2. Five Weeks in a Balloon (44 chapters) URL: https://www.gutenberg.org/files/3526/3526-0.txt\n",
    "   \n",
    "1. Edgar Wallace\n",
    "   2. Bones in London (12 chapters) URL: https://www.gutenberg.org/ebooks/27525.txt.utf-8\n",
    "   2. The Book of All-Power (19 chapters) URL: https://www.gutenberg.org/ebooks/24920.txt.utf-8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions that need answering:\n",
    "\n",
    " 1. What question are you trying to solve (or prove wrong) ?   \n",
    " __No questions to answer for this challenge.__\n",
    " 1. What kind of data do you have? -> describe the source.. \n",
    " __Sentiment analaysis, using bag of words, tfidf, Random Forest, Bernoulli Naive Bayes, and Multinomial Naive Bayes.__\n",
    " 1. Do some EDA, plots\n",
    " 1. What's missing from the data and how do you deal with it?  \n",
    " __No missing data here.__\n",
    " 1. How can you add, change, or remove features to get more out of your data?  \n",
    " __No added features here; it's sentiment analysis.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Report Template\n",
    "\n",
    "#### \n",
    "\n",
    "\n",
    "#### Key Learning. \n",
    "You cannot always improve on a model for even 5 percentage points.  Especially when using bag of words which has no configurable parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2715,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Constants\n",
    "max_iterations         = 10            # set it to > 0 for determining the features inportance\n",
    "random_state           = 57\n",
    "rows_in_training_set   = 10000\n",
    "rows_in_test_set       = 200000\n",
    "test_size              = 0.10\n",
    "train_size             = 0.90\n",
    "rfc_test_size          = 50000\n",
    "rfc_train_size         = 5000\n",
    "sample_size            = 10\n",
    "run_CountVectorizer    = False\n",
    "run_TfidfVectorizer    = True\n",
    "BegTimeStampNewlines   = 3\n",
    "EndTimeStampNewlines   = 3\n",
    "EndTimeStamp           = '\\n'*EndTimeStampNewlines+'End'\n",
    "BegTimeStamp           = 'End'+'\\n'*BegTimeStampNewlines\n",
    "SustainerSTDDEVLimit   = 0.020\n",
    "\n",
    "num_clusters = 12\n",
    "target_column = 'yyyy'\n",
    "xcolumnname = 'xxxx'\n",
    "CrossValidations = 5 # We are using 5 cross validations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2716,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Controls for running sentiment_analyzer\n",
    "flag_to_run_rf = False\n",
    "flag_to_plot_them = False\n",
    "flag_to_run_correlation_matrix = False\n",
    "flag_to_run_features_importance = False\n",
    "flag_to_run_gradient_boosting  = False\n",
    "flag_to_run_linear_regression  = False\n",
    "flag_to_run_logistic_regression = False\n",
    "flag_to_run_lasso_regression = False\n",
    "flag_to_run_ridge_regression = False\n",
    "flag_to_run_svc = False\n",
    "flag_to_run_vectorizer_nb = False\n",
    "flag_to_run_sentiment_analyzer = False\n",
    "flag_to_run_affinity_propagation = False\n",
    "flag_to_run_kmeans = False\n",
    "flag_to_run_mean_shift = False\n",
    "flag_to_run_spectral_clustering = False\n",
    "flag_to_run_elbow_plot = False\n",
    "\n",
    "debug = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2717,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": false
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to /Users/lou/nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import chardet\n",
    "import datetime\n",
    "from sklearn import datasets, ensemble, metrics, linear_model\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "import time, sys\n",
    "import seaborn as sns\n",
    "from sklearn.svm import SVC\n",
    "import statsmodels.api as sm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import cross_val_predict, cross_val_score, GridSearchCV,cross_val_score, train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import MinMaxScaler, normalize\n",
    "from IPython.display import Markdown, display\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import cluster, metrics\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import pairwise_distances, mean_squared_error\n",
    "from sklearn.cluster import AffinityPropagation, KMeans, MeanShift, estimate_bandwidth, SpectralClustering\n",
    "from scipy.spatial.distance import cdist\n",
    "import spacy\n",
    "import re\n",
    "from nltk.corpus import gutenberg, stopwords\n",
    "import nltk\n",
    "from nltk.corpus.reader.plaintext import PlaintextCorpusReader\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.cluster import KMeansClusterer\n",
    "from collections import Counter\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "import random\n",
    "import string\n",
    "import unicodedata\n",
    "import codecs\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import platform\n",
    "\n",
    "\n",
    "nltk.download('gutenberg') # Load the gutenberg nltk works\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2718,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regression = False\n"
     ]
    }
   ],
   "source": [
    "# add this to a dictionary\n",
    "# Constants\n",
    "max_iterations         = 10            # set it to > 0 for determining the features inportance\n",
    "random_state           = 57\n",
    "test_size              = 0.10\n",
    "train_size             = 0.90\n",
    "\n",
    "begin_string = '\\n'*3+'Begin'\n",
    "end_string = 'End'+'\\n'*3\n",
    "\n",
    "# Regression/Classification control\n",
    "Regression = False \n",
    "\n",
    "print(\"Regression = {}\".format(Regression))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2719,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Display preferences.\n",
    "%matplotlib inline\n",
    "pd.options.display.float_format = '{:.3f}'.format\n",
    "pd.set_option('display.max_rows', 1000)\n",
    "pd.set_option('display.max_row', 1000)\n",
    "pd.set_option('display.max_columns', 600)\n",
    "pd.set_option('display.width', 4000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2720,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def plot_time_to_complete():\n",
    "    objects = ('BernoulliNB', 'MultinomialNB', 'Logistic Regression')\n",
    "    y_pos = np.arange(len(objects))\n",
    "    performance = [18,17,32]\n",
    "\n",
    "    plt.bar(y_pos, performance, align='center', alpha=0.5)\n",
    "    plt.xticks(y_pos, objects)\n",
    "    plt.ylabel('Time in Minutes')\n",
    "    plt.title('Yelp Sentiment Analysis Time to Complete')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2721,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def file_stuff():\n",
    "    # Use this for stand-alone file\n",
    "    \n",
    "    path = \"../../../../\"\n",
    "    filename = \"Datafiles/bostonmarathon/results/2013/results.csv\"\n",
    "    print(\"fullfilename = {}\".format(path+filename))\n",
    "    df = pd.read_csv(path+filename)\n",
    "    print(\"There are {} rows in this file.\".format(df.shape[0]))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2722,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def text_cleaner(text):\n",
    "    '''\n",
    "    # Utility function for standard text cleaning.\n",
    "    '''\n",
    "    # Visual inspection identifies a form of punctuation spaCy does not\n",
    "    # recognize: the double dash '--'.  Better get rid of it now!\n",
    "    \n",
    "    text = re.sub(r'--',' ',text)\n",
    "    text = re.sub(\"[\\[].*?[\\]]\", \"\", text)\n",
    "    text = ' '.join(text.split())\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2723,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def load_clean_parse_group_gutenberg(gutenberg_file, author, percent_of_file): \n",
    "    \"\"\"\n",
    "    # currently, this function handles:\n",
    "    #    Persuasion, Austen\n",
    "    #    Alice In Wonderland Austen\n",
    "    #    Paradise Lost Milton\n",
    "    #    Moby Dick Melville\n",
    "    \"\"\"\n",
    "\n",
    "    # Load and clean the data.\n",
    "    if gutenberg_file == \"persuasion\":\n",
    "        file_to_load = gutenberg.raw('austen-persuasion.txt')\n",
    "        book = re.sub(r'Chapter \\d+', '', persuasion)\n",
    "    elif gutenberg_file == \"alice\":\n",
    "        file_to_load = gutenberg.raw('carroll-alice.txt')\n",
    "        book = re.sub(r'CHAPTER .*', '', file_to_load)\n",
    "    elif gutenberg_file == \"paradise\":\n",
    "        file_to_load = gutenberg.raw('milton-paradise.txt')\n",
    "        book = re.sub(r'BOOK .*', '', file_to_load)\n",
    "    elif gutenberg_file == \"moby\":\n",
    "        file_to_load = gutenberg.raw('melville-moby_dick.txt')\n",
    "        book = re.sub(r'BOOK .*', '', file_to_load)\n",
    "\n",
    "    book = text_cleaner(book[:int(len(book)/percent_of_file)])\n",
    "    \n",
    "    nlp = spacy.load('en')\n",
    "    book_doc = nlp(book)\n",
    "    \n",
    "    print(\"book_doc datatype is {}\".format(type(book_doc)))\n",
    "    \n",
    "#     book_sents = pd.DataFrame() # I just added this\n",
    "#     book_sents = [[sent, author] for sent in book_doc.sents]\n",
    "    return pd.DataFrame([[sent, author] for sent in book_doc.sents]), book_doc\n",
    "    \n",
    "#     return book_sents, book_doc # previously, I had return pd.DataFrame(book_sents), book_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2724,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def parse_gutenberg(book):\n",
    "    '''\n",
    "    # Parse the cleaned novels\n",
    "    '''\n",
    "    \n",
    "    nlp = spacy.load('en')\n",
    "#     book_doc = nlp(book)\n",
    "\n",
    "    return nlp(book)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2725,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def group_gutenberg(book_doc, author):\n",
    "    '''\n",
    "    # Group into sentences.\n",
    "    '''\n",
    "    print(\"book_doc datatype is {}\".format(type(book_doc)))\n",
    "    \n",
    "    book_sents = [[sent, author] for sent in book_doc.sents]\n",
    "\n",
    "    # Combine the sentences from the two novels into one data frame.\n",
    "#     sentences = pd.DataFrame(book_sents)\n",
    "    if debug:\n",
    "        sentences.head()\n",
    "        \n",
    "    return pd.DataFrame(book_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2726,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def dataset_cleanup(df):\n",
    "\n",
    "    # data Cleanup --> last used for the Boston Marathon challenge\n",
    "    \n",
    "#     Left over from the Boston Marathon challenge...\n",
    "  \n",
    "    df['gender_int'] = np.where(df['gender'] == 'M', 1, 0).astype(float)\n",
    "    df['bib_int'] = df['bib'].replace(to_replace=r'[W|F]', value='-', regex=True).astype(int)\n",
    "    kcolumns = ['5k', '10k', '20k', '25k', '30k', '35k', '40k', 'half']\n",
    "    for kcol in kcolumns:\n",
    "        df[kcol] = np.where(df[kcol] == '-', 0, df[kcol])\n",
    "        df[kcol] = df[kcol].astype(float)\n",
    "    df['5kpace']   = df['5k']/5.0\n",
    "    df['10kpace']  = df['10k']/10.0\n",
    "    df['20kpace']  = df['20k']/20.0\n",
    "    df['halfpace'] = df['half']/21.095\n",
    "    df['25kpace']  = df['25k']/25.0\n",
    "    df['30kpace']  = df['30k']/30.0\n",
    "    df['35kpace']  = df['35k']/35.0\n",
    "    df['40kpace']  = df['40k']/40.0\n",
    "    df['officialpace'] = df['official']/42.19\n",
    "    # df['raceavg'] = ,axis=0).mean()\n",
    "    df['racestd'] = df[['5kpace','10kpace','20kpace','halfpace','25kpace','30kpace','35kpace','40kpace','officialpace']].std(axis=1)\n",
    "    df['raceavg'] = df[['5kpace','10kpace','20kpace','halfpace','25kpace','30kpace','35kpace','40kpace','officialpace']].mean(axis=1)\n",
    "#     X = df[['age', 'gender_int','genderdiv', 'country', 'official','racestd','raceavg']]\n",
    "#     X = pd.get_dummies(X)\n",
    "\n",
    "    df.drop('ctz', axis=1, inplace=True)\n",
    "    df.drop('state',axis=1, inplace=True)\n",
    "    # these are the 2% sustainers.  They can be running at any pace, but they are consistent!\n",
    "    df['sustainer'] = np.where(df['racestd'] <= SustainerSTDDEVLimit, 1, 0).astype(float) # they sustained their pace very well for the race\n",
    "    scaler = MinMaxScaler()\n",
    "    \n",
    "    scaler.fit(df[['age']])\n",
    "    df['age_scaled'] = scaler.transform(df[['age']]).astype(float)\n",
    "    \n",
    "    scaler.fit(df[['overall']])\n",
    "    df['overall_scaled'] = scaler.transform(df[['overall']]).astype(float)\n",
    "    \n",
    "    scaler.fit(df[['pace']])\n",
    "    df['pace_scaled'] = scaler.transform(df[['pace']])\n",
    "    \n",
    "    scaler.fit(df[['official']])\n",
    "    df['official_scaled'] = scaler.transform(df[['official']]).astype(float)\n",
    "    \n",
    "    display('columns are now', df.columns)\n",
    "#     df = fcn_MinMaxScaler(df, 'age', 'age_scaled')\n",
    "#     df = fcn_MinMaxScaler(df, 'official', 'official_scaled')\n",
    "    X = df[['age_scaled', 'sustainer', 'gender_int', 'racestd']]\n",
    "#     X = pd.get_dummies(X)\n",
    "   \n",
    "    \n",
    "    display(\"df columns cpt 92310: \", df.columns)\n",
    "    \n",
    "    global target_column, xcolumnname, ycolumnname\n",
    "    \n",
    "#     target_column = 'overall_scaled'\n",
    "#     xcolumnname = 'age_scaled'\n",
    "    ycolumnname = target_column\n",
    "    \n",
    "    y = df[target_column]\n",
    "    printFormatted(\"target, y column is {}\".format(target_column))\n",
    "\n",
    "    if debug == True:\n",
    "        print_timestamp(\"X and y variables created\")\n",
    "        \n",
    "    printFormatted('we have cleaned up the dataframe.')\n",
    "    display_column_names('df values', df)\n",
    "    display_column_names('X values', X)\n",
    "    return df, X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2727,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def printFormatted(string):\n",
    "    newline = '\\n'\n",
    "    display(Markdown(string))\n",
    "    write_to_logfile(string+newline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2728,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def fcn_MinMaxScaler(dataframe, orig_column, new_column):\n",
    "    display(\"cp 1: In fcn_MinMaxScaler.  shape is:\", dataframe.shape)\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(dataframe[['{}'.format(orig_column)]])\n",
    "    dataframe[['{}'.format(new_column)]] = scaler.transform(dataframe['{}'.format(orig_column)])\n",
    "    display(\"cp 2: In fcn_MinMaxScaler.  shape is:\", dataframe.shape)\n",
    "    \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2729,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def plot_facet():\n",
    "    g = sns.FacetGrid(data=df, col='stars')\n",
    "    g.map(plt.hist, 'message_length', bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2730,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def write_to_logfile(message, mdformat=''):\n",
    "    bufsize = 0\n",
    "    with open('TestResults.md', 'a+') as the_file:\n",
    "        the_file.write('{} {}'.format(mdformat, message))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2731,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def plot_model_accuracy():\n",
    "    objects = ('BernoulliNB', 'MultinomialNB', 'Logistic Regression')\n",
    "    y_pos = np.arange(len(objects))\n",
    "    performance = [75.81,85.98,91.08]\n",
    "\n",
    "    plt.bar(y_pos, performance, align='center', alpha=0.5)\n",
    "    plt.xticks(y_pos, objects)\n",
    "    plt.ylabel('Accuracy Percent')\n",
    "    plt.title('Yelp Sentiment Analysis Accuracy')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2732,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def print_timestamp(displaytext):    \n",
    "    import sys\n",
    "    import datetime\n",
    "    datetime_now = str(datetime.datetime.now())\n",
    "    printFormatted(\"{:19.19}: In: {} {} \".format(datetime_now, sys._getframe(1).f_code.co_name, displaytext))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2733,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def return_current_datetime():\n",
    "    datetime_now = str(datetime.datetime.now())\n",
    "    return datetime_now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2734,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def data_demographics(dataframe, num_rows):\n",
    "\n",
    "    display(\"dataframe.isnull().sum()\", dataframe.isnull().sum())\n",
    "\n",
    "    display(\"dataframe.columns\\n\", dataframe.columns)\n",
    "    display(\"dataframe.head({})\\n\".format(num_rows), dataframe.head(num_rows))\n",
    "\n",
    "    display(\"dataframe.sample({})\\n\".format(num_rows), dataframe.sample(num_rows))\n",
    "    display(\"dataframe.dtypes\\n\", dataframe.dtypes)\n",
    "    display(\"dataframe.describe()\\n\", dataframe.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2735,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def plot_them():\n",
    "    for column in X_train.columns:\n",
    "#         plt.hist(X_train[column]*100, bins=40)\n",
    "        plt.scatter(y_train, X_train[column]*100)\n",
    "        plt.xlabel(column)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2736,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def rfc_and_feature_importances(rf):    # Here we are using Random Forest classifier method to determine the top 30 features.\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, train_size=train_size)\n",
    "    \n",
    "    ## Fit the model on your training data.\n",
    "    rf.fit(X_train, y_train) \n",
    "    \n",
    "    ## And score it on your testing data.\n",
    "    rf.score(X_test, y_test)\n",
    "\n",
    "    feature_importance = rf.feature_importances_\n",
    "\n",
    "    # Make importances relative to max importance.\n",
    "    feature_importance = 100.0 * (feature_importance / feature_importance.max())\n",
    "    sorted_idx = np.argsort(feature_importance)\n",
    "    cols=X.columns[sorted_idx].tolist() \n",
    "    cols=cols[::-1]\n",
    "    pos = np.arange(sorted_idx.shape[0]) + .5\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.barh(pos, feature_importance[sorted_idx], align='center')\n",
    "    plt.yticks(pos, X.columns[sorted_idx])\n",
    "    plt.xlabel('Relative Importance')\n",
    "    plt.title('Variable Importance')\n",
    "    plt.show()\n",
    "#     print(\"We are returning these columns {}\".format(cols))\n",
    "    return cols[:30] # return it sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2737,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def run_features_importance(rf,n):\n",
    "# Here we will return the feature importances\n",
    "    all_feature_important_columns = []\n",
    " \n",
    "    for i in range(1,n):\n",
    "        print_timestamp('running rfc iteration {} features importance for {} times'.format(i,n))\n",
    "        columns2 = rfc_and_feature_importances(rf)\n",
    "#         columns2.extend('{}'.format(i))\n",
    "        all_feature_important_columns = all_feature_important_columns + columns2\n",
    "    #     print(\"all_feature_import_columns={}\".format(all_feature_important_columns))\n",
    "\n",
    "    print(\"\\nBOD:\\nall_feature_important_columns = {}\\nEOD\".format(sorted(all_feature_important_columns)))\n",
    "    for feature in set(all_feature_important_columns):\n",
    "        print_timestamp(\"the NOC of feature {} in all_feature_important_columns is {}\".format(feature, all_feature_important_columns.count(feature)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2738,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def run_correlation_matrix():\n",
    "    \n",
    "    print_timestamp('Begin'+'\\n'*3)\n",
    "    \n",
    "    # Setup the correlation matrix.\n",
    "    corrmat = X.corr()\n",
    "    print(corrmat)\n",
    "\n",
    "    # Set up the subplots\n",
    "    f, ax = plt.subplots(figsize=(12, 9))\n",
    "\n",
    "    # Let's draw the heatmap using seaborn.\n",
    "    sns.heatmap(corrmat, vmax=.6, square=True)\n",
    "    plt.show()\n",
    "    \n",
    "    print_timestamp('\\n'*3+'End')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2739,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def data_characteristics():\n",
    "    \n",
    "    printFormatted(\"#### Columns used in the dataset\")\n",
    "    display(df.columns)\n",
    "\n",
    "    print(\"\\n\\n\")\n",
    "    printFormatted(\"#### Describe of the df dataset\")\n",
    "    display(df.describe())\n",
    "\n",
    "    print(\"\\n\\n\")\n",
    "    printFormatted(\"#### Sample of 10 from the dataset\")\n",
    "    display(df.sample(sample_size))\n",
    "\n",
    "    print(\"\\n\\n\")\n",
    "    printFormatted(\"#### Number of nulls in X\")\n",
    "    display(X.isnull().sum())\n",
    "    print(\"\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2740,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def training_test_set(X, y):\n",
    "#     global X_train, X_test, y_train, y_test\n",
    "    # Let's fit it with the RFC training set\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, train_size=train_size, random_state=0)\n",
    "    print(\"train_size = {}, X_train is {}, and y_train is {}\".format(train_size, len(X_train), len(y_train)))\n",
    "    print(\"test_size  = {}, X_test  is {}, and y_test is {}\".format(test_size, len(X_test), len(y_test)))\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2741,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def run_rf(X_train=None, X_test=None, y_train=None, y_test=None, params=None, cross_validate=None):\n",
    "    \n",
    "    rfc = ensemble.RandomForestClassifier(n_estimators=1000)\n",
    "    if params != None:\n",
    "        print(\"In run_rf, params = {}\".format(params)) \n",
    "        rfc.set_params(**params)\n",
    "        \n",
    "    ## Fit the model on your training data.\n",
    "    rfc_fit = rfc.fit(X_train, y_train)  \n",
    "    \n",
    "    #   Let's score it with the test data set    this is new 13-Aug-2019\n",
    "    print_training_and_test_scores(rfc_fit, X_train, X_test, y_train, y_test) # new on 13-Aug-2019\n",
    "    \n",
    "#   Let's produce the metrics scores\n",
    "    print_metrics_score(rfc_fit, X_train, X_test, y_train, y_test) # new on 13-Aug-2019\n",
    "    \n",
    "#   Let's run cross validation \n",
    "    if cross_validate == True:\n",
    "        print_cross_validation_scores(rfc_fit, X_train, X_test, y_train, y_test)\n",
    "        \n",
    "#   Let's run the confusion matrix\n",
    "    if confusion_matrix == True:\n",
    "        confusion_matrix_function(rfc_fit, X_train, X_test, y_train, y_test)\n",
    "    \n",
    "#     ## Let's score it with the training data set\n",
    "#     train_score = rfc.score(X_train, y_train)\n",
    "#     printFormatted(\"### Training score = {:.2%}\".format(train_score))\n",
    "\n",
    "#     ## Let's score it with the test data set\n",
    "#     test_score = rfc.score(X_test, y_test)\n",
    "#     printFormatted(\"### Test score = {:.2%}\".format(test_score))\n",
    "    \n",
    "#     metrics_train_score = metrics.accuracy_score(y_train, y_pred_class2)\n",
    "#     metrics_test_score =  metrics.accuracy_score(y_test, y_pred_class)\n",
    "\n",
    "#     printFormatted('###  Metrics train accuracy score = {:.2%} with {}'.format(metrics_train_score, 'Random Forest Classifier'))\n",
    "#     printFormatted('###  Metrics test accuracy score = {:.2%} with {}'.format(metrics_test_score, 'Random Forest Classifier'))\n",
    "    \n",
    "#     if cross_validate == True:\n",
    "#         accuracy = cross_val_score(rfc, X_train, y_train, scoring='accuracy', cv = 5)\n",
    "#         printFormatted(\"### Cross validation scores:  {}\".format(accuracy))\n",
    "#         printFormatted(\"### Accuracy of Model with Cross Validation average is: {:.2%}\".format(accuracy.mean()))\n",
    "        \n",
    "#     #   Let's produce the metrics scores\n",
    "#     print_metrics_score(mnb_fit, X_train, X_test, y_train, y_test) # new on 13-Aug-2019\n",
    "      \n",
    "    print_timestamp('End run_rfr part 1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2742,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def run_BernoulliNB(X_train=None, X_test=None, y_train=None, y_test=None, params=None, cross_validate=None):\n",
    "    \n",
    "    # Our data is binary / boolean, so we're importing the Bernoulli classifier.\n",
    "\n",
    "    # Instantiate our model and store it in a new variable.\n",
    "    bnb = BernoulliNB()\n",
    "\n",
    "    # Fit our model to the data.\n",
    "    bnb_fit = bnb.fit(X_train, y_train)\n",
    "\n",
    "    # Classify, storing the result in a new variable.\n",
    "#     y_pred = bnb.predict(data)\n",
    "#     y_pred = bnb.predict(X_train)\n",
    "\n",
    "    # Display our results.\n",
    "#     print(\"Number of mislabeled points out of a total {} points : {}\".format(\n",
    "#        X_train.shape[0],\n",
    "#         (y_test != y_pred).sum() \n",
    "#     ))\n",
    "    \n",
    "#   Let's score it with the test data set    this is new 13-Aug-2019\n",
    "    print_training_and_test_scores(bnb_fit, X_train, X_test, y_train, y_test) # new on 13-Aug-2019\n",
    "    \n",
    "#   Let's produce the metrics scores\n",
    "    print_metrics_score(bnb_fit, X_train, X_test, y_train, y_test) # new on 13-Aug-2019\n",
    "    \n",
    "#   Let's run cross validation \n",
    "    if cross_validate == True:\n",
    "        print_cross_validation_scores(bnb_fit, X_train, X_test, y_train, y_test)\n",
    "        \n",
    "#   Let's run the confusion matrix\n",
    "    if confusion_matrix == True:\n",
    "        confusion_matrix_function(bnb_fit, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2743,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def sentiment_analyzer(path, parameters, classifier, tfidf_parms, X_train=None, X_test=None, y_train=None, y_test=None, cross_validate=None):\n",
    "    # path A = the old path\n",
    "    # path B = the new path, no CountVectorizer at all\n",
    "    \n",
    "# run block of code and catch warnings\n",
    "  \n",
    "    if debug == True:\n",
    "        print_timestamp(BegTimeStamp+\" running with path={}\".format(path))\n",
    "    \n",
    "    global vectorized\n",
    "    vectorized = True\n",
    "    \n",
    "    pipeline_array = []\n",
    "   \n",
    "    if path == \"A\":\n",
    "        if classifier == 'bnb':\n",
    "            pipeline_array.append(Pipeline([\n",
    "                ('tfidf', TfidfVectorizer(**tfidf_parms)),\n",
    "                ('clf',   BernoulliNB(**parameters))\n",
    "            ]))\n",
    "        elif classifier == 'svc':\n",
    "            pipeline_array.append(Pipeline([\n",
    "                ('tfidf', TfidfVectorizer(**tfidf_parms)),\n",
    "                ('clf',   SVC(kernel = 'linear', **parameters))\n",
    "            ])) \n",
    "        elif classifier == 'mlb':\n",
    "            pipeline_array.append(Pipeline([\n",
    "                ('tfidf', TfidfVectorizer(**tfidf_parms)),\n",
    "                ('clf',   MultinomialNB(**parameters))\n",
    "            ]))\n",
    "        elif classifier == 'logit':\n",
    "            pipeline_array.append(Pipeline([\n",
    "                ('tfidf', TfidfVectorizer(**tfidf_parms)),\n",
    "                ('clf',   LogisticRegression(**parameters))\n",
    "            ]))\n",
    "        elif classifier == 'rfc':\n",
    "            pipeline_array.append(Pipeline([\n",
    "                ('tfidf', TfidfVectorizer(**tfidf_parms)),\n",
    "                ('clf',   ensemble.RandomForestClassifier(**parameters))\n",
    "            ]))  \n",
    "            \n",
    "    elif path == \"B\":\n",
    "        if classifier == 'bnb':\n",
    "            pipeline_array.append(Pipeline([\n",
    "                ('tfidf', TfidfVectorizer(**tfidf_parms)),\n",
    "                ('clf',   BernoulliNB(**parameters))\n",
    "            ]))\n",
    "        elif classifier == 'svc':\n",
    "            pipeline_array.append(Pipeline([\n",
    "                ('tfidf', TfidfVectorizer(**tfidf_parms)),\n",
    "                ('clf',   SVC(kernel = 'linear', **parameters))\n",
    "            ])) \n",
    "        elif classifier == 'mlb':\n",
    "            pipeline_array.append(Pipeline([\n",
    "                ('tfidf', TfidfVectorizer(**tfidf_parms)),\n",
    "                ('clf',   MultinomialNB(**parameters))\n",
    "            ]))\n",
    "        elif classifier == 'logit':\n",
    "            pipeline_array.append(Pipeline([\n",
    "                ('tfidf', TfidfVectorizer()),\n",
    "                ('clf',   LogisticRegression(**parameters))\n",
    "            ]))\n",
    "        elif classifier == 'rfc':\n",
    "            pipeline_array.append(Pipeline([\n",
    "                ('tfidf', TfidfVectorizer(**tfidf_parms)),\n",
    "                ('clf',   ensemble.RandomForestClassifier(**parameters))\n",
    "            ]))\n",
    "\n",
    "    pipe = pipeline_array[0]\n",
    "    \n",
    "    try:\n",
    "        vect_name_list = str(pipe.named_steps['vect']).split('(')\n",
    "        vect_name = \"vect = {}, \".format(vect_name_list[0])\n",
    "    except:\n",
    "        vect_name = ''\n",
    "\n",
    "    classifier_name_list=str(pipe.named_steps['clf']).split('(')\n",
    "    classifier_name=classifier_name_list[0]\n",
    "    tfidf_name_list = str(pipe.named_steps['tfidf']).split('(')\n",
    "    if len(tfidf_name_list) > 0:\n",
    "        tfidf_name = tfidf_name_list[0]\n",
    "    else:\n",
    "        tfidf_name = ''\n",
    "\n",
    "    printFormatted(\"###  Now running with: {} tfidf={} and clf={} {}\\nparameters={} \\n\\n tfidf_parms={}\".format( vect_name,\n",
    "                                                                                                tfidf_name,\n",
    "                                                                                                classifier_name,\n",
    "                                                                                                return_current_datetime(),\n",
    "                                                                                                parameters,\n",
    "                                                                                                tfidf_parms\n",
    "                                                                                                ))\n",
    "    pipefit = pipe.fit(X_train, y_train)\n",
    "\n",
    "#   Let's score it with the test data set    this is new 13-Aug-2019\n",
    "    print_training_and_test_scores(pipefit, X_train, X_test, y_train, y_test) # new on 13-Aug-2019\n",
    "    \n",
    "#   Let's produce the metrics scores\n",
    "    print_metrics_score(pipefit, X_train, X_test, y_train, y_test) # new on 13-Aug-2019\n",
    "    \n",
    "#   Let's run cross validation \n",
    "    if cross_validate == True:\n",
    "        print_cross_validation_scores(pipefit, X_train, X_test, y_train, y_test)\n",
    "        \n",
    "#   Let's run the confusion matrix\n",
    "    if confusion_matrix == True:\n",
    "        confusion_matrix_function(pipefit, X_train, X_test, y_train, y_test)\n",
    "            \n",
    "    if debug == True:\n",
    "        printFormatted(\"Steps information: {}\".format(pipe.steps))\n",
    "        print_timestamp(\"Finished running pipeline with:\\n{}: \".format(classifier_name))\n",
    "        \n",
    "    print_timestamp(EndTimeStamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2744,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def print_training_and_test_scores(model, X_train, X_test, y_train, y_test):\n",
    "    \n",
    "    ## Let's score it with the test data set    this is new 13-Aug-2019\n",
    "    training_score = model.score(X_train, y_train) \n",
    "    printFormatted(\"### Training score = {:.2%}\".format(training_score))\n",
    "    \n",
    "    ## Let's score it with the test data set  this is new 13-Aug-2019\n",
    "    test_score = model.score(X_test, y_test)\n",
    "    printFormatted(\"### Test score = {:.2%}\".format(test_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2745,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def print_metrics_score(fit_model, X_train, X_test, y_train, y_test):\n",
    "    y_pred_class  = fit_model.predict(X_test)\n",
    "    metrics_test_score =  metrics.accuracy_score(y_test, y_pred_class)\n",
    "    printFormatted('###  Metrics test accuracy score = {:.2%}'.format(metrics_test_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2746,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def print_cross_validation_scores(fit_model, X_train, X_test, y_train, y_test):\n",
    "    \n",
    "        accuracy = cross_val_score(fit_model, X_train, y_train, scoring='accuracy', cv = 5)\n",
    "        printFormatted(\"### Cross validation scores:  {}\".format(accuracy))\n",
    "        printFormatted(\"### Accuracy of Model with Cross Validation average is: {:.2%}\".format(accuracy.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2747,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def run_gradient_boosting():\n",
    "\n",
    "    print_timestamp('Begin')\n",
    "    \n",
    "    clf = ensemble.GradientBoostingClassifier(**params)\n",
    "\n",
    "    #Let's run cross validate score with the training data set\n",
    "    cross_val_score(clf, X_train, y_train, cv=5)\n",
    "\n",
    "    loss_function = 'deviance' # could be exponential\n",
    "    depth_value = 8\n",
    "    params = {'n_estimators': 500,\n",
    "              'max_depth': 8,\n",
    "              'loss_function': loss_function,\n",
    "              'max_leaf_nodes': depth_value, # 8 worked best...\n",
    "              'min_samples_leaf': depth_value * 3\n",
    "              ,'random_state' : random_state\n",
    "             }\n",
    "\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    predict_train = clf.predict(X_train)\n",
    "    predict_test = clf.predict(X_test)\n",
    "    \n",
    "    print_timestamp('End')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2748,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def run_svc(X_train=None, X_test=None, y_train=None, y_test=None, cross_validate=None, params=None):\n",
    "\n",
    "    print_timestamp('\\n'*3+'Begin run_svc')\n",
    "    \n",
    "    # Let's do a linear Support Vector Classifier\n",
    "    print_timestamp('Running SVC(kernel=linear')\n",
    "    svm = SVC(kernel = 'linear')\n",
    "    \n",
    "    if params == True:\n",
    "        print(\"In run_rf, params = {}\".format(params)) \n",
    "        svm.set_params(**params)\n",
    "    \n",
    "    # Let's fit the training model\n",
    "    print_timestamp('Running svm.fit')\n",
    "    svm.fit(X_train, y_train)\n",
    "    \n",
    "    # Let's score the training set\n",
    "    print_timestamp('Running svm.score for the training set')\n",
    "    svm_training_score = svm.score(X_train, y_train)\n",
    "    printFormatted(\"###  SVM Training score={:.2%}\".format(svm_training_score))\n",
    "\n",
    "    # Let's score the test set\n",
    "    print_timestamp('Running svm.fit for the test set')\n",
    "    svm_test_score = svm.score(X_test, y_test)\n",
    "    printFormatted(\"###  SVM Test score={:.2%}\".format(svm_test_score))\n",
    "\n",
    "    if cross_validate == True:\n",
    "        accuracy = cross_val_score(svm, X_train, y_train, scoring='accuracy', cv = 5)\n",
    "        printFormatted(\"### Cross validation scores:  {}\".format(accuracy))\n",
    "        printFormatted(\"### Accuracy of Model with Cross Validation average is: {:.2%}\".format(accuracy.mean()))\n",
    "\n",
    "    print_timestamp('\\n'*3+'End run_svc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2749,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def run_logistic_regression():\n",
    "    print_timestamp('\\n'*3+'Begin')\n",
    "\n",
    "    lr = LogisticRegression(C=1e20, solver='lbfgs', max_iter=1000)\n",
    "\n",
    "    print_timestamp('Running lr.fit for the training set')\n",
    "    lr.fit(X_train, y_train)\n",
    "    \n",
    "    print_timestamp('Running lr.fit for the training set')\n",
    "    print('\\nR-squared simple model training set yields:')\n",
    "    print(lr.score(X_train, y_train))\n",
    "    print(\"here comes the test set\")\n",
    "    lrscore = lr.score(X_test, y_test)\n",
    "    printFormatted(\"###  Logistic Regression score={:.2%}\".format(lrscore))\n",
    "    \n",
    "    print_timestamp('\\n'*3+'End')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2750,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def run_linear_regression():\n",
    "\n",
    "    print_timestamp('\\n'*3+'Begin')\n",
    "\n",
    "    regr = linear_model.LinearRegression()\n",
    "\n",
    "    print_timestamp('Running regr.fit for the training set')\n",
    "    regr.fit(X_train, y_train)\n",
    "    \n",
    "    print(\"\\nCoeffecients: \\n\", regr.coef_)\n",
    "    print(\"\\nIntercept: \\n\", regr.intercept_)\n",
    "    print(\"\\nR-squared for training data set:\")\n",
    "    print(regr.score(X_train, y_train))\n",
    "    \n",
    "    print(\"\\nR-squared for test data set:\")\n",
    "    print(regr.score(X_test, y_test))\n",
    "    \n",
    "    print_timestamp('End run_linear_regression.\\n\\n')\n",
    "    \n",
    "    print_timestamp('\\n'*3+'End')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2751,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def run_ridge_regression():\n",
    "    # Fitting a ridge regression model. Alpha is the regularization\n",
    "    # parameter (usually called lambda). As alpha gets larger, parameter\n",
    "    # shrinkage grows more pronounced. Note that by convention, the\n",
    "    # intercept is not regularized. Since we standardized the data\n",
    "    # earlier, the intercept should be equal to zero and can be dropped.\n",
    "    print_timestamp('\\n'*3+'Begin')\n",
    "    \n",
    "    ridgeregr = linear_model.Ridge(alpha=10, fit_intercept=False) \n",
    "    ridgeregr.fit(X_train, y_train)\n",
    "    print(ridgeregr.score(X_train, y_train))\n",
    "\n",
    "    print_timestamp('\\n'*3+'End')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2752,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def run_affinity_propagation(data, parameters=None):\n",
    "      \n",
    "    # returns the prediction, and the number of clusters\n",
    "    \n",
    "    print_timestamp('\\n'*3+'starting AffinityPropagation')\n",
    "\n",
    "    print_timestamp('\\n'*3+'Begin')\n",
    "    \n",
    "    ap = AffinityPropagation(**parameters)\n",
    "#     ap = AffinityPropagation(damping=0.5,\n",
    "#                          max_iter=200,\n",
    "#                          convergence_iter=15,\n",
    "#                          copy=True,\n",
    "#                          preference=None,\n",
    "#                          affinity='euclidean',\n",
    "#                          verbose=False) \n",
    "\n",
    "#     model = ap.fit(data)\n",
    "    pred = ap.fit_predict(data)\n",
    "\n",
    "#     Z = merge_predict_and_cluster(data, target, pred) # let's merge the data dataframe, prediction, and the cluster\n",
    "    \n",
    "    # Pull the number of clusters and cluster assignments for each data point.\n",
    "    cluster_centers_indices = ap.cluster_centers_indices_\n",
    "    n_clusters_ = len(cluster_centers_indices)\n",
    "    labels = ap.labels_\n",
    "    \n",
    "    print('Estimated number of clusters: {}'.format(n_clusters_))\n",
    "\n",
    "    labels = ap.labels_\n",
    "    \n",
    "    print(\"from run_affinity_propagation {}\".format(metrics.silhouette_score(data, labels, metric='euclidean')))\n",
    "    \n",
    "    print_timestamp('\\n'*3+'finished with AffinityPropagation')\n",
    "    \n",
    "    return pred, n_clusters_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2753,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def run_kmeans(data, parameters=None):\n",
    "    # returns the prediction for the cluster\n",
    "    \n",
    "    print_timestamp('\\n'*3+'Begin')\n",
    "    debug = False\n",
    "    \n",
    "    if debug == True:\n",
    "        print(\"received inside run_means: data.shape = {}\".format(data.shape))\n",
    "        print(\"inside run_kmeans, and the da# returns the prediction for the cluster.\")\n",
    "        print(\"here comes the null check for data inside run_kmeans\")\n",
    "        print(\"here comes data.head(50)\")\n",
    "#         display(data.head(50))\n",
    "        print(\"running with number of clusters = {}\".format(parameters['n_clusters']))\n",
    "        print(\"in run_kmeans, parameters = {}\".format(parameters))\n",
    "        print(\"in run_kmeans, data is a {} datatype.\".format(type(data)))\n",
    "#         num_sample_rows = 30\n",
    "        print(\"here is a sample of the data dataframe of {} rows.\".format(num_sample_rows))\n",
    "#         display(data.sample(num_sample_rows))\n",
    "              \n",
    "    km = KMeans(**parameters)\n",
    "\n",
    "    pred = km.fit_predict(data)\n",
    "\n",
    "    print_timestamp('\\n'*3+'End')\n",
    "    \n",
    "    return pred   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2754,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def merge_predict_and_cluster(dataframe, target, predict):\n",
    "    Z = pd.merge(dataframe, target, left_index=True, right_index=True)\n",
    "    Z = pd.merge(Z, pd.DataFrame(predict), left_index=True, right_index=True)\n",
    "    Z.rename(columns={Z.columns[-1]: 'cluster'}, inplace=True)\n",
    "    \n",
    "    return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2755,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def confusion_matrix_function(model_fit, X_train, X_test, y_train, y_test):\n",
    "    \n",
    "    y_pred_class  = model_fit.predict(X_test)\n",
    "    \n",
    "    conf_matrix = confusion_matrix_function(y_test, y_pred_class)\n",
    "    printFormatted(\"### Confusion Matrix:  {}\".format(conf_matrixscores))\n",
    "    \n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2756,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def run_spectral_clustering(data, parameters=None):\n",
    "    display_dataframe_shape('entering run_spectral_clustering, data has shape of:', data)\n",
    "#     display_dataframe_shape('entering run_spectral_clustering, target has shape of:', target)\n",
    "    print_timestamp('\\n'*3+'Begin')\n",
    "    \n",
    "#     for clusternum in range(2, K):\n",
    "    print_timestamp(\"Running spectral_clustering with {} clusters.\".format(parameters['n_clusters']))\n",
    "#     n_clusters=K\n",
    "\n",
    "    # Declare and fit the model.\n",
    "    sc = SpectralClustering(**parameters)\n",
    "    sc.fit(data)\n",
    "\n",
    "    #Predicted clusters.\n",
    "    predict=sc.fit_predict(data)\n",
    "\n",
    "#     Z = merge_predict_and_cluster(data, target, predict) # let's merge the data dataframe, prediction, and the cluster\n",
    "\n",
    "    if debug == True:\n",
    "        pass\n",
    "#         display_dataframe_shape('in run_spectral_clustering, Z has shape of:', Z)\n",
    "#         display_dataframe_shape('in run_spectral_clustering, target has shape of:', target)\n",
    "#         display(\"the datatypes of Z are\", Z.dtypes)\n",
    "\n",
    "#     plt.scatter(Z['cluster'], Z[target_column], c=Z['cluster'])\n",
    "#     plt.show()\n",
    "\n",
    "    labels = sc.labels_\n",
    "    print(\"from spectral clustering {}\".format(metrics.silhouette_score(data, labels, metric='euclidean')))\n",
    "\n",
    "#     print('Comparing the assigned categories to the ones in the data:')\n",
    "#     print(pd.crosstab(target,predict))\n",
    "    \n",
    "    print_timestamp('\\n'*3+'End')\n",
    "    \n",
    "    return predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2757,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def do_the_elbow(X, maxK=11):\n",
    "    printFormatted(\"## We are plotting the elbow method!\")\n",
    "    print_timestamp('\\n'*3+'Begin')\n",
    "    # calculate distortion for a range of number of cluster\n",
    "    distortions = []\n",
    "    for i in range(1, maxK):\n",
    "        km = KMeans(\n",
    "            n_clusters=i, init='random',\n",
    "            n_init=10, max_iter=300,\n",
    "            tol=1e-04, random_state=0\n",
    "        )\n",
    "        km.fit(X)\n",
    "        distortions.append(km.inertia_)\n",
    "\n",
    "    # plot\n",
    "    plt.plot(range(1, maxK), distortions, marker='o')\n",
    "    plt.xlabel('Number of clusters')\n",
    "    plt.ylabel('Distortion')\n",
    "    plt.show()\n",
    "    \n",
    "    print_timestamp('\\n'*3+'End')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2758,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def plot_it_clusters(dataframe, xvalue, yvalue, title):\n",
    "    \n",
    "    if debug == True:\n",
    "        display_dataframe_shape('entry received in plot_it_clusters', dataframe)\n",
    "        display(dataframe.dtypes)\n",
    "\n",
    "    data_demographics(dataframe, 10)\n",
    "        \n",
    "    plt.rcParams['figure.figsize'] = [xvalue, yvalue]\n",
    "    plt.xlabel(xcolumnname)\n",
    "    plt.ylabel(ycolumnname)\n",
    "    \n",
    "    df0 = dataframe[dataframe.cluster == 0]\n",
    "    df1 = dataframe[dataframe.cluster == 1]\n",
    "    df2 = dataframe[dataframe.cluster == 2]\n",
    "    df3 = dataframe[dataframe.cluster == 3]\n",
    "    df4 = dataframe[dataframe.cluster == 4]\n",
    "    df5 = dataframe[dataframe.cluster == 5]\n",
    "    \n",
    "    plt.scatter(df0[xcolumnname], df0[ycolumnname], color='green')\n",
    "    plt.scatter(df1[xcolumnname], df1[ycolumnname], color='red')\n",
    "    plt.scatter(df2[xcolumnname], df2[ycolumnname], color='blue')\n",
    "    plt.scatter(df3[xcolumnname], df3[ycolumnname], color='black')\n",
    "    plt.scatter(df4[xcolumnname], df4[ycolumnname], color='magenta')\n",
    "    plt.scatter(df5[xcolumnname], df5[ycolumnname], color='orange')\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "        \n",
    "#     plt.scatter(km.cluster_centers_[:,0],km.cluster_centers_[:,1],color='purple',marker='*',label='centroid')\n",
    "    \n",
    "#     if type == 'KMeans':\n",
    "#         plt.xlabel('Age')\n",
    "#         plt.ylabel('Income ($)')\n",
    "#         plt.legend()\n",
    "#         plt.scatter(km.cluster_centers[:,0], \n",
    "#                     km.cluster_centers[:,1],\n",
    "#                     marker = '*',\n",
    "#                     label = 'centroid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2759,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def run_mean_shift(data, parameters=None):\n",
    "    \n",
    "    # returns the cluster prediction, and the number of clusters\n",
    "    \n",
    "    print_timestamp('\\n'*3+'Begin')  \n",
    "    \n",
    "    print(\"inside run_mean_shift, the parameters received are {}\".format(parameters))\n",
    "    \n",
    "    X_train = data\n",
    "    \n",
    "#     :::\n",
    "#     bandwidth = estimate_bandwidth(X_train, quantile=0.2, n_samples=500)\n",
    "\n",
    "#     # Declare and fit the model.\n",
    "#     ms = MeanShift(bandwidth=bandwidth, bin_seeding=True)\n",
    "#     ms.fit(X_train)\n",
    "\n",
    "#     # Extract cluster assignments for each data point.\n",
    "#     labels = ms.labels_\n",
    "\n",
    "#     # Coordinates of the cluster centers.\n",
    "#     cluster_centers = ms.cluster_centers_\n",
    "\n",
    "#     # Count our clusters.\n",
    "#     n_clusters_ = len(np.unique(labels))\n",
    "\n",
    "#     print(\"Number of estimated clusters: {}\".format(n_clusters_)) \n",
    "    \n",
    "#         :::\n",
    "    \n",
    "    # Here we set the bandwidth. This function automatically derives a bandwidth\n",
    "    # number based on an inspection of the distances among points in the data.\n",
    "#     estimate_bandwidth(X, quantile=0.3, n_samples=None, random_state=0, n_jobs=None)\n",
    "    bandwidth = estimate_bandwidth(X_train, quantile=0.2, n_jobs=4, random_state=57)\n",
    "\n",
    "    # Declare and fit the model.\n",
    "#     ms = MeanShift(bandwidth=bandwidth, bin_seeding=True)\n",
    "    ms = MeanShift(bandwidth=2)\n",
    "    if debug == True:\n",
    "        display_dataframe_shape('this is the shape of data coming into run_mean_shift', data)\n",
    "    ms.fit(data)\n",
    "\n",
    "#     if debug == True:\n",
    "#         display_dataframe_shape('this is the shape of target coming into run_mean_shift', target)\n",
    "        \n",
    "    pred = ms.predict(data)\n",
    "    \n",
    "    if debug == True:\n",
    "        display_dataframe_shape('this is the shape of pred after predict in run_mean_shift', data)\n",
    "        \n",
    "#     Z = merge_predict_and_cluster(data, target, pred) # let's merge the data dataframe, prediction, and the cluster\n",
    "\n",
    "    # Extract cluster assignments for each data point.\n",
    "    labels = ms.labels_\n",
    "\n",
    "    print(\"from mean shift {}\".format(metrics.silhouette_score(data, labels, metric='euclidean')))\n",
    "    \n",
    "    # Coordinates of the cluster centers.\n",
    "    cluster_centers = ms.cluster_centers_\n",
    "\n",
    "    # Count our clusters.\n",
    "    n_clusters_ = len(np.unique(labels))\n",
    "\n",
    "    print(\"Number of estimated clusters: {}\".format(n_clusters_))\n",
    "    \n",
    "    print_timestamp('\\n'*3+'End')\n",
    "    \n",
    "    return pred, n_clusters_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2760,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def run_tfidf_vectorizer(df, max_df=0.5, min_df=2, stop_words='english', lowercase=True, use_idf=True, norm=u'l2', smooth_idf=True):\n",
    "    print_timestamp(\"in run_tfidf_vectorizer: df is a {} datatype.\".format(type(df)))\n",
    "    vectorizer = TfidfVectorizer(\n",
    "                             max_df=max_df, # drop words that occur in more than half the paragraphs\n",
    "                             min_df=min_df, # only use words that appear at least twice\n",
    "                             stop_words=stop_words, \n",
    "                             lowercase=lowercase, #convert everything to lower case (since Alice in Wonderland has the HABIT of CAPITALIZING WORDS for EMPHASIS)\n",
    "                             use_idf=use_idf,#we definitely want to use inverse document frequencies in our weighting\n",
    "                             norm=norm, #Applies a correction factor so that longer paragraphs and shorter paragraphs get treated equally\n",
    "                             smooth_idf=smooth_idf #Adds 1 to all document frequencies, as if an extra document existed that used every word once.  Prevents divide-by-zero errors\n",
    "                            )\n",
    "\n",
    "    #Applying the vectorizer\n",
    "    tfidf_df = vectorizer.fit(df)\n",
    "    \n",
    "    return tfidf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2761,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def run_tfidf_vectorizer2(df, parameters=None):\n",
    "    print_timestamp(\"in run_tfidf_vectorizer: df is a {} datatype.\".format(type(df)))\n",
    "    if parameters:\n",
    "        print(\"parms = {}\".format(parameters))\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    vectorizer.set_params(**parameters)\n",
    "\n",
    "    #Applying the vectorizer\n",
    "    tfidf_df = vectorizer.fit_transform(df)\n",
    "    print(\"tfidf_df is a {} datatype\".format(type(tfidf_df)))\n",
    "    \n",
    "    tfidf_df_dense = tfidf_df.toarray()\n",
    "    print(\"tfidf_df_dense is a {} datatype\".format(type(tfidf_df_dense)))\n",
    "\n",
    "    return tfidf_df_dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2762,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def text_cleaner(text):\n",
    "    # Visual inspection identifies a form of punctuation spaCy does not\n",
    "    # recognize: the double dash '--'.  Better get rid of it now!\n",
    "    text = re.sub(r'--',' ',text)\n",
    "    text = re.sub(\"[\\[].*?[\\]]\", \"\", text)\n",
    "    text = ' '.join(text.split())\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2763,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def bag_of_words(text):\n",
    "    \n",
    "    # Filter out punctuation and stop words.\n",
    "    allwords = [token.lemma_\n",
    "                for token in text\n",
    "                if not token.is_punct\n",
    "                and not token.is_stop]\n",
    "    \n",
    "    # Return the most common words.\n",
    "    return [item[0] for item in Counter(allwords).most_common(2000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2764,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def bow_features_dev(sentences, common_words):\n",
    "    display(sentences.head(10))\n",
    "    num_sentences_to_print = 10\n",
    "    print(\"inside bow_features: sentences is a {} datatype, of {} length,\\nand common_words is a {} datatype of {} length.\"\n",
    "          .format(type(sentences), sentences.shape[0], type(common_words), len(common_words)))\n",
    "    print(\"here come {} sentences: {}\".format(num_sentences_to_print, sentences[0:num_sentences_to_print]))\n",
    "    # Scaffold the data frame and initialize counts to zero.\n",
    "    df = pd.DataFrame(columns=common_words)\n",
    "    print(\"in bow_features: sentences.iloc[0] = {}\".format(sentences.iloc[0]))\n",
    "    df['text_sentence'] = sentences.iloc[0] # this could be the problem\n",
    "    df['text_source'] = sentences.iloc[1]   # this too could be the problem...\n",
    "    df.loc[:, common_words] = 0\n",
    "    \n",
    "    # Process each row, counting the occurrence of words in each sentence.\n",
    "    num_values = len(list(enumerate(df['text_sentence'])))\n",
    "    print(\"There are {} enumerated items for iterations.\".format(num_values))\n",
    "    for i, sentence in enumerate(df['text_sentence']):\n",
    "        \n",
    "        # Convert the sentence to lemmas, then filter out punctuation,\n",
    "        # stop words, and uncommon words.\n",
    "        words = [token.lemma_\n",
    "                 for token in sentence\n",
    "                 if (\n",
    "                     not token.is_punct\n",
    "                     and not token.is_stop\n",
    "                     and token.lemma_ in common_words\n",
    "                 )]\n",
    "        \n",
    "        # Populate the row with word counts.\n",
    "        for word in words:\n",
    "            df.loc[i, word] += 1\n",
    "        \n",
    "        # This counter is just to make sure the kernel didn't hang.\n",
    "        if i % 50 == 0:\n",
    "            print(\"Processing row {}\".format(i))\n",
    "            \n",
    "    return pd.DataFrame(df)\n",
    "\n",
    "#     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2765,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def bow_features(sentences, common_words):\n",
    "  \n",
    "    # Scaffold the data frame and initialize counts to zero.\n",
    "    df = pd.DataFrame(columns=common_words)\n",
    "    df['text_sentence'] = sentences[0] # this could be the problem\n",
    "    df['text_source'] = sentences[1]   # this too could be the problem...\n",
    "    df.loc[:, common_words] = 0\n",
    "    \n",
    "    # Process each row, counting the occurrence of words in each sentence.\n",
    "    for i, sentence in enumerate(df['text_sentence']):\n",
    "        \n",
    "        # Convert the sentence to lemmas, then filter out punctuation,\n",
    "        # stop words, and uncommon words.\n",
    "        words = [token.lemma_\n",
    "                 for token in sentence\n",
    "                 if (\n",
    "                     not token.is_punct\n",
    "                     and not token.is_stop\n",
    "                     and token.lemma_ in common_words\n",
    "                 )]\n",
    "        \n",
    "        # Populate the row with word counts.\n",
    "        for word in words:\n",
    "            df.loc[i, word] += 1\n",
    "        \n",
    "        # This counter is just to make sure the kernel didn't hang.\n",
    "        if i % 50 == 0:\n",
    "            print(\"Processing row {}\".format(i))\n",
    "            \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2766,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def run_mnb(X_train=None, X_test=None, y_train=None, y_test=None, cross_validate=None, params=None):\n",
    "\n",
    "    mnb = MultinomialNB()\n",
    "    \n",
    "    if params == True:\n",
    "        print(\"In run_rf, params = {}\".format(params)) \n",
    "        mnb.set_params(**params)\n",
    "        \n",
    "    mnb_fit = mnb.fit(X_train, y_train)\n",
    "        \n",
    "#     training_score = mnb.score(X_train, y_train) \n",
    "#     printFormatted(\"### Training score = {:.2%}\".format(training_score))\n",
    "    \n",
    "#      ## Let's score it with the test data set\n",
    "#     test_score = mnb.score(X_test, y_test)\n",
    "    \n",
    "    #   Let's score it with the test data set    this is new 13-Aug-2019\n",
    "    print_training_and_test_scores(mnb_fit, X_train, X_test, y_train, y_test) # new on 13-Aug-2019\n",
    "    \n",
    "#   Let's produce the metrics scores\n",
    "    print_metrics_score(mnb_fit, X_train, X_test, y_train, y_test) # new on 13-Aug-2019\n",
    "    \n",
    "    if cross_validate == True:\n",
    "        print_cross_validation_scores(mnb_fit, X_train, X_test, y_train, y_test)\n",
    "        \n",
    "    if confusion_matrix == True:\n",
    "        confusion_matrix_function(mnb_fit, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2767,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def word2vec_function(sentences, workers=4, min_count=10, window=6, sg=0, sample=1e-3, size=300, hs=1):\n",
    "# import gensim\n",
    "# from gensim.models import word2vec\n",
    "# original values from the curriculum:\n",
    "# workers = 4, min_count=10, window=6, sg=0, sample=1e-3, size=300, hs=1\n",
    "\n",
    "    model = word2vec.Word2Vec(\n",
    "        sentences,\n",
    "        workers=workers,     # Number of threads to run in parallel (if your computer does parallel processing).\n",
    "        min_count=min_count,  # Minimum word count threshold.\n",
    "        window=window,      # Number of words around target word to consider.\n",
    "        sg=sg,          # Use CBOW because our corpus is small.\n",
    "        sample=sample ,  # Penalize frequent words.\n",
    "        size=size,      # Word vector length.\n",
    "        hs=hs           # Use hierarchical softmax.\n",
    "    )\n",
    "\n",
    "    print('done!')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2768,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def vectorizer_nb(type_of_vectorizer):\n",
    "\n",
    "    print_timestamp(BegTimeStamp)\n",
    "    \n",
    "    # 1. import and instantiate CountVectorizer (with the default parameters)\n",
    "\n",
    "    # 2. instantiate CountVectorizer (vectorizer)\n",
    "\n",
    "#     X = df.message\n",
    "#     y = df.sentiment_label\n",
    "\n",
    "    # split X and y into training and testing sets\n",
    "    # by default, it splits 75% training and 25% test\n",
    "    # random_state=1 for reproducibility\n",
    "    \n",
    "#     X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n",
    "\n",
    "    # 3. fit & transform\n",
    "    if type_of_vectorizer == 'Count':\n",
    "        print(\"We are running with CountVectorizer\")\n",
    "        vectorizer = CountVectorizer()\n",
    "        vectorizer.fit(X_train)\n",
    "        vectorizer_method = 'CountVectorizer'\n",
    "    elif type_of_vectorizer == 'Tfidf':\n",
    "        print(\"We are running with TfidfVectorizer\")\n",
    "        vectorizer = TfidfVectorizer()\n",
    "        vectorizer.fit_transform(X_train)\n",
    "        vectorizer_method = 'TfidfVectorizer'\n",
    "    \n",
    "    # 4. transform training data\n",
    "    X_train_dtm = vectorizer.transform(X_train)\n",
    "\n",
    "    # equivalently: combine fit and transform into a single step\n",
    "    # this is faster and what most people would do\n",
    "    X_train_dtm = vectorizer.fit_transform(X_train)\n",
    "\n",
    "    # 4. transform testing data (using fitted vocabulary) into a document-term matrix\n",
    "    X_test_dtm = vectorizer.transform(X_test)\n",
    "\n",
    "    # 1. import\n",
    "\n",
    "    # 2. instantiate a Multinomial Naive Bayes model\n",
    "    nb = MultinomialNB()\n",
    "\n",
    "    # 3. train the model \n",
    "    # using X_train_dtm (timing it with an IPython \"magic command\")\n",
    "\n",
    "    nb.fit(X_train_dtm, y_train)\n",
    "    \n",
    "    \n",
    "    # 4. make class predictions for X_test_dtm\n",
    "    y_pred_class = nb.predict(X_test_dtm)\n",
    "\n",
    "    # calculate accuracy of class predictions\n",
    "\n",
    "    met_test_score = metrics.accuracy_score(y_test, y_pred_class)\n",
    "    printFormatted('###  With {} vectorizer, the metrics accuracy score = {:.2%}'.format(vectorizer_method,\n",
    "                                                                                         met_test_score))\n",
    "    \n",
    "    print_timestamp(EndTimeStamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2769,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def display_column_names(label, df):\n",
    "    display(\"Label: {}: Column names are:\".format(label), df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2770,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def display_datatype(var):\n",
    "    # this function just returns the data type of the variable var\n",
    "    return \"{}\".format(type(var))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2771,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def display_dataframe_shape(label, df):\n",
    "    display(\"Label: {}: Dataframe shape is:\".format(label), df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2772,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def run_it(X_train, X_test, y_train, y_test, y):\n",
    "    \n",
    "#     file_stuff()\n",
    "    \n",
    "#     data_cleanup()\n",
    "    \n",
    "    print_timestamp('\\n'*3+'Begin')\n",
    "    \n",
    "    if Regression == True:\n",
    "        print_timestamp(\"We are running with a Regression model\")\n",
    "    elif Regression == False:\n",
    "        print_timestamp(\"We are running with a Classifier model\")\n",
    "    else:\n",
    "        print_timestamp(\"We have failed to set the Regression variable\")\n",
    "        sys.exit(main())\n",
    "        \n",
    "\n",
    "    if flag_to_plot_them == True:\n",
    "        plot_them()\n",
    "\n",
    "    if flag_to_run_features_importance == True:\n",
    "        \n",
    "        number_of_features_to_consider = 50\n",
    "        params = {'n_estimators': 100}\n",
    "\n",
    "        if Regression == True:\n",
    "            print_timestamp('We are running RandomForestRegressor')\n",
    "            rf = ensemble.RandomForestRegressor(**params)\n",
    "            \n",
    "        else:\n",
    "            print_timestamp('We are running RandomForestClassifier')\n",
    "            rf = ensemble.RandomForestClassifier(**params)\n",
    "\n",
    "        run_features_importance(rf, number_of_features_to_consider)\n",
    "\n",
    "    if flag_to_run_correlation_matrix == True:\n",
    "        run_correlation_matrix()\n",
    "        \n",
    "    if flag_to_run_sentiment_analyzer == True:\n",
    "        path = \"B\"\n",
    "\n",
    "\n",
    "        for path in ['A']:\n",
    "            for vectorizer_iterator in ['logit', 'mlb', 'bnb']:\n",
    "                if vectorizer_iterator == 'rfc':\n",
    "                    sentiment_analyzer(path=path, parameters=params, classifier=vectorizer_iterator, tfidf_parms=tfidf_parms)\n",
    "\n",
    "                elif vectorizer_iterator == 'bnb':\n",
    "                    parameters = {}\n",
    "                    sentiment_analyzer(path=path, parameters=parameters, classifier=vectorizer_iterator, tfidf_parms=tfidf_parms)\n",
    "\n",
    "                elif vectorizer_iterator == 'mlb':\n",
    "                    parameters = {}\n",
    "                    sentiment_analyzer(path=path, parameters=parameters, classifier=vectorizer_iterator, tfidf_parms=tfidf_parms)\n",
    "\n",
    "                elif vectorizer_iterator == 'logit': # newton-cg took too long. sag and saga about the same as lbfgs.\n",
    "                    tfidf_parms = {'max_features' :  10000 } # determined this through iterative testing\n",
    "                    parameters = {'C' :1e20, 'solver': 'lbfgs', 'max_iter': 1000} # max_iter=100 reports warning, try 1000\n",
    "                    sentiment_analyzer(path=path, parameters=parameters, classifier=vectorizer_iterator, tfidf_parms=tfidf_parms)\n",
    "\n",
    "                elif vectorizer_iterator == 'svc':\n",
    "                    parameters = {}\n",
    "                    sentiment_analyzer(path=path, parameters=parameters, classifier=vectorizer_iterator, tfidf_parms=tfidf_parms)\n",
    "                    if confusion_matrix != None:\n",
    "                        confusion_matrix_function(y_test, y_pred_class)\n",
    "\n",
    "    if flag_to_run_rf == True:\n",
    "        #     params = {}\n",
    "        params = {'n_estimators': 100} \n",
    "\n",
    "        if Regression == True:\n",
    "            rf = ensemble.RandomForestRegressor(**params)\n",
    "            print_timestamp('We are running RandomForestRegressor')\n",
    "        else:\n",
    "            rf = ensemble.RandomForestClassifier(**params)\n",
    "            print_timestamp('We are running RandomForestClassifier')\n",
    "\n",
    "        run_rf(rf)\n",
    "\n",
    "    if flag_to_run_gradient_boosting  == True:\n",
    "        run_gradient_boosting()\n",
    "\n",
    "    if flag_to_run_linear_regression  == True:\n",
    "        run_linear_regression()\n",
    "\n",
    "    if flag_to_run_logistic_regression == True:\n",
    "        run_logistic_regression()\n",
    "\n",
    "    if flag_to_run_svc == True:\n",
    "        run_svc() \n",
    "\n",
    "    if flag_to_run_ridge_regression == True:\n",
    "        run_ridge_regression()\n",
    "        \n",
    "    if flag_to_run_vectorizer_nb == True:\n",
    "        for vectorizer_iterator in ['Count', 'Tfidf']:\n",
    "            vectorizer_nb(vectorizer_iterator)\n",
    "        \n",
    "    if flag_to_run_kmeans == True:\n",
    "        method = KMeans(\n",
    "             n_clusters=num_clusters\n",
    "#                 ,random_state=42\n",
    "#                 ,init='random'\n",
    "#                 ,n_init=10\n",
    "#                 ,max_iter=300\n",
    "#                 ,tol=1e-04 \n",
    "        )\n",
    "        df1 = run_kmeans(X_train, y_train, num_clusters)\n",
    "        plot_it_clusters(df1, xvalue=16, yvalue=16, title=\"KMeans with number of clusters = {}\".format(num_clusters))\n",
    "        display(\"next plot please\")\n",
    "\n",
    "    if flag_to_run_affinity_propagation == True:\n",
    "        display_column_names('columns of X_train going into affinity_propagation: ', X_train)\n",
    "        df2, ap_num_clusters = run_affinity_propagation(X_train, y_train)\n",
    "        plot_it_clusters(df2, xvalue=16, yvalue=16, title=\"Affinity Propagation with number of clusters = {}\".format(ap_num_clusters))\n",
    "        \n",
    "    if flag_to_run_mean_shift == True:\n",
    "        df3, mean_shift_num_clusters = run_mean_shift(X_train, y_train)\n",
    "        plot_it_clusters(df3, xvalue=16, yvalue=16, title=\"Mean Shift with number of clusters = {}\".format(mean_shift_num_clusters))\n",
    "    \n",
    "    if flag_to_run_spectral_clustering == True:\n",
    "        df4 = run_spectral_clustering(X_train, y_train, K=num_clusters)\n",
    "        plot_it_clusters(df4, xvalue=16, yvalue=16,title=\"Spectral clustering with number of clusters = {}\".format(num_clusters) )\n",
    "\n",
    "    print_timestamp('End'+'\\n'*3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2773,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def cluster_analyzer(path, parameters, classifier, tfidf_parms, X_train=None, X_test=None, y_train=None, y_test=None, cross_validate=None):\n",
    "    # path A = the old path\n",
    "    # path B = the new path, no CountVectorizer at all\n",
    "    \n",
    "# run block of code and catch warnings\n",
    "  \n",
    "    if debug == True:\n",
    "        print_timestamp(BegTimeStamp+\" running with path={}\".format(path))\n",
    "    \n",
    "#     global vectorized\n",
    "#     vectorized = True\n",
    "    \n",
    "    pipeline_array = []\n",
    "   \n",
    "    if path == \"A\":\n",
    "        if clusterer == 'kmn':\n",
    "            # Kmeans\n",
    "            if vec == 'tfidf':\n",
    "                pipeline_array.append(Pipeline([ ('tfidf', TfidfVectorizer(**tfidf_parms)), ('clf',   Kmeans(**parameters))]))\n",
    "            elif vec == 'w2v':\n",
    "                pipeline_array.append(Pipeline([ ('w2v', w2vec_vectorize_text()),   ('clus',   Kmeans(**parameters))]))\n",
    "\n",
    "\n",
    "        elif clusterer == 'ap':\n",
    "            # Affinity Propagation\n",
    "            ap = AffinityPropagation(parameters)\n",
    "            if vec == 'tfidf':\n",
    "                pipeline_array.append(Pipeline([('tfidf', TfidfVectorizer(**tfidf_parms)),('ap',   ap(**parameters))   ])) \n",
    "            elif vec == 'w2v':\n",
    "                pipeline_array.append(Pipeline([('w2v', w2vec_vectorize_text()),('clus',   ap(**parameters))   ])) \n",
    "                # w2vec_vectorize_text(sentences=df_book_blocks['block_text_clean'], size=num_features, min_count=1,sg=1, seed=57, workers=4)                      \n",
    "          \n",
    "        elif clusterer == 'sc':\n",
    "            # Spectral Clustering\n",
    "            pipeline_array.append(Pipeline([\n",
    "                ('tfidf', TfidfVectorizer(**tfidf_parms)),\n",
    "                ('clf',   MultinomialNB(**parameters))\n",
    "            ]))\n",
    "        elif clusterer == 'ms':\n",
    "            # Means Shift\n",
    "            pipeline_array.append(Pipeline([\n",
    "                ('tfidf', TfidfVectorizer(**tfidf_parms)),\n",
    "                ('clf',   LogisticRegression(**parameters))\n",
    "            ]))\n",
    "\n",
    "    pipe = pipeline_array[0]\n",
    "    \n",
    "    try:\n",
    "        vect_name_list = str(pipe.named_steps['vect']).split('(')\n",
    "        vect_name = \"vect = {}, \".format(vect_name_list[0])\n",
    "    except:\n",
    "        vect_name = ''\n",
    "\n",
    "    classifier_name_list=str(pipe.named_steps['clf']).split('(')\n",
    "    classifier_name=classifier_name_list[0]\n",
    "    tfidf_name_list = str(pipe.named_steps['tfidf']).split('(')\n",
    "    if len(tfidf_name_list) > 0:\n",
    "        tfidf_name = tfidf_name_list[0]\n",
    "    else:\n",
    "        tfidf_name = ''\n",
    "\n",
    "    printFormatted(\"###  Now running with: {} tfidf={} and clf={} {}\\nparameters={} \\n\\n tfidf_parms={}\".format( vect_name,\n",
    "                                                                                                tfidf_name,\n",
    "                                                                                                classifier_name,\n",
    "                                                                                                return_current_datetime(),\n",
    "                                                                                                parameters,\n",
    "                                                                                                tfidf_parms\n",
    "                                                                                                ))\n",
    "    pipefit = pipe.fit_predict(X_train, y_train)\n",
    "\n",
    "#   Let's score it with the test data set    this is new 13-Aug-2019\n",
    "    print_training_and_test_scores(pipefit, X_train, X_test, y_train, y_test) # new on 13-Aug-2019\n",
    "    \n",
    "#   Let's produce the metrics scores\n",
    "    print_metrics_score(pipefit, X_train, X_test, y_train, y_test) # new on 13-Aug-2019\n",
    "    \n",
    "#   Let's run cross validation \n",
    "    if cross_validate == True:\n",
    "        print_cross_validation_scores(pipefit, X_train, X_test, y_train, y_test)\n",
    "        \n",
    "#   Let's run the confusion matrix\n",
    "    if confusion_matrix == True:\n",
    "        confusion_matrix_function(pipefit, X_train, X_test, y_train, y_test)\n",
    "            \n",
    "    if debug == True:\n",
    "        printFormatted(\"Steps information: {}\".format(pipe.steps))\n",
    "        print_timestamp(\"Finished running pipeline with:\\n{}: \".format(classifier_name))\n",
    "        \n",
    "    print_timestamp(EndTimeStamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2774,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def main(entry_point):\n",
    "        \n",
    "    if entry_point == 0:\n",
    "        print_timestamp(\"Starting main()\")\n",
    "#         df = file_stuff()\n",
    "#         data_demographics(df, 5)\n",
    "#         display_column_names('post data_demographics of df', df)\n",
    "#         df, X, y = dataset_cleanup(df)\n",
    "#         display_column_names('post dataset_cleanup on X', X)\n",
    "#         data_demographics(df, 5)\n",
    "#         display_column_names('post data_demographics on X #2', X)\n",
    "# #         make_X_and_Y()\n",
    "#         X_train, X_test, y_train, y_test = training_test_set(X, y)\n",
    "#         display_column_names('after training_test_set: columns of X_train going into affinity_propagation: ', X_train)\n",
    "#         data_characteristics()\n",
    "#         plot_time_to_complete()\n",
    "#         plot_model_accuracy()\n",
    "#         plot_facet()\n",
    "\n",
    "    if flag_to_run_elbow_plot == True:   \n",
    "        do_the_elbow(X)\n",
    "    run_it(X_train, X_test, y_train, y_test, y)\n",
    "        \n",
    "    print_timestamp(\"Ending main()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2775,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def do_word_counts_from_text_files(gutenberg_books, regex=None, book_divisor=1 ):\n",
    "    print_timestamp(\"Starting do_word_counts()\")\n",
    "      \n",
    "    sentences = pd.DataFrame()\n",
    "    \n",
    "    \n",
    "    book_list = [('milton-paradise.txt',r'BOOK .*') , ('melville-moby_dick.txt',r'BOOK .*',)]\n",
    "#     gutenberg_books is a list of files on the computer\n",
    "    for book in book_list:\n",
    "#     paradise_book = 'milton-paradise.txt'\n",
    "#     moby_book = 'melville-moby_dick.txt'\n",
    "\n",
    "        book - gutenberg.raw(book)\n",
    "        book = re.sub(book[1],'', book)\n",
    "#     paradise = gutenberg.raw(paradise_book)\n",
    "#     paradise = re.sub(r'BOOK .*', '', paradise)\n",
    "\n",
    "#     moby = gutenberg.raw(moby_book)\n",
    "#     moby = re.sub(r'BOOK .*', '', moby)\n",
    "        book = text_cleaner(book[:int(len(book)/book_divisor)])\n",
    "\n",
    "\n",
    "    paradise = text_cleaner(paradise[:int(len(paradise)/paradise_divisor)])\n",
    "    moby     = text_cleaner(moby[:int(len(moby)/moby_divisor)])\n",
    "\n",
    "    if debug == True:\n",
    "        print(\"length of paradise is {}, and length of moby is {}.\".format(len(paradise), len(moby)))\n",
    "\n",
    "    paradise_doc = nlp(paradise)\n",
    "    moby_doc = nlp(moby)\n",
    "\n",
    "    paradise_sents = [[sent, \"Milton\"] for sent in paradise_doc.sents]\n",
    "    moby_sents = [[sent, \"Melville\"] for sent in moby_doc.sents]\n",
    "\n",
    "    print(\"paradise_sents is a {} datatype, and moby_sents is a {} datatype.\".format(display_datatype(paradise_sents), display_datatype(moby_sents)))\n",
    "    # a better way to do this is with pd.append... from https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.append.html\n",
    "#     >>> df = pd.DataFrame([[1, 2], [3, 4]], columns=list('AB'))\n",
    "#     >>> df\n",
    "#        A  B\n",
    "#     0  1  2\n",
    "#     1  3  4\n",
    "#     >>> df2 = pd.DataFrame([[5, 6], [7, 8]], columns=list('AB'))\n",
    "#     >>> df.append(df2, ignore_index=True)\n",
    "#            A  B\n",
    "#         0  1  2\n",
    "#         1  3  4\n",
    "#         2  5  6\n",
    "#         3  7  8\n",
    "    sentences = pd.DataFrame(paradise_sents + moby_sents) \n",
    "    sentences = pd.DataFrame(paradise_sents)\n",
    "    sentences.append(moby_sents, ignore_index=True)\n",
    "    \n",
    "    if debug == True:\n",
    "        display(\"Here is a sample from sentences:\\n\", sentences.sample(20))  \n",
    "        print(\"sentences is a {} datatype.\".format(display_datatype(sentences)))\n",
    "        sentences.head(30)\n",
    "\n",
    "    paradisewords = bag_of_words(paradise_doc)\n",
    "    mobywords = bag_of_words(moby_doc)\n",
    "    common_words = list(set(paradisewords + mobywords))\n",
    "    word_counts = bow_features(sentences, common_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2776,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def main_nlp_old(entry_point):\n",
    "    \n",
    "    # this function was specifically designed for this NLP Challenge\n",
    "    \n",
    "    if entry_point == 0:\n",
    "        confusion_matrix = None\n",
    "        \n",
    "        paradise_divisor = 20 # was 20\n",
    "        moby_divisor     = 53 # was 53\n",
    "        \n",
    "        print_timestamp(\"Starting main_nlp()\")\n",
    "        \n",
    "        paradise_book = 'milton-paradise.txt'\n",
    "        moby_book = 'melville-moby_dick.txt'\n",
    "        \n",
    "        paradise = gutenberg.raw(paradise_book)\n",
    "        paradise = re.sub(r'BOOK .*', '', paradise)\n",
    "\n",
    "        moby = gutenberg.raw(moby_book)\n",
    "        moby = re.sub(r'BOOK .*', '', moby)\n",
    "        \n",
    "        paradise = text_cleaner(paradise[:int(len(paradise)/paradise_divisor)])\n",
    "        moby     = text_cleaner(moby[:int(len(moby)/moby_divisor)])\n",
    "        \n",
    "        if debug == True:\n",
    "            print(\"length of paradise is {}, and length of moby is {}.\".format(len(paradise), len(moby)))\n",
    "        \n",
    "        paradise_doc = nlp(paradise)\n",
    "        moby_doc = nlp(moby)\n",
    "        \n",
    "        paradise_sents = [[sent, \"Milton\"] for sent in paradise_doc.sents]\n",
    "        moby_sents = [[sent, \"Melville\"] for sent in moby_doc.sents]\n",
    "\n",
    "        print(\"paradise_sents is a {} datatype, and moby_sents is a {} datatype.\".format(display_datatype(paradise_sents), display_datatype(moby_sents)))\n",
    "        sentences = pd.DataFrame(paradise_sents + moby_sents) \n",
    "        if debug == True:\n",
    "            display(\"Here is a sample from sentences:\\n\", sentences.sample(20))  \n",
    "            print(\"sentences is a {} datatype.\".format(display_datatype(sentences)))\n",
    "            sentences.head(30)\n",
    "        \n",
    "        paradisewords = bag_of_words(paradise_doc)\n",
    "        mobywords = bag_of_words(moby_doc)\n",
    "        common_words = list(set(paradisewords + mobywords))\n",
    "        word_counts = bow_features(sentences, common_words)\n",
    "        \n",
    "        if debug == True:  display(\"words_counts sample:\\n\", word_counts.sample(20))\n",
    "        word_counts['author'] = np.where(word_counts['text_source'] == 'Milton', pd.to_numeric(1), pd.to_numeric(0))\n",
    "\n",
    "        X = np.array(word_counts.drop(['text_sentence', 'text_source', 'author'], 1))\n",
    "        Y = word_counts['author']\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.4, random_state=0)\n",
    "        \n",
    "        for runit in range(0,2):\n",
    "            if runit == 0:\n",
    "                \n",
    "                n_estimators = 100\n",
    "                \n",
    "                params = {'n_estimators': n_estimators}\n",
    "                printFormatted('## Running Random Forests with Word Counts')\n",
    "                run_rf(X_train=X_train, X_test=X_test, y_train=y_train, y_test=y_test, params=params, cross_validate=True)\n",
    "\n",
    "                printFormatted('## Running MultiNomial Naive Bayes with Word Counts')\n",
    "                params = {}\n",
    "                run_mnb(X_train=X_train, X_test=X_test, y_train=y_train, y_test=y_test, params=params, cross_validate=True)\n",
    "\n",
    "                printFormatted('## Running Bernoulli Naive Bayes with Word Counts')\n",
    "                run_BernoulliNB(X_train=X_train, X_test=X_test, y_train=y_train, y_test=y_test, params=params, cross_validate=True) \n",
    "       \n",
    "            else:\n",
    "            \n",
    "                author = []\n",
    "                all_paras = []\n",
    "                for paragraph in gutenberg.paras(paradise_book):\n",
    "                    para = paragraph[0]\n",
    "                    para = [re.sub(r'--', '', word) for word in para]\n",
    "                    all_paras.append(' '.join(para))\n",
    "                    author.append('Milton')\n",
    "\n",
    "                for paragraph in gutenberg.paras(moby_book):\n",
    "                    para = paragraph[0]\n",
    "                    para = [re.sub(r'--', '', word) for word in para]\n",
    "                    all_paras.append(' '.join(para))\n",
    "                    author.append('Melville')\n",
    "\n",
    "                paragraphs = pd.DataFrame()        \n",
    "                paragraphs['paragraphs'] = all_paras\n",
    "                paragraphs['author'] = author\n",
    "                paragraphs['author'] = np.where(paragraphs['author'] == 'Milton', pd.to_numeric(1), pd.to_numeric(0))\n",
    "                \n",
    "                if debug == True:\n",
    "                    display(\"paragraphs sampe:\\n\", paragraphs.sample(20))\n",
    "                    display(\"describe paragraphs\\n\", paragraphs.describe())\n",
    "\n",
    "                X = paragraphs['paragraphs']\n",
    "                Y = paragraphs['author']\n",
    "                printFormatted('## Running with Paragraph Counts and tfidf and others')\n",
    "\n",
    "                X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.4, random_state=0)\n",
    "                params = {}\n",
    "                tfidf_parms = {}\n",
    "                for vectorizer_iterator in ['rfc', 'bnb', 'mlb', 'svc']:\n",
    "\n",
    "                    path = 'A'\n",
    "                    if vectorizer_iterator == 'rfc':\n",
    "                        sentiment_analyzer(path=path, parameters=params, classifier=vectorizer_iterator, tfidf_parms=tfidf_parms, \n",
    "                                           X_train=X_train, X_test=X_test, y_train=y_train, y_test=y_test, cross_validate=True)\n",
    "\n",
    "                    elif vectorizer_iterator == 'bnb':\n",
    "                        parameters = {}\n",
    "                        sentiment_analyzer(path=path, parameters=params, classifier=vectorizer_iterator, tfidf_parms=tfidf_parms, \n",
    "                                           X_train=X_train, X_test=X_test, y_train=y_train, y_test=y_test, cross_validate=True)\n",
    "\n",
    "                    elif vectorizer_iterator == 'mlb':\n",
    "                        parameters = {}\n",
    "                        sentiment_analyzer(path=path, parameters=params, classifier=vectorizer_iterator, tfidf_parms=tfidf_parms, \n",
    "                                           X_train=X_train, X_test=X_test, y_train=y_train, y_test=y_test, cross_validate=True)\n",
    "                        \n",
    "                    elif vectorizer_iterator == 'logit': # newton-cg took too long. sag and saga about the same as lbfgs.\n",
    "                        tfidf_parms = {'max_features' : 10000 } # determined this through iterative testing\n",
    "                        \n",
    "                        parameters = {'C' :1e20, 'solver': 'lbfgs', 'max_iter': 1000} # max_iter=100 reports warning, try 1000\n",
    "                        sentiment_analyzer(path=path, parameters=parameters, classifier=vectorizer_iterator, tfidf_parms=tfidf_parms, \n",
    "                                           X_train=X_train, X_test=X_test, y_train=y_train, y_test=y_test, cross_validate=True)\n",
    "\n",
    "                    elif vectorizer_iterator == 'svc':\n",
    "                        parameters = {}\n",
    "                        sentiment_analyzer(path=path, parameters=parameters, classifier=vectorizer_iterator, tfidf_parms=tfidf_parms, \n",
    "                                           X_train=X_train, X_test=X_test, y_train=y_train, y_test=y_test, cross_validate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2777,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# main_nlp(0)\n",
    "# print(\"it ran ok, dude!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2778,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "if debug == 300:\n",
    "    import os\n",
    "    import pandas as pd\n",
    "    import nltk\n",
    "\n",
    "    from nltk.corpus.reader.plaintext import PlaintextCorpusReader\n",
    "\n",
    "    debug = 16\n",
    "    big_paras = []\n",
    "\n",
    "    corpusdir = '/Users/lou/GITHubProjects/Thinkful/Datafiles/UnsupervisedLearningCapstone/fiction_corpus/'\n",
    "    fiction_corpus = PlaintextCorpusReader(corpusdir, '.*.txt') \n",
    "    documents_stat = fiction_corpus.fileids()\n",
    "\n",
    "    paragraph_book = pd.DataFrame()\n",
    "    paragraph_list = []\n",
    "    chapter_heading = u\"^[C][Hh][aA][pP][tT][eE][Rr].*\"\n",
    "    chapter_first = u\"^[C][Hh][aA][pP][tT][eE][Rr] [I][O][F][1]\"\n",
    "\n",
    "    for book in documents_stat: #documents_stat:\n",
    "        print(\"Now processing book={}\".format(book))\n",
    "        paragraph_list = []\n",
    "        paragraph_number = -1 # used for page counting\n",
    "        EndDisplay = False\n",
    "        endparagraph = -1 # used for the end paragraph number\n",
    "        paragraphcounter = -1 # used to count the number of paragraphs in the book\n",
    "        chapter = 1\n",
    "        book_chapters = pd.DataFrame()\n",
    "        chapters = []\n",
    "        out_of_the_header = False\n",
    "        print(\"book={}\".format(book))\n",
    "        book_details=book.split('_')\n",
    "        print(\"book_details={}\".format(book_details))\n",
    "        author_number = int(int(book_details[0]) * 100)\n",
    "        book_number = int(int(book_details[2]) * 10)\n",
    "        author_booknumber = int(int(book_details[0]) * 100 + int(book_details[2]) * 10)\n",
    "        print(\"author_number={}, book_number={}, author_booknum={}\".format(author_number,\n",
    "                                                                           book_number,\n",
    "                                                                           author_booknumber))\n",
    "\n",
    "        # get the number of chapters in the book\n",
    "        for paras in fiction_corpus.paras(book): \n",
    "            paragraph = \" \".join(paras[0])\n",
    "            paragraphcounter += 1\n",
    "            if debug & 64:   \n",
    "                print(\"paragraphcounter={}\".format(paragraphcounter))\n",
    "                print(\"paragraph={}\".format(paragraph))\n",
    "\n",
    "            if u\"*** END\" in paragraph.upper():\n",
    "                endparagraph = paragraphcounter\n",
    "                print(\"*** THERE ARE {} chapters in the book {}\".format(endparagraph, book))\n",
    "                if debug:    \n",
    "                    print(\"endparagraph = {}\".format(endparagraph))\n",
    "                break\n",
    "\n",
    "        # iterate through all of the paragraphs in the current book\n",
    "        for paras in fiction_corpus.paras(book):\n",
    "    #         print(\"author={}, book={},chapter={}, len(chapters)={} at paras+1\".format(author_number, book_number, chapter, len(chapters)))\n",
    "            paragraph = \" \".join(paras[0])\n",
    "            paragraph_number += 1\n",
    "            if debug & 16:    print(\"debug 16:  paragraph_number now equals {}, paragraph={}\".format(paragraph_number, \n",
    "                                                                                                     paragraph.upper()))\n",
    "\n",
    "            if out_of_the_header is False and chapter_heading not in paragraph.upper():\n",
    "                if debug & 32:    print(\"xy23: paragraph_number={}, and paragraph={}\".format(paragraph_number, paragraph))\n",
    "                still_in_the_gutenburg_header = True\n",
    "                # ^[C][Hh][aA][pP][tT][eE][Rr]\n",
    "            elif (paragraph.upper() in u\"^[C][Hh][aA][pP][tT][eE][Rr] [I][1][O]\") or out_of_the_header is True: # first paragraph for the book\n",
    "                if u\"CHAPTER I\" in paragraph.upper():\n",
    "                    out_of_the_header = True\n",
    "                    if debug & 128:    print(\"We are out of chapter 0, here dude...\")\n",
    "\n",
    "    #             paragraph_number += 1 # we are on the next chapter....\n",
    "                if paragraph_number > endparagraph: # we have we gone past the end of the actual book,write to the book_chapters data frame\n",
    "                    print(\"\\n #### BREAK ENCOUNTERED !!!\\nAnd we are at paragraph_number={}, and we are in book:{}\\n\".format(paragraph_number,book))\n",
    "                    print(\"endparagraph = {}\".format(endparagraph))\n",
    "\n",
    "                    for chapter_item in chapters: \n",
    "                        book_chapters.append(chapter_item, ignore_index=True)\n",
    "                    print(\"we just processed {} chapter_items in chapters.\".format(len(chapters)))\n",
    "                    chapters = []\n",
    "    #                 break\n",
    "                else: # not in the last paragraph\n",
    "                    paragraph_list.append(paragraph)\n",
    "                    print(\"paragraph_list[0]={}\".format(paragraph_list[0]))\n",
    "                    if \"CHAPTER\" in paragraph.upper(): # this is the beginning of a new chapter - write the current chapter to the chapters list\n",
    "                        chapter += 1\n",
    "                        if debug:  print(\"chapter is now {}\".format(chapter))\n",
    "                        full_chapter_from_paragraph = ''\n",
    "                        for paragaph_item in paragraph_list:\n",
    "                            full_chapter_from_paragraph += \" \" + paragaph_item\n",
    "                        chapters = chapters.append([author_number, book_number, author_booknumber, chapter, full_chapter_from_paragraph ])\n",
    "                        print(\"aye just processed {} paragraph_list items.\".format(len(paragraph_list)))\n",
    "\n",
    "    print(\"The books have all\\nBeen loaded.\\nSir.\\nAnything else for the evening?\\n\")\n",
    "    paragraph_book.columns = ['author','book','author_book','chapter','chapter_text']\n",
    "    print(\"At the end, and there are {} lines in book_chapters\".format(book_chapters.shape))\n",
    "\n",
    "    display(book_chapters.head(30))\n",
    "    display(book_chapters.tail(30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2779,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(string): \n",
    "  \n",
    "    # punctuation marks \n",
    "    punctuations = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~'''\n",
    "\n",
    "    # traverse the given string and if any punctuation \n",
    "    # marks occur replace it with null \n",
    "    for x in string.lower(): \n",
    "        if x in punctuations: \n",
    "            string = string.replace(x, \"\")\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2780,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shoehorn_unicode_into_ascii(s):\n",
    "    return unicodedata.normalize('NFKD', s).encode('ascii','ignore').decode('utf-8', 'ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2781,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_whitespace(s):\n",
    "    s2 = str(s).replace(r'\\.|\\!|\\?|\\'|,|-|\\(|\\)',\" \")\n",
    "    s2.replace('\\n',' ').replace('\\r',' ')\n",
    "    s2 = [''.join(c for c in s if c not in string.punctuation) for s in s2]\n",
    "    return str(s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2782,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a module for Text Processing\n",
    "def text_process(tex):\n",
    "    # 1. Removal of Punctuation Marks\n",
    "    nopunct = [char for char in tex if char not in string.punctuation]\n",
    "    nopunct = ''.join(nopunct)\n",
    "    lemmatiser = WordNetLemmatizer()\n",
    "    # 2. Lemmatisation\n",
    "    a = ''\n",
    "    i = 0\n",
    "    for i in range(len(nopunct.split())):\n",
    "        b = lemmatiser.lemmatize(nopunct.split()[i], pos=\"v\")\n",
    "        a = a + b + ' '\n",
    "\n",
    "    #3 Removal of Stopwords\n",
    "     \n",
    "    return ' '.join([str(word) for word in a.split() if word.lower() not in stopwords.words('english')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2783,
   "metadata": {},
   "outputs": [],
   "source": [
    "def w2vec_vectorize_text(sentences=None, size=None, min_count=1, sg=1, seed=57,\n",
    "                         workers=4):\n",
    "    \n",
    "    m = Word2Vec(sentences=sentences, size=size, min_count=min_count,\n",
    "                 sg=sg, seed=seed)\n",
    "    \n",
    "    def word_vectorizer3(sent, m):\n",
    "        vec = []\n",
    "        numw = 0\n",
    "        for w in sent:\n",
    "            try:\n",
    "                if numw == 0:\n",
    "                    vec = m[w]\n",
    "                else:\n",
    "                    vec = np.add(vec, m[w])\n",
    "                numw += 1\n",
    "            except:\n",
    "                pass\n",
    "            return np.asarray(vec) / numw\n",
    "    \n",
    "    l = []\n",
    "\n",
    "    for i in tqdm(sentences):\n",
    "        l.append(word_vectorizer3(i, m))\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2784,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_nlp(entry_point=0):\n",
    "    \n",
    "    print_timestamp('\\n'*3+'Begin')\n",
    "    \n",
    "    debug = 2\n",
    "    big_paras = []\n",
    "\n",
    "    corpusdir = '/Users/lou/GITHubProjects/Thinkful/Datafiles/UnsupervisedLearningCapstone/fiction_corpus/'\n",
    "    fiction_corpus = PlaintextCorpusReader(corpusdir, '.*.txt') \n",
    "    documents_stat = fiction_corpus.fileids()\n",
    "    if debug == 11:    print(\"documents_stat={} and is a {} datatype\".format(documents_stat, type(documents_stat)))\n",
    "    documents_stat_0 = []\n",
    "    documents_stat_0.append(documents_stat[0])\n",
    "\n",
    "    if debug == 12:    print(\"documents_stat_0 is a {} datatype\".format(type(documents_stat)))\n",
    "    item_num = 0\n",
    "\n",
    "    book_block = []\n",
    "    word_counts= {}\n",
    "\n",
    "    for book in documents_stat:\n",
    "        item_num += 1\n",
    "\n",
    "        if debug== 13:    print(\"item_num={}\".format(item_num))\n",
    "    #    big_fiction = fiction_corpus.raw(book)\n",
    "        big_fiction_words = fiction_corpus.words(book)\n",
    "\n",
    "        if debug == 14:    print(\"big_fiction is a {} datatype\".format(type(big_fiction_words)))\n",
    "    #    big_fiction = str(big_fiction, 'utf-8', 'ignore')\n",
    "        big_fiction_len= len(big_fiction_words)\n",
    "        book_items = book.split('_')\n",
    "        author = book_items[0]\n",
    "        book_name = book_items[3]\n",
    "        wordcount1 = len(big_fiction_words)\n",
    "        if debug == 15:    print(\"big_fiction is words={}\".format(len(big_fiction_words)))\n",
    "\n",
    "        num_blocks = 128\n",
    "#         t_list = []\n",
    "        length = 500\n",
    "        interblock = 250\n",
    "        \n",
    "        for i in range(1,num_blocks):\n",
    "            if i == 1:\n",
    "                start =  1000\n",
    "                end = start + length\n",
    "            else:\n",
    "                start =  interblock + end\n",
    "                end = start + length \n",
    "    #         t_list.append((start, start+length))\n",
    "\n",
    "            big_fictionwords0 = big_fiction_words\n",
    "            if debug == 16:    print(\"big_fictionwords0 is a {} datatype\".format(type(big_fictionwords0)))\n",
    "\n",
    "    #         print(\"words {} thru {} are: {}\".format(2000, 3000, \" \".join(big_fiction_words[2000:3000])))\n",
    "            if debug == 17:    print(\"start={}, end={}\".format(start, end))\n",
    "            book_text = \" \".join(big_fiction_words[start:end]).lower()\n",
    "            book_text = remove_punctuation(book_text)\n",
    "#             book_text = text_process(book_text)\n",
    "            book_text = str(shoehorn_unicode_into_ascii(str(book_text)))\n",
    "\n",
    "\n",
    "\n",
    "            book_block.append([int(author), book_name, int(i), str(book_text)])\n",
    "            if debug == 18:  print(\"the length of zooga book_block is now {}\".format(len(book_block)))\n",
    "            if debug == 1 and i == 5:\n",
    "                print(\"book_name={}, book_text={}\".format(book_name, book_text))\n",
    "\n",
    "    if debug == 19:    print(\"book_block length is {}\".format(len(book_block)))\n",
    "\n",
    "    if debug == 20:    print(\"by the way, there are {} book_blocks.\".format(len(book_block)))\n",
    "        \n",
    "#     for book_block_item in book_block:\n",
    "#         if debug == 21:    print(\"author = {}, book={}, block={}.\".format(book_block_item[0],\n",
    "#                                                        book_block_item[1],\n",
    "#                                                        book_block_item[2]))\n",
    "#         book_block_key = str(book_block_item[0])+str(book_block_item[1]+str(book_block_item[2]))\n",
    "#         word_counts[book_block_key] = len(book_block_item[3].split(' '))\n",
    "\n",
    "    if debug == 22:\n",
    "        for book, word_counted in word_counts.items():\n",
    "            print(\"book={}, word_count = {}\".format(book, word_counted))\n",
    "\n",
    "#         with open(corpusdir + 'corpus_test.TXT', 'w') as corpustext:\n",
    "#             for file_lines in book_block:\n",
    "#                 corpustext.writelines(str(file_lines[0])+','+str(file_lines[1])+','+str(file_lines[2])+','+str(file_lines[3]+'\\n'))  \n",
    "\n",
    "        print(\"there were {} words in the last big_fiction.\".format(len(big_fiction_words)))\n",
    "        print(\"words {} thru {} are: {}\".format(2000, 3000, \" \".join(big_fiction_words[2000:3000])))\n",
    "    if debug == 23:  display(\"the length of book_block was {}\".format(len(book_block)))\n",
    "    \n",
    "    df_book_blocks = pd.DataFrame(book_block, columns=['author', 'book', 'book_block', 'block_text'] )\n",
    "    df_book_blocks['block_text_length'] = df_book_blocks['block_text'].apply(len)\n",
    "    display(\"## book_block:\", df_book_blocks.head(10))\n",
    "    \n",
    "#     run_it(X_train, X_test, y_train, y_test)\n",
    "    \n",
    "    df_book_blocks['block_text'].replace('', np.nan, inplace=True)\n",
    "    df_book_blocks.dropna(axis=1)\n",
    "    df_book_blocks.dropna(how='any', inplace=True)\n",
    "    df_book_blocks.block_text = df_book_blocks.block_text.str.strip()\n",
    "    df_book_blocks.block_text = df_book_blocks.block_text.str.replace('^ +| +$', '_')\n",
    "    df_book_blocks = df_book_blocks[df_book_blocks.block_text != '...']\n",
    "    df_book_blocks['block_text'].sample(20)\n",
    "#     display(\"null schtuff\\n:\", df_book_blocks.isnull().sum())\n",
    "#     display(\"df_book_blocks.columns\", df_book_blocks.columns)\n",
    "    # tfidf_X = run_tfidf_vectorizer(paragraph_book['paragraph'])\n",
    "    # display(\"tfidf_X is a {} datatype.\".format(type(tfidf_X)))\n",
    "    # display(tfidf_X)\n",
    "    X = df_book_blocks[['book_block', 'block_text']]\n",
    "    if debug:  display(\"shape of X is\", X.shape)\n",
    "    y = df_book_blocks['author']\n",
    "    if debug:  display(\"shape of y is\", y.shape)\n",
    "    # y.sample(20)\n",
    "    # X_train, X_test = train_test_split(tfidf_X)\n",
    "\n",
    "    vectorizer = TfidfVectorizer(\n",
    "#                                     max_df=0.5, \n",
    "#                                     min_df=2, \n",
    "#                                     stop_words='english', \n",
    "#                                     lowercase=True, \n",
    "#                                     use_idf=True, \n",
    "#                                     norm=u'l2', \n",
    "#                                     smooth_idf=True, \n",
    "                                    analyzer=text_process,\n",
    "                                    max_features=5000\n",
    "    )\n",
    "    #                              max_df=max_df, # drop words that occur in more than half the paragraphs\n",
    "    #                              min_df=min_df, # only use words that appear at least twice\n",
    "    #                              stop_words=stop_words, \n",
    "    #                              lowercase=lowercase, #convert everything to lower case (since Alice in Wonderland has the HABIT of CAPITALIZING WORDS for EMPHASIS)\n",
    "    #                              use_idf=use_idf,#we definitely want to use inverse document frequencies in our weighting\n",
    "    #                              norm=norm, #Applies a correction factor so that longer paragraphs and shorter paragraphs get treated equally\n",
    "    #                              smooth_idf=smooth_idf #Adds 1 to all document frequencies, as if an extra document existed that used every word once.  Prevents divide-by-zero errors\n",
    "    #                             )\n",
    "\n",
    "    #Applying the vectorizer\n",
    "\n",
    "    # tfidf_df = vectorizer.fit(df)\n",
    "    display(X['block_text'].head(20))\n",
    "    \n",
    "    display(\"the length of X is {}\".format(X.size))\n",
    "    print(\"here comes {} sample rows from X\".format(10))\n",
    "    display(X.sample(10))\n",
    "    model = gensim.models.Word2Vec(X.block_text, min_count=1, size=50)\n",
    "    \n",
    "    X1 = model.wv.vectors\n",
    "    print(\"############### the length of X1 from model.wv.syn0 is {} ###################, and the type is {}, and shape is {}\".format(len(X1), type(X1), X1.shape))\n",
    "#     print(\"here comes X1\")\n",
    "#     display(X1)        \n",
    "            \n",
    "    NUM_CLUSTERS = 10\n",
    "#     kclusterer = KMeansClusterer(NUM_CLUSTERS, distance=nltk.cluster.util.cosine_distance, repeats=25)\n",
    "#     assigned_clusters = kclusterer.cluster(X, assign_clusters=True)\n",
    "#     print(\"assigned clusters:\\n\", assigned_clusters)\n",
    "    \n",
    "    kmeans = KMeans(n_clusters=NUM_CLUSTERS)\n",
    "#     kmeans_fit = kmeans.fit(X)\n",
    "    kmeans_fit_predict = kmeans.fit_predict(X1)\n",
    "    print(\"kmeans_fit_predict:\\n\", kmeans_fit_predict)\n",
    "    display(\"size of kmeans_fit_predict is {}\".format(len(kmeans_fit_predict)))\n",
    "    print(\"We will now exit.\")\n",
    "    exit(-1)\n",
    "#     X_train, X_test, y_train, y_test = training_test_set(X, y) # split the dataframe\n",
    "#     display(\"X.sample(20)\", X.sample(20))\n",
    "    if debug == 24:\n",
    "        display(\"shape of X_train() is\", X_train.shape)\n",
    "        display(\"shape of X_test is\", X_test.shape)\n",
    "        display(\"X_train.head(20):\", X_train.head(20))\n",
    "#     tfidf_train = vectorizer.fit_transform(X_train['block_text']) # 1A: run fit and transform on the training data set\n",
    "#     tfidf_test_trans = vectorizer.transform(X_test['block_text']) # 1B: now run transoform on the test data set\n",
    "\n",
    "#     tfidf_train_df = pd.DataFrame(tfidf_train.toarray())# , columns=vectorizer.get_feature_names())\n",
    "#     tfidf_test_df = pd.DataFrame(tfidf_test_trans.toarray())# , columns=vectorizer.get_feature_names())\n",
    "#     display(\"tfidf_train_df shape is {}\".format(tfidf_train_df.shape))\n",
    "\n",
    "    K = 10\n",
    "#     if debug:\n",
    "#         print(\"tfidf_train_df shape is {}\".format(tfidf_train_df.shape))\n",
    "\n",
    "#     bow_transformer = CountVectorizer(analyzer=text_process).fit(X_train)\n",
    "#     text_bow_train = bow_transformer.transform(X_train) # ONLY TRAINING DATA\n",
    "    \n",
    "#     text_bow_test = bow_transformer.transform(X_test) #TEST DATA only\n",
    "    \n",
    "    word2vec_transformer = vectorizer =  word2vec_function(X['block_text'], workers=4, min_count=10, window=6, sg=0, sample=1e-3, size=300, hs=1).fit(X_train) # ONLY TRAINING DATA\n",
    "    word2vec_test = word2vec_transformer.transformer(X) # TEST DATA only\n",
    "    chapters = X['block_text']\n",
    "    model - Word2Vec(chapters, min_count=1)\n",
    "    \n",
    "#     kmeans_cluster_test =  KMeans(n_clusters=K,max_iter=1000, random_state=42).fit_predict(tfidf_test_df)\n",
    "#     kmeans_cluster_train = KMeans(n_clusters=K,max_iter=1000, random_state=42).fit_predict(tfidf_train_df)\n",
    "#     print(\"length of text_bow_test is {}\".format(len(text_bow_test.toarray())))\n",
    "#     kmeans_cluster_test =  KMeans(n_clusters=K, random_state=42).fit_predict(text_bow_train.T) #\n",
    "    kmeans_cluster_test =  KMeans(n_clusters=K, random_state=42).fit_predict(word2vec_test) #\n",
    "    \n",
    "    if debug == 25:\n",
    "        print(\"tfidf_train_df is a {} datatype\".format(type(tfidf_train_df)))\n",
    "    cluster_series_test = pd.Series(kmeans_cluster_test)\n",
    "    cluster_series_train = pd.Series(kmeans_cluster_train)\n",
    "\n",
    "    if debug == 26:\n",
    "        print(\"the shape of cluster_series_test is {}\".format(cluster_series_test.shape))\n",
    "        print(\"the shape of cluster_series_train is {}\".format(cluster_series_train.shape))\n",
    "\n",
    "    # need to add in all of the other columns along with the new features, and the cluster.....\n",
    "#     display(\"cluster_series_test head(20)\", cluster_series_test.head(20))\n",
    "    Z_test = pd.merge(cluster_series_test.rename('cluster'),tfidf_test_df,left_index=True, right_index=True)\n",
    "\n",
    "    Z_test2 = pd.merge(X, Z_test,left_index=True, right_index=True)\n",
    "    Z_test2 = pd.merge(y, Z_test2, left_index=True, right_index=True)\n",
    "    Z_test.rename(columns={Z_test.columns[0]: 'cluster'}, inplace=True)\n",
    "    if debug == 27:\n",
    "        print(\"shape of {} is {}, shape of {} is {}, shape of {} is {}\".format(\"tfidf_test_df\", \n",
    "                                                                           tfidf_test_df.shape, \n",
    "                                                                           \"kmeans_cluster_test\", \n",
    "                                                                           kmeans_cluster_test.shape,\n",
    "                                                                           \"Z_test\", Z_test.shape))\n",
    "\n",
    "    Z_train = pd.merge(cluster_series_train.rename('cluster'),tfidf_train_df,left_index=True, right_index=True)\n",
    "    Z_train.rename(columns={Z_train.columns[0]: 'cluster'}, inplace=True)\n",
    "    if debug == 28:\n",
    "        print(\"shape of {} is {}, shape of {} is {}, shape of {} is {}\".format(\"tfidf_train_df\", \n",
    "                                                                           tfidf_train_df.shape, \n",
    "                                                                           \"kmeans_cluster_train\", \n",
    "                                                                           kmeans_cluster_train.shape,\n",
    "                                                                           \"Z_train\", Z_train.shape))\n",
    "    # Z2 = Z.join(X)\n",
    "    # Z2 = pd.merge(Z, X, left_index=True, right_index=True)\n",
    "    # display(Z.sample(20))\n",
    "    if debug == 29:\n",
    "        display(\"shape of Z_test is {}, shape of X is {}\".format(Z_test.shape, X.shape))\n",
    "        display(Z_test.sample(20))\n",
    "    if flag_to_run_elbow_plot == True:\n",
    "        do_the_elbow(Z_test, maxK=20) # skip the author column\n",
    "\n",
    "    if debug == 30:\n",
    "        display(\"shape of Z_train is {}, shape of X is {}\".format(Z_train.shape, X.shape))\n",
    "        display(Z_train.sample(20))\n",
    "\n",
    "    # X_trainZ, X_testZ, y_trainZ, y_testZ = train_test_split(Z.loc[:, ~Z.columns.isin(['author', ''])) # split the dataframe\n",
    "    # df.loc[:, df.columns != 'b']\n",
    "    # df.loc[:, ~df.columns.isin(['col1', 'col2'])]\n",
    "\n",
    "                # display(Z[['cluster']].sample(20))\n",
    "    # Z = pd.merge(Z, pd.DataFrame(predict), left_index=True, right_index=True)\n",
    "    # Z.rename(columns={Z.columns[-1]: 'cluster'}, inplace=True)\n",
    "    # predicted = cluster.predict(Z['cluster'])\n",
    "    # predicted = cluster_series(X_test)\n",
    "    # predicted = cluster.predict(Z)\n",
    "    # display(predicted.head(20))\n",
    "    print(\"### NOW GET OUT OF HERE, YOU KNUCKLEHEAD!!! ###\")\n",
    "    display(Z_test2.head(40))\n",
    "    print_timestamp('\\n'*3+'End')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2785,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_csv(dataframe, filename):\n",
    "    try:\n",
    "        dataframe.to_csv(filename, sep='|', encoding='utf-8')\n",
    "        print(\"the dataframe {} was successfully written to file {}\".format(dataframe))\n",
    "    except:\n",
    "        print(\"Writing dataframe to csv in file {} failed.\".format(filename))\n",
    "    finally:\n",
    "        print(\"thank you for your service.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2786,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_pd_read_back(df=None, debugdir=None):\n",
    "    \n",
    "    randomnumber = random.randint(1,101000)\n",
    "    filename = debugdir + 'tempfilename{}.csv'.format(randomnumber)\n",
    "    print(\"in write_pd_read_back, writing to filename={}\".format(filename))\n",
    "    df.to_csv(filename, sep='|', encoding='utf-8')\n",
    "    \n",
    "    print(\"checkpoint 2.0: shape of dataframe in write_pd_read_back is {}.\".format(df.shape))    \n",
    "    print(\"filename = {}\".format(filename))\n",
    "    df2 = pd.read_csv(filename, sep='|', encoding='utf-8,')\n",
    "    \n",
    "    if debug == False:\n",
    "        os.remove(filename) \n",
    "        \n",
    "    return df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2787,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_nlp_new(entry_point=0):\n",
    "    \n",
    "    print_timestamp('\\n'*3+'Begin')\n",
    "    num_features = 10000\n",
    "    \n",
    "    print(\"We are running with {} features.\".format(num_features))\n",
    "    \n",
    "    current_platform = platform.system()\n",
    "    print(\"platform.system = {}\".format(platform.system()))\n",
    "        \n",
    "    debug = 2\n",
    "    big_paras = []\n",
    "\n",
    "    if current_platform == 'Darwin':\n",
    "        corpusdir = r\"/Users/lou/GITHubProjects/Thinkful/Datafiles/UnsupervisedLearningCapstone/fiction_corpus\"\n",
    "        debugdir = r'/Users/lou/GITHubProjects/Thinkful/HomeworkProjects/Unit4/Lesson5/Project1/'\n",
    "    else:\n",
    "        corpusdir = r\"..\\..\\..\\..\\Datafiles\\UnsupervisedLearningCapstone\\fiction_corpus\"\n",
    "        debugdir = r'C:\\\\Users\\\\10267347\\\\Downloads\\\\'\n",
    "        \n",
    "    fiction_corpus = PlaintextCorpusReader(corpusdir, '.*.txt', encoding='utf-8')\n",
    "    documents_stat = fiction_corpus.fileids()\n",
    "    if debug == 11:    print(\"documents_stat={} and is a {} datatype\".format(documents_stat, type(documents_stat)))\n",
    "    documents_stat_0 = []\n",
    "    documents_stat_0.append(documents_stat[0])\n",
    "\n",
    "    if debug == 12:    print(\"documents_stat_0 is a {} datatype\".format(type(documents_stat)))\n",
    "    item_num = 0\n",
    "\n",
    "    book_block = []\n",
    "    word_counts= {}\n",
    "\n",
    "    for book in documents_stat:\n",
    "        item_num += 1\n",
    "\n",
    "        if debug== 13:    print(\"item_num={}\".format(item_num))\n",
    "    #    big_fiction = fiction_corpus.raw(book)\n",
    "        big_fiction_words = fiction_corpus.words(book)\n",
    "\n",
    "        big_fiction_len= len(big_fiction_words)\n",
    "        book_items = book.split('_')\n",
    "        author = book_items[0]\n",
    "        book_name = book_items[3]\n",
    "        wordcount1 = len(big_fiction_words)\n",
    "        if debug == 15:    print(\"big_fiction is words={}\".format(len(big_fiction_words)))\n",
    "\n",
    "        num_blocks = 128\n",
    "#         t_list = []\n",
    "        length = 500\n",
    "        interblock = 250\n",
    "        \n",
    "        for i in range(1,num_blocks):\n",
    "            if i == 1:\n",
    "                start =  1000\n",
    "                end = start + length\n",
    "            else:\n",
    "                start =  interblock + end\n",
    "                end = start + length \n",
    "    #         t_list.append((start, start+length))\n",
    "\n",
    "            big_fictionwords0 = big_fiction_words\n",
    "            if debug == 16:    print(\"big_fictionwords0 is a {} datatype\".format(type(big_fictionwords0)))\n",
    "\n",
    "    #         print(\"words {} thru {} are: {}\".format(2000, 3000, \" \".join(big_fiction_words[2000:3000])))\n",
    "            if debug == 17:    print(\"start={}, end={}\".format(start, end))\n",
    "            book_text = \" \".join(big_fiction_words[start:end]).lower()\n",
    "            book_text = remove_punctuation(book_text)\n",
    "#             book_text = text_process(book_text)\n",
    "            book_text = str(shoehorn_unicode_into_ascii(str(book_text)))\n",
    "\n",
    "\n",
    "\n",
    "            book_block.append([int(author), book_name, int(i), str(book_text)])\n",
    "    \n",
    "    df_book_blocks = pd.DataFrame(book_block, columns=['author', 'book', 'book_block', 'block_text'] )\n",
    "    \n",
    "#     display(\"## book_block:\", df_book_blocks.head(10))\n",
    "    \n",
    "#     run_it(X_train, X_test, y_train, y_test)\n",
    "    \n",
    "    df_book_blocks['block_text'].replace('', np.nan, inplace=True)\n",
    "    df_book_blocks.dropna(axis=1)\n",
    "    df_book_blocks.dropna(how='any', inplace=True)\n",
    "    df_book_blocks.block_text = df_book_blocks.block_text.str.strip()\n",
    "    df_book_blocks.block_text = df_book_blocks.block_text.str.replace('^ +| +$', '_')\n",
    "    df_book_blocks = df_book_blocks[df_book_blocks.block_text != '...']\n",
    "    df_book_blocks['block_text'].sample(20)\n",
    "#     df_book_blocks[\"id\"] = df_book_blocks.index + 1\n",
    "#     df['salary'] = df['SAL-RATE'].apply(money_to_float)\n",
    "    df_book_blocks['block_text_clean'] = df_book_blocks['block_text'].apply(text_process)\n",
    "    df_book_blocks['block_text_length'] = df_book_blocks['block_text_clean'].apply(len)\n",
    "    columnsTitles = ['author', 'book', 'book_block', 'block_text_clean', 'block_text_length']\n",
    "    df_book_blocks = df_book_blocks.reindex(columns=columnsTitles)\n",
    "#     df_book_blocks.set_index('id')\n",
    "    \n",
    "#     print(\"checkpoint 1: shape of df_book_blocks is {}\".format(df_book_blocks.shape))\n",
    "    X = df_book_blocks[['book_block', 'block_text_clean']]\n",
    "    if debug:  display(\"shape of X is\", X.shape)\n",
    "    y = df_book_blocks['author']\n",
    "    if debug:  display(\"shape of y is\", y.shape)\n",
    "    \n",
    "    print_timestamp('\\n'*3+'Starting w2vec_vectorize_text...')\n",
    "#     print(\"checkpoint 2.000 before stuff: shape of df_book_blocks is {}\".format(df_book_blocks.shape))    \n",
    "    l3 = w2vec_vectorize_text(sentences=df_book_blocks['block_text_clean'], size=num_features, min_count=1,sg=1, seed=57, workers=4)\n",
    "#     print(\"checkpoint 2.001 after merge: length of l3 is {}\".format(len(l3)))\n",
    "    lower_scaled_value = 0\n",
    "    upper_scaled_value = 1\n",
    "    scaler = MinMaxScaler(feature_range=(lower_scaled_value,upper_scaled_value))\n",
    "    l3_scaled = scaler.fit_transform(l3)\n",
    "#     print(\"checkpoint 2.002 after merge: length of l3 is {}\".format(len(l3)))\n",
    "    print_timestamp('\\n'*3+'Done with w2vec_vectorize_text...')\n",
    "\n",
    "    X1 = pd.DataFrame(np.array(l3_scaled))\n",
    "#     X1 = X1.astype(int)\n",
    "\n",
    "    df_book_blocks.reset_index(inplace=True)\n",
    "#     print(\"checkpoint 2.0: shape of df_book_blocks is {}, and X1 is {}\".format(df_book_blocks.shape,\n",
    "#                                                                               X1.shape))\n",
    "\n",
    "    df_book_blocks = pd.merge(left=df_book_blocks, right=X1, left_index=True, right_index=True ) #how='inner', on='id')\n",
    "    column_names_new = {}\n",
    "    for i in range(0,num_features):\n",
    "        column_names_new[i]='w2v_{}'.format(i)\n",
    "    df_book_blocks.rename(columns=column_names_new, inplace=True)\n",
    "#     print(\"checkpoint 2.2 after merge: shape of df_book_blocks is {}\".format(df_book_blocks.shape))\n",
    "    \n",
    "    print_timestamp('\\n'*3+'Starting tfidf_vectorize_text...')\n",
    "    \n",
    "    parms1 = {}\n",
    "    parms1[\"max_features\"] = num_features\n",
    "    l4 = run_tfidf_vectorizer2(df_book_blocks['block_text_clean'], parameters=parms1)\n",
    "    l4_scaled = scaler.fit_transform(l4)\n",
    "    df4 = pd.DataFrame(l4_scaled)\n",
    "#     df4 = df4.astype(int)\n",
    "    \n",
    "    column_names_new = {}\n",
    "    for i in range(0,num_features):\n",
    "        column_names_new[i]='tfidf_{}'.format(i)\n",
    "    df4.rename(columns=column_names_new, inplace=True)\n",
    "#     print(\"checkpoint 2.3 after merge: shape of df_book_blocks is {}\".format(df_book_blocks.shape))\n",
    "    \n",
    "    print_timestamp('\\n'*3+'Done with tfidf_vectorize_text...')\n",
    "    df_book_blocks = pd.merge(left=df_book_blocks, right=df4, left_index=True, right_index=True) # how='inner', on='id')\n",
    "\n",
    "    if debug == True:\n",
    "        print(\"columns for df_book_blocks are:\")\n",
    "        print(df_book_blocks.columns)\n",
    "        \n",
    "    print(\"Now we are getting the cluster from tfidf...\")\n",
    "\n",
    "#     df_w2v = df_book_blocks.iloc[:, 5 : num_features + 4]\n",
    "    kmeans_parameters = { \"n_clusters\": num_clusters\n",
    "                         ,\"precompute_distances\": True \n",
    "                         ,\"random_state\": 57\n",
    "                         ,\"init\": \"random\"\n",
    "                         ,\"n_init\": 10\n",
    "                         ,\"max_iter\": 300\n",
    "                         ,\"tol\": 1e-04 \n",
    "                 }\n",
    "#     cluster_w2v = run_kmeans(df_book_blocks.iloc[:, -num_features*2 : -num_features], parameters=kmeans_parameters)\n",
    "    w2v_df = df_book_blocks.iloc[:, -num_features*2 : -num_features]\n",
    "    tfidf_df = df_book_blocks.iloc[:, -num_features :]\n",
    "    \n",
    "#     print(\"Here comes w2v_df - 20 rows\")\n",
    "#     display(w2v_df.head(20))\n",
    "    \n",
    "#     print(\"Here comes tfidf_df - 20 rows\")\n",
    "#     display(tfidf_df.head(20))\n",
    "    \n",
    "    columnnum = 4\n",
    "    for cluster in ['km', 'ms', 'sc', 'ap']:\n",
    "        columnnum += 1     \n",
    "        print_timestamp(\"Now working on w2v, cluster type: {}\".format(cluster))\n",
    "        parameters = {}\n",
    "        if cluster == 'km':\n",
    "            parameters = kmeans_parameters\n",
    "            cluster_w2v = run_kmeans(w2v_df, parameters=kmeans_parameters)\n",
    "        elif cluster == 'ap':\n",
    "            parameters = {}\n",
    "            cluster_w2v, num_cluster_returned = run_affinity_propagation(w2v_df, parameters=parameters)\n",
    "        elif cluster == 'ms':\n",
    "            parameters = {}\n",
    "            cluster_w2v, num_clusters_returned = run_mean_shift(w2v_df, parameters=parameters)\n",
    "        elif cluster == 'sc':\n",
    "            parameters = {\"n_clusters\": num_clusters}\n",
    "            cluster_w2v = run_spectral_clustering(w2v_df, parameters=parameters)\n",
    "\n",
    "        df_book_blocks.insert(columnnum,'cluster_w2v_{}'.format(cluster), cluster_w2v)\n",
    "        \n",
    "        print(\"now completing cluster type of {}\".format(cluster))\n",
    "#         print(\"Here comes w2v_df - 20 rows\")\n",
    "#         display(w2v_df.head(20))\n",
    "\n",
    "#         print(\"Here comes tfidf_df - 20 rows\")\n",
    "#         display(tfidf_df.head(20))\n",
    "\n",
    "        \n",
    "    cluster_df = df_book_blocks.iloc[:, -num_features :]\n",
    "#     cluster_tfidf = run_kmeans(df_book_blocks.iloc[:, num_features + 5 : num_features*2 + 4], parameters=kmeans_parameters)\n",
    "#     cluster_tfidf = run_kmeans(df_book_blocks.iloc[:, -num_features :], parameters=kmeans_parameters)\n",
    "#     df_book_blocks.insert(6,'cluster_tfidf', cluster_tfidf) \n",
    "\n",
    "    prefix = 'clus_tfidf_'\n",
    "    for cluster in ['km', 'ms', 'sc', 'ap']:\n",
    "        columnnum += 1 \n",
    "        print_timestamp(\"Now working on tfidf, cluster type: {}\".format(cluster))\n",
    "        \n",
    "        if cluster == 'km':\n",
    "            parameters = kmeans_parameters\n",
    "            cluster_tfidf = run_kmeans(tfidf_df, parameters=kmeans_parameters)\n",
    "        elif cluster == 'ap':\n",
    "            parameters = {}            \n",
    "            cluster_tfidf, num_clusters_returned = run_affinity_propagation(tfidf_df, parameters=parameters)\n",
    "        elif cluster == 'ms':\n",
    "            parameters = {}\n",
    "            cluster_tfidf, num_clusters_returned = run_mean_shift(tfidf_df, parameters=parameters)\n",
    "        elif cluster == 'sc':\n",
    "            parameters = {\"n_clusters\": num_clusters}\n",
    "            cluster_tfidf = run_spectral_clustering(tfidf_df, parameters=parameters)\n",
    "\n",
    "        df_book_blocks.insert(columnnum,'{}_{}'.format(prefix, cluster), cluster_tfidf)\n",
    "\n",
    "    print(\"here are 20 random rows from df_book_blocks\")\n",
    "#     display(df_book_blocks[['author', 'book', 'book_block', 'block_text_length', 'cluster_w2v', 'cluster_tfidf']].sample(20))\n",
    "    display(df_book_blocks.sample(20))\n",
    "    \n",
    "    # cluster_w2v_km\tcluster_w2v_ms\tcluster_w2v_sc\tcluster_w2v_ap\n",
    "#     df_book_blocks['w2v_km'] = np.where(df_book_blocks['author'] == df_book_blocks['cluster_w2v_km'], 1,0).astype(int)\n",
    "#     df_book_blocks['w2v_ms'] = np.where(df_book_blocks['author'] == df_book_blocks['cluster_w2v_ms'], 1,0).astype(int)\n",
    "#     df_book_blocks['w2v_sc'] = np.where(df_book_blocks['author'] == df_book_blocks['cluster_w2v_sc'], 1,0).astype(int)\n",
    "#     df_book_blocks['w2v_ap'] = np.where(df_book_blocks['author'] == df_book_blocks['cluster_w2v_ap'], 1,0).astype(int)\n",
    "    \n",
    "#     df_tips.query(\"sex=='Male'\")['total_bill'].mean()\n",
    "#     total_correct_w2v_km = df_tips.query(\"sex=='Male'\")['total_bill'].mean()\n",
    "#     for col in ['w2v_km', 'w2v_ms', 'w2v_sc', 'w2v_ap']:\n",
    "#         display(\"for column {}\".format(col), df_book_blocks.groupby(by=col).count())\n",
    "    display(\"This is all folks.  And now back to you, Johnny!\")\n",
    "    return df_book_blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "2019-10-06 22:37:04: In: main_nlp_new \n",
       "\n",
       "\n",
       "Begin "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are running with 10000 features.\n",
      "platform.system = Darwin\n",
      "checkpoint 1: shape of df_book_blocks is (2309, 5)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'shape of X is'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(2309, 2)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'shape of y is'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(2309,)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "2019-10-06 22:39:53: In: main_nlp_new \n",
       "\n",
       "\n",
       "Starting w2vec_vectorize_text... "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = main_nlp_new(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"author correct counts for w2v_kmeans is {}\".format(df[df['cluster_w2v_km'] == df['author']['book'].count()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_w2_km = df[df.cluster_w2v_km == df.author]['book'].count()\n",
    "x_w2_ms = df[df.cluster_w2v_ms == df.author]['book'].count()\n",
    "x_w2_sc = df[df.cluster_w2v_sc == df.author]['book'].count()\n",
    "x_w2_ap = df[df.cluster_w2v_ap == df.author]['book'].count()\n",
    "print(\"w2v Kmeans correct percentage = {:g}%\".format(x_w2_km/df.shape[0]))\n",
    "print(\"w2v Mean shift correct percentage = {:g}%\".format(x_w2_ms/df.shape[0]))\n",
    "print(\"w2v Spectral Clustering correct percentage = {:g}%\".format(x_w2_sc/df.shape[0]))\n",
    "print(\"w2v Affinity Propagation correct percentage = {:g}%\".format(x_w2_ap/df.shape[0]))\n",
    "print(\"length of df is {}\".format(df.shape[0]))\n",
    "print(\"x_w2_km={}, x_w2_ms={}, x_w2_sc={}, x_w2_ap={}\".format(x_w2_km,\n",
    "                                                              x_w2_ms,\n",
    "                                                              x_w2_sc,\n",
    "                                                              x_w2_ap))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[df.cluster_w2v_km == df.author][['book','author','cluster_w2v_km']]\n",
    "for i in ['cluster_w2v_km','cluster_w2v_ms','cluster_w2v_sc','cluster_w2v_ap']:\n",
    "    display(df[df[i] == df.author][['author',i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if debug == 39:\n",
    "\n",
    "    if platform.system() == 'Darwin':\n",
    "        path = r'/Users/lou/GITHubProjects/Thinkful/HomeworkProjects/Unit4/Lesson5/Project1/debug/'\n",
    "    else:\n",
    "        path = r'C:\\\\Users\\\\10267347\\\\Downloads\\\\'\n",
    "\n",
    "    filename1=path+'df_book_blocks_pre.csv'\n",
    "    filename2=path+'X1_pre.csv'\n",
    "\n",
    "    print(\"filename1 = {}\".format(filename1))\n",
    "    df_book_blocks_csv = pd.read_csv(filename1, sep='|', encoding='utf-8,')\n",
    "\n",
    "    print(\"filename2 = {}\".format(filename2))\n",
    "    X1_csv = pd.read_csv(filename2, sep='|', encoding='utf-8')\n",
    "\n",
    "    print(\"the shape of df_book_blocks_csv is {}\".format(df_book_blocks_csv.shape))\n",
    "    print(\"the shape of X1_csv is {}\".format(X1_csv.shape))\n",
    "\n",
    "    df_book_blocks_csv = pd.merge(left=df_book_blocks_csv, right=X1_csv, left_index=True, right_index=True)\n",
    "\n",
    "    print(\"the shape of df_book_blocks_merge_X1 is {}\".format(df_book_blocks_csv.shape))\n",
    "\n",
    "    display(df_book_blocks_csv.head(20))\n",
    "    display(df_book_blocks_csv.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Summary of result runs\n",
    "\n",
    "|Model|NLP Feature Generator|Training Score|:Test Score|Metrics Accuracy|Accuracy of Model|Status|\n",
    "|:----|:--------------------|:-------------|:----------|:---------------|:----------------|:----|\n",
    "|Random Forests|Word Counts|98.37|86.59|86.59|82.05| |\n",
    "|Multinomial Naive Bayes|Word Counts|97.55|92.28|92.28|88.58| |\n",
    "|Bernoulli Naive Bayes|Word Counts|82.34|83.33|83.33|76.09| |\n",
    "|Random Forests|tfidf|99.76|98.76|98.76|99.11|**Winner** |\n",
    "|Multinomial Naive Bayes|tfidf|99.11|98.76|98.76|99.11|**Winner** |\n",
    "|Bernoulli Naive Bayes|tfidf|99.05|98.76|98.76|99.11|**Winner**|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Todos:\n",
    "columnsTitles = ['onething', 'secondthing', 'otherthing']\n",
    "\n",
    "frame = frame.reindex(columns=columnsTitles)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
