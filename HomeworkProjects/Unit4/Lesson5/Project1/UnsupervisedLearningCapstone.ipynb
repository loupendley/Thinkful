{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1153,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "url = 'https://www.thinkful.com'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsupervised Learning Capstone: Build you own NLP model  \n",
    "\n",
    "For this challenge, you will need to choose a corpus of data from nltk or another source that includes categories you can predict and create an analysis pipeline that includes the following steps:\n",
    "\n",
    "For this project you'll dig into a large amount of text and apply most of what you've covered in this unit and in the course so far.\n",
    "\n",
    "1.  First, pick a set of texts. This can be either a series of novels, chapters, or articles. Anything you'd like. It just has to have multiple entries of varying characteristics. At least 100 should be good. There should also be at least 10 different authors, but try to keep the texts related (either all on the same topic of from the same branch of literature - something to make classification a bit more difficult than obviously different subjects).\n",
    "\n",
    "This capstone can be an extension of your NLP challenge if you wish to use the same corpus. If you found problems with that data set that limited your analysis, however, it may be worth using what you learned to choose a new corpus. Reserve 25% of your corpus as a test set.\n",
    "\n",
    "The first technique is to create a series of clusters. Try several techniques and pick the one you think best represents your data. Make sure there is a narrative and reasoning around why you have chosen the given clusters. Are authors consistently grouped into the same cluster?\n",
    "\n",
    "2.  Next, perform some unsupervised feature generation and selection using the techniques covered in this unit and elsewhere in the course. Using those features then build models to attempt to classify your texts by author. Try different permutations of unsupervised and supervised techniques to see which combinations have the best performance.\n",
    "\n",
    "3.  Lastly return to your holdout group. Does your clustering on those members perform as you'd expect? Have your clusters remained stable or changed dramatically? What about your model? Is it's performance consistent?\n",
    "\n",
    "If there is a divergence in the relative stability of your model and your clusters, delve into why.\n",
    "\n",
    "Your end result should be a write up of how clustering and modeling compare for classifying your texts. What are the advantages of each? Why would you want to use one over the other? Approximately 3-5 pages is a good length for your write up, and remember to include visuals to help tell your story!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My authors:\n",
    "\n",
    "1. John Buchan\n",
    "    2. The Thirty-nine Steps (10 chapters) URL: http://www.gutenberg.org/cache/epub/558/pg558.txt\n",
    "    2. Mr. Standfast (22 chapters) URL: https://www.gutenberg.org/ebooks/560.txt.utf-8\n",
    "    \n",
    "1. Edgar Rice Burroughs\n",
    "   2. Tarzan of the Apes (28 chapters) URL: https://www.gutenberg.org/ebooks/78.txt.utf-8\n",
    "   2. Son of Tarzan (27 chapters) URL: https://www.gutenberg.org/ebooks/90.txt.utf-8\n",
    "   \n",
    "1. Alexander Dumas\n",
    "   2. The Count of Monte Cristo (117 chapters) URL: https://www.gutenberg.org/files/1184/1184-0.txt\n",
    "   2. The Three Muskateers (66 chapters) URL: https://www.gutenberg.org/files/1257/1257-0.txt\n",
    "   \n",
    "1. H. Rider Haggard\n",
    "   2. King Solomon's Mines (20 chapters) URL: https://www.gutenberg.org/ebooks/2166.txt.utf-8\n",
    "   2. Finished (23 chapters) https://www.gutenberg.org/ebooks/1724.txt.utf-8\n",
    "\n",
    "1. Anthony Hope\n",
    "   2. The Prisoner of Zennda (22 chapters) URL: https://www.gutenberg.org/files/95/95-0.txt\n",
    "   2. Rupert of Hentzau (21 chapters) URL: https://www.gutenberg.org/files/1145/1145-0.txt\n",
    "   \n",
    "1. Rudyard Kipling\n",
    "   2. Kim (15 chapters) URL: https://www.gutenberg.org/ebooks/2226.txt.utf-8\n",
    "   2. Captains Courageous (10 chapters) URL: https://www.gutenberg.org/ebooks/2225.txt.utf-8\n",
    "   \n",
    "1. Talbot Mundy\n",
    "   2. King of the Kyber Rifles (28 chapters) URL: https://www.gutenberg.org/files/6066/6066-0.txt\n",
    "   2. Lion of Petra (13 chapters) URL: https://www.gutenberg.org/ebooks/19307.txt.utf-8\n",
    "   \n",
    "1. Baroness Orczy\n",
    "   2. The Scarlet Pimpernel (31 chapters) URL: https://www.gutenberg.org/ebooks/60.txt.utf-8\n",
    "   2. The Elusive Pimpernel (35 chapters) URL: https://www.gutenberg.org/files/2785/2785-0.txt\n",
    "   \n",
    "1. Jules Verne\n",
    "   2. Around the World in Eighty Days (37 chapters) URL: https://www.gutenberg.org/ebooks/103.txt.utf-8\n",
    "   2. Five Weeks in a Balloon (44 chapters) URL: https://www.gutenberg.org/files/3526/3526-0.txt\n",
    "   \n",
    "1. Edgar Wallace\n",
    "   2. Bones in London (12 chapters) URL: https://www.gutenberg.org/ebooks/27525.txt.utf-8\n",
    "   2. The Book of All-Power (19 chapters) URL: https://www.gutenberg.org/ebooks/24920.txt.utf-8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions that need answering:\n",
    "\n",
    " 1. What question are you trying to solve (or prove wrong) ?   \n",
    " __No questions to answer for this challenge.__\n",
    " 1. What kind of data do you have? -> describe the source.. \n",
    " __Sentiment analaysis, using bag of words, tfidf, Random Forest, Bernoulli Naive Bayes, and Multinomial Naive Bayes.__\n",
    " 1. Do some EDA, plots\n",
    " 1. What's missing from the data and how do you deal with it?  \n",
    " __No missing data here.__\n",
    " 1. How can you add, change, or remove features to get more out of your data?  \n",
    " __No added features here; it's sentiment analysis.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Report Template\n",
    "\n",
    "#### \n",
    "\n",
    "\n",
    "#### Key Learning. \n",
    "You cannot always improve on a model for even 5 percentage points.  Especially when using bag of words which has no configurable parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1154,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Constants\n",
    "max_iterations         = 10            # set it to > 0 for determining the features inportance\n",
    "random_state           = 57\n",
    "rows_in_training_set   = 10000\n",
    "rows_in_test_set       = 200000\n",
    "test_size              = 0.10\n",
    "train_size             = 0.90\n",
    "rfc_test_size          = 50000\n",
    "rfc_train_size         = 5000\n",
    "sample_size            = 10\n",
    "run_CountVectorizer    = False\n",
    "run_TfidfVectorizer    = True\n",
    "BegTimeStampNewlines   = 3\n",
    "EndTimeStampNewlines   = 3\n",
    "EndTimeStamp           = '\\n'*EndTimeStampNewlines+'End'\n",
    "BegTimeStamp           = 'End'+'\\n'*BegTimeStampNewlines\n",
    "SustainerSTDDEVLimit   = 0.020\n",
    "\n",
    "num_clusters = 3\n",
    "target_column = 'yyyy'\n",
    "xcolumnname = 'xxxx'\n",
    "CrossValidations = 5 # We are using 5 cross validations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1155,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Controls for running sentiment_analyzer\n",
    "flag_to_run_rf = False\n",
    "flag_to_plot_them = False\n",
    "flag_to_run_correlation_matrix = False\n",
    "flag_to_run_features_importance = False\n",
    "flag_to_run_gradient_boosting  = False\n",
    "flag_to_run_linear_regression  = False\n",
    "flag_to_run_logistic_regression = False\n",
    "flag_to_run_lasso_regression = False\n",
    "flag_to_run_ridge_regression = False\n",
    "flag_to_run_svc = False\n",
    "flag_to_run_vectorizer_nb = False\n",
    "flag_to_run_sentiment_analyzer = False\n",
    "flag_to_run_affinity_propagation = False\n",
    "flag_to_run_kmeans = False\n",
    "flag_to_run_mean_shift = False\n",
    "flag_to_run_spectral_clustering = False\n",
    "flag_to_run_elbow_plot = False\n",
    "\n",
    "debug = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1156,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": false
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to /Users/lou/nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import chardet\n",
    "import datetime\n",
    "from sklearn import datasets, ensemble, metrics, linear_model\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "import time, sys\n",
    "import seaborn as sns\n",
    "from sklearn.svm import SVC\n",
    "import statsmodels.api as sm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import cross_val_predict, cross_val_score, GridSearchCV,cross_val_score, train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import MinMaxScaler, normalize\n",
    "from IPython.display import Markdown, display\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import cluster, metrics\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import pairwise_distances, mean_squared_error\n",
    "from sklearn.cluster import AffinityPropagation, KMeans, MeanShift, estimate_bandwidth, SpectralClustering\n",
    "from scipy.spatial.distance import cdist\n",
    "import spacy\n",
    "import re\n",
    "from nltk.corpus import gutenberg, stopwords\n",
    "import nltk\n",
    "from nltk.corpus.reader.plaintext import PlaintextCorpusReader\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.cluster import KMeansClusterer\n",
    "from collections import Counter\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "import random\n",
    "import string\n",
    "import unicodedata\n",
    "import codecs\n",
    "from tqdm import tqdm\n",
    "\n",
    "nltk.download('gutenberg') # Load the gutenberg nltk works\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1157,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regression = False\n"
     ]
    }
   ],
   "source": [
    "# add this to a dictionary\n",
    "# Constants\n",
    "max_iterations         = 10            # set it to > 0 for determining the features inportance\n",
    "random_state           = 57\n",
    "test_size              = 0.10\n",
    "train_size             = 0.90\n",
    "\n",
    "begin_string = '\\n'*3+'Begin'\n",
    "end_string = 'End'+'\\n'*3\n",
    "\n",
    "# Regression/Classification control\n",
    "Regression = False \n",
    "\n",
    "print(\"Regression = {}\".format(Regression))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1158,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Display preferences.\n",
    "%matplotlib inline\n",
    "pd.options.display.float_format = '{:.3f}'.format\n",
    "pd.set_option('display.max_rows', 1000)\n",
    "pd.set_option('display.max_row', 1000)\n",
    "pd.set_option('display.max_columns', 600)\n",
    "pd.set_option('display.width', 4000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1159,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def plot_time_to_complete():\n",
    "    objects = ('BernoulliNB', 'MultinomialNB', 'Logistic Regression')\n",
    "    y_pos = np.arange(len(objects))\n",
    "    performance = [18,17,32]\n",
    "\n",
    "    plt.bar(y_pos, performance, align='center', alpha=0.5)\n",
    "    plt.xticks(y_pos, objects)\n",
    "    plt.ylabel('Time in Minutes')\n",
    "    plt.title('Yelp Sentiment Analysis Time to Complete')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1160,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def file_stuff():\n",
    "    # Use this for stand-alone file\n",
    "    \n",
    "    path = \"../../../../\"\n",
    "    filename = \"Datafiles/bostonmarathon/results/2013/results.csv\"\n",
    "    print(\"fullfilename = {}\".format(path+filename))\n",
    "    df = pd.read_csv(path+filename)\n",
    "    print(\"There are {} rows in this file.\".format(df.shape[0]))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1161,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def text_cleaner(text):\n",
    "    '''\n",
    "    # Utility function for standard text cleaning.\n",
    "    '''\n",
    "    # Visual inspection identifies a form of punctuation spaCy does not\n",
    "    # recognize: the double dash '--'.  Better get rid of it now!\n",
    "    \n",
    "    text = re.sub(r'--',' ',text)\n",
    "    text = re.sub(\"[\\[].*?[\\]]\", \"\", text)\n",
    "    text = ' '.join(text.split())\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1162,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def load_clean_parse_group_gutenberg(gutenberg_file, author, percent_of_file): \n",
    "    \"\"\"\n",
    "    # currently, this function handles:\n",
    "    #    Persuasion, Austen\n",
    "    #    Alice In Wonderland Austen\n",
    "    #    Paradise Lost Milton\n",
    "    #    Moby Dick Melville\n",
    "    \"\"\"\n",
    "\n",
    "    # Load and clean the data.\n",
    "    if gutenberg_file == \"persuasion\":\n",
    "        file_to_load = gutenberg.raw('austen-persuasion.txt')\n",
    "        book = re.sub(r'Chapter \\d+', '', persuasion)\n",
    "    elif gutenberg_file == \"alice\":\n",
    "        file_to_load = gutenberg.raw('carroll-alice.txt')\n",
    "        book = re.sub(r'CHAPTER .*', '', file_to_load)\n",
    "    elif gutenberg_file == \"paradise\":\n",
    "        file_to_load = gutenberg.raw('milton-paradise.txt')\n",
    "        book = re.sub(r'BOOK .*', '', file_to_load)\n",
    "    elif gutenberg_file == \"moby\":\n",
    "        file_to_load = gutenberg.raw('melville-moby_dick.txt')\n",
    "        book = re.sub(r'BOOK .*', '', file_to_load)\n",
    "\n",
    "    book = text_cleaner(book[:int(len(book)/percent_of_file)])\n",
    "    \n",
    "    nlp = spacy.load('en')\n",
    "    book_doc = nlp(book)\n",
    "    \n",
    "    print(\"book_doc datatype is {}\".format(type(book_doc)))\n",
    "    \n",
    "#     book_sents = pd.DataFrame() # I just added this\n",
    "#     book_sents = [[sent, author] for sent in book_doc.sents]\n",
    "    return pd.DataFrame([[sent, author] for sent in book_doc.sents]), book_doc\n",
    "    \n",
    "#     return book_sents, book_doc # previously, I had return pd.DataFrame(book_sents), book_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1163,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def parse_gutenberg(book):\n",
    "    '''\n",
    "    # Parse the cleaned novels\n",
    "    '''\n",
    "    \n",
    "    nlp = spacy.load('en')\n",
    "#     book_doc = nlp(book)\n",
    "\n",
    "    return nlp(book)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1164,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def group_gutenberg(book_doc, author):\n",
    "    '''\n",
    "    # Group into sentences.\n",
    "    '''\n",
    "    print(\"book_doc datatype is {}\".format(type(book_doc)))\n",
    "    \n",
    "    book_sents = [[sent, author] for sent in book_doc.sents]\n",
    "\n",
    "    # Combine the sentences from the two novels into one data frame.\n",
    "#     sentences = pd.DataFrame(book_sents)\n",
    "    if debug:\n",
    "        sentences.head()\n",
    "        \n",
    "    return pd.DataFrame(book_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1165,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def dataset_cleanup(df):\n",
    "\n",
    "    # data Cleanup --> last used for the Boston Marathon challenge\n",
    "    \n",
    "#     Left over from the Boston Marathon challenge...\n",
    "  \n",
    "    df['gender_int'] = np.where(df['gender'] == 'M', 1, 0).astype(float)\n",
    "    df['bib_int'] = df['bib'].replace(to_replace=r'[W|F]', value='-', regex=True).astype(int)\n",
    "    kcolumns = ['5k', '10k', '20k', '25k', '30k', '35k', '40k', 'half']\n",
    "    for kcol in kcolumns:\n",
    "        df[kcol] = np.where(df[kcol] == '-', 0, df[kcol])\n",
    "        df[kcol] = df[kcol].astype(float)\n",
    "    df['5kpace']   = df['5k']/5.0\n",
    "    df['10kpace']  = df['10k']/10.0\n",
    "    df['20kpace']  = df['20k']/20.0\n",
    "    df['halfpace'] = df['half']/21.095\n",
    "    df['25kpace']  = df['25k']/25.0\n",
    "    df['30kpace']  = df['30k']/30.0\n",
    "    df['35kpace']  = df['35k']/35.0\n",
    "    df['40kpace']  = df['40k']/40.0\n",
    "    df['officialpace'] = df['official']/42.19\n",
    "    # df['raceavg'] = ,axis=0).mean()\n",
    "    df['racestd'] = df[['5kpace','10kpace','20kpace','halfpace','25kpace','30kpace','35kpace','40kpace','officialpace']].std(axis=1)\n",
    "    df['raceavg'] = df[['5kpace','10kpace','20kpace','halfpace','25kpace','30kpace','35kpace','40kpace','officialpace']].mean(axis=1)\n",
    "#     X = df[['age', 'gender_int','genderdiv', 'country', 'official','racestd','raceavg']]\n",
    "#     X = pd.get_dummies(X)\n",
    "\n",
    "    df.drop('ctz', axis=1, inplace=True)\n",
    "    df.drop('state',axis=1, inplace=True)\n",
    "    # these are the 2% sustainers.  They can be running at any pace, but they are consistent!\n",
    "    df['sustainer'] = np.where(df['racestd'] <= SustainerSTDDEVLimit, 1, 0).astype(float) # they sustained their pace very well for the race\n",
    "    scaler = MinMaxScaler()\n",
    "    \n",
    "    scaler.fit(df[['age']])\n",
    "    df['age_scaled'] = scaler.transform(df[['age']]).astype(float)\n",
    "    \n",
    "    scaler.fit(df[['overall']])\n",
    "    df['overall_scaled'] = scaler.transform(df[['overall']]).astype(float)\n",
    "    \n",
    "    scaler.fit(df[['pace']])\n",
    "    df['pace_scaled'] = scaler.transform(df[['pace']])\n",
    "    \n",
    "    scaler.fit(df[['official']])\n",
    "    df['official_scaled'] = scaler.transform(df[['official']]).astype(float)\n",
    "    \n",
    "    display('columns are now', df.columns)\n",
    "#     df = fcn_MinMaxScaler(df, 'age', 'age_scaled')\n",
    "#     df = fcn_MinMaxScaler(df, 'official', 'official_scaled')\n",
    "    X = df[['age_scaled', 'sustainer', 'gender_int', 'racestd']]\n",
    "#     X = pd.get_dummies(X)\n",
    "   \n",
    "    \n",
    "    display(\"df columns cpt 92310: \", df.columns)\n",
    "    \n",
    "    global target_column, xcolumnname, ycolumnname\n",
    "    \n",
    "#     target_column = 'overall_scaled'\n",
    "#     xcolumnname = 'age_scaled'\n",
    "    ycolumnname = target_column\n",
    "    \n",
    "    y = df[target_column]\n",
    "    printFormatted(\"target, y column is {}\".format(target_column))\n",
    "\n",
    "    if debug == True:\n",
    "        print_timestamp(\"X and y variables created\")\n",
    "        \n",
    "    printFormatted('we have cleaned up the dataframe.')\n",
    "    display_column_names('df values', df)\n",
    "    display_column_names('X values', X)\n",
    "    return df, X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1166,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def printFormatted(string):\n",
    "    newline = '\\n'\n",
    "    display(Markdown(string))\n",
    "    write_to_logfile(string+newline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1167,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def fcn_MinMaxScaler(dataframe, orig_column, new_column):\n",
    "    display(\"cp 1: In fcn_MinMaxScaler.  shape is:\", dataframe.shape)\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(dataframe[['{}'.format(orig_column)]])\n",
    "    dataframe[['{}'.format(new_column)]] = scaler.transform(dataframe['{}'.format(orig_column)])\n",
    "    display(\"cp 2: In fcn_MinMaxScaler.  shape is:\", dataframe.shape)\n",
    "    \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1168,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def plot_facet():\n",
    "    g = sns.FacetGrid(data=df, col='stars')\n",
    "    g.map(plt.hist, 'message_length', bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1169,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def write_to_logfile(message, mdformat=''):\n",
    "    bufsize = 0\n",
    "    with open('TestResults.md', 'a+') as the_file:\n",
    "        the_file.write('{} {}'.format(mdformat, message))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1170,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def plot_model_accuracy():\n",
    "    objects = ('BernoulliNB', 'MultinomialNB', 'Logistic Regression')\n",
    "    y_pos = np.arange(len(objects))\n",
    "    performance = [75.81,85.98,91.08]\n",
    "\n",
    "    plt.bar(y_pos, performance, align='center', alpha=0.5)\n",
    "    plt.xticks(y_pos, objects)\n",
    "    plt.ylabel('Accuracy Percent')\n",
    "    plt.title('Yelp Sentiment Analysis Accuracy')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1171,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def print_timestamp(displaytext):    \n",
    "    import sys\n",
    "    import datetime\n",
    "    datetime_now = str(datetime.datetime.now())\n",
    "    printFormatted(\"{:19.19}: In: {} {} \".format(datetime_now, sys._getframe(1).f_code.co_name, displaytext))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1172,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def return_current_datetime():\n",
    "    datetime_now = str(datetime.datetime.now())\n",
    "    return datetime_now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1173,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def data_demographics(dataframe, num_rows):\n",
    "\n",
    "    display(\"dataframe.isnull().sum()\", dataframe.isnull().sum())\n",
    "\n",
    "    display(\"dataframe.columns\\n\", dataframe.columns)\n",
    "    display(\"dataframe.head({})\\n\".format(num_rows), dataframe.head(num_rows))\n",
    "\n",
    "    display(\"dataframe.sample({})\\n\".format(num_rows), dataframe.sample(num_rows))\n",
    "    display(\"dataframe.dtypes\\n\", dataframe.dtypes)\n",
    "    display(\"dataframe.describe()\\n\", dataframe.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1174,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def plot_them():\n",
    "    for column in X_train.columns:\n",
    "#         plt.hist(X_train[column]*100, bins=40)\n",
    "        plt.scatter(y_train, X_train[column]*100)\n",
    "        plt.xlabel(column)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1175,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def rfc_and_feature_importances(rf):    # Here we are using Random Forest classifier method to determine the top 30 features.\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, train_size=train_size)\n",
    "    \n",
    "    ## Fit the model on your training data.\n",
    "    rf.fit(X_train, y_train) \n",
    "    \n",
    "    ## And score it on your testing data.\n",
    "    rf.score(X_test, y_test)\n",
    "\n",
    "    feature_importance = rf.feature_importances_\n",
    "\n",
    "    # Make importances relative to max importance.\n",
    "    feature_importance = 100.0 * (feature_importance / feature_importance.max())\n",
    "    sorted_idx = np.argsort(feature_importance)\n",
    "    cols=X.columns[sorted_idx].tolist() \n",
    "    cols=cols[::-1]\n",
    "    pos = np.arange(sorted_idx.shape[0]) + .5\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.barh(pos, feature_importance[sorted_idx], align='center')\n",
    "    plt.yticks(pos, X.columns[sorted_idx])\n",
    "    plt.xlabel('Relative Importance')\n",
    "    plt.title('Variable Importance')\n",
    "    plt.show()\n",
    "#     print(\"We are returning these columns {}\".format(cols))\n",
    "    return cols[:30] # return it sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1176,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def run_features_importance(rf,n):\n",
    "# Here we will return the feature importances\n",
    "    all_feature_important_columns = []\n",
    " \n",
    "    for i in range(1,n):\n",
    "        print_timestamp('running rfc iteration {} features importance for {} times'.format(i,n))\n",
    "        columns2 = rfc_and_feature_importances(rf)\n",
    "#         columns2.extend('{}'.format(i))\n",
    "        all_feature_important_columns = all_feature_important_columns + columns2\n",
    "    #     print(\"all_feature_import_columns={}\".format(all_feature_important_columns))\n",
    "\n",
    "    print(\"\\nBOD:\\nall_feature_important_columns = {}\\nEOD\".format(sorted(all_feature_important_columns)))\n",
    "    for feature in set(all_feature_important_columns):\n",
    "        print_timestamp(\"the NOC of feature {} in all_feature_important_columns is {}\".format(feature, all_feature_important_columns.count(feature)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1177,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def run_correlation_matrix():\n",
    "    \n",
    "    print_timestamp('Begin'+'\\n'*3)\n",
    "    \n",
    "    # Setup the correlation matrix.\n",
    "    corrmat = X.corr()\n",
    "    print(corrmat)\n",
    "\n",
    "    # Set up the subplots\n",
    "    f, ax = plt.subplots(figsize=(12, 9))\n",
    "\n",
    "    # Let's draw the heatmap using seaborn.\n",
    "    sns.heatmap(corrmat, vmax=.6, square=True)\n",
    "    plt.show()\n",
    "    \n",
    "    print_timestamp('\\n'*3+'End')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1178,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def data_characteristics():\n",
    "    \n",
    "    printFormatted(\"#### Columns used in the dataset\")\n",
    "    display(df.columns)\n",
    "\n",
    "    print(\"\\n\\n\")\n",
    "    printFormatted(\"#### Describe of the df dataset\")\n",
    "    display(df.describe())\n",
    "\n",
    "    print(\"\\n\\n\")\n",
    "    printFormatted(\"#### Sample of 10 from the dataset\")\n",
    "    display(df.sample(sample_size))\n",
    "\n",
    "    print(\"\\n\\n\")\n",
    "    printFormatted(\"#### Number of nulls in X\")\n",
    "    display(X.isnull().sum())\n",
    "    print(\"\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1179,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def training_test_set(X, y):\n",
    "#     global X_train, X_test, y_train, y_test\n",
    "    # Let's fit it with the RFC training set\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, train_size=train_size, random_state=0)\n",
    "    print(\"train_size = {}, X_train is {}, and y_train is {}\".format(train_size, len(X_train), len(y_train)))\n",
    "    print(\"test_size  = {}, X_test  is {}, and y_test is {}\".format(test_size, len(X_test), len(y_test)))\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1180,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def run_rf(X_train=None, X_test=None, y_train=None, y_test=None, params=None, cross_validate=None):\n",
    "    \n",
    "    rfc = ensemble.RandomForestClassifier(n_estimators=1000)\n",
    "    if params != None:\n",
    "        print(\"In run_rf, params = {}\".format(params)) \n",
    "        rfc.set_params(**params)\n",
    "        \n",
    "    ## Fit the model on your training data.\n",
    "    rfc_fit = rfc.fit(X_train, y_train)  \n",
    "    \n",
    "    #   Let's score it with the test data set    this is new 13-Aug-2019\n",
    "    print_training_and_test_scores(rfc_fit, X_train, X_test, y_train, y_test) # new on 13-Aug-2019\n",
    "    \n",
    "#   Let's produce the metrics scores\n",
    "    print_metrics_score(rfc_fit, X_train, X_test, y_train, y_test) # new on 13-Aug-2019\n",
    "    \n",
    "#   Let's run cross validation \n",
    "    if cross_validate == True:\n",
    "        print_cross_validation_scores(rfc_fit, X_train, X_test, y_train, y_test)\n",
    "        \n",
    "#   Let's run the confusion matrix\n",
    "    if confusion_matrix == True:\n",
    "        confusion_matrix_function(rfc_fit, X_train, X_test, y_train, y_test)\n",
    "    \n",
    "#     ## Let's score it with the training data set\n",
    "#     train_score = rfc.score(X_train, y_train)\n",
    "#     printFormatted(\"### Training score = {:.2%}\".format(train_score))\n",
    "\n",
    "#     ## Let's score it with the test data set\n",
    "#     test_score = rfc.score(X_test, y_test)\n",
    "#     printFormatted(\"### Test score = {:.2%}\".format(test_score))\n",
    "    \n",
    "#     metrics_train_score = metrics.accuracy_score(y_train, y_pred_class2)\n",
    "#     metrics_test_score =  metrics.accuracy_score(y_test, y_pred_class)\n",
    "\n",
    "#     printFormatted('###  Metrics train accuracy score = {:.2%} with {}'.format(metrics_train_score, 'Random Forest Classifier'))\n",
    "#     printFormatted('###  Metrics test accuracy score = {:.2%} with {}'.format(metrics_test_score, 'Random Forest Classifier'))\n",
    "    \n",
    "#     if cross_validate == True:\n",
    "#         accuracy = cross_val_score(rfc, X_train, y_train, scoring='accuracy', cv = 5)\n",
    "#         printFormatted(\"### Cross validation scores:  {}\".format(accuracy))\n",
    "#         printFormatted(\"### Accuracy of Model with Cross Validation average is: {:.2%}\".format(accuracy.mean()))\n",
    "        \n",
    "#     #   Let's produce the metrics scores\n",
    "#     print_metrics_score(mnb_fit, X_train, X_test, y_train, y_test) # new on 13-Aug-2019\n",
    "      \n",
    "    print_timestamp('End run_rfr part 1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1181,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def run_BernoulliNB(X_train=None, X_test=None, y_train=None, y_test=None, params=None, cross_validate=None):\n",
    "    \n",
    "    # Our data is binary / boolean, so we're importing the Bernoulli classifier.\n",
    "\n",
    "    # Instantiate our model and store it in a new variable.\n",
    "    bnb = BernoulliNB()\n",
    "\n",
    "    # Fit our model to the data.\n",
    "    bnb_fit = bnb.fit(X_train, y_train)\n",
    "\n",
    "    # Classify, storing the result in a new variable.\n",
    "#     y_pred = bnb.predict(data)\n",
    "#     y_pred = bnb.predict(X_train)\n",
    "\n",
    "    # Display our results.\n",
    "#     print(\"Number of mislabeled points out of a total {} points : {}\".format(\n",
    "#        X_train.shape[0],\n",
    "#         (y_test != y_pred).sum() \n",
    "#     ))\n",
    "    \n",
    "#   Let's score it with the test data set    this is new 13-Aug-2019\n",
    "    print_training_and_test_scores(bnb_fit, X_train, X_test, y_train, y_test) # new on 13-Aug-2019\n",
    "    \n",
    "#   Let's produce the metrics scores\n",
    "    print_metrics_score(bnb_fit, X_train, X_test, y_train, y_test) # new on 13-Aug-2019\n",
    "    \n",
    "#   Let's run cross validation \n",
    "    if cross_validate == True:\n",
    "        print_cross_validation_scores(bnb_fit, X_train, X_test, y_train, y_test)\n",
    "        \n",
    "#   Let's run the confusion matrix\n",
    "    if confusion_matrix == True:\n",
    "        confusion_matrix_function(bnb_fit, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1182,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def sentiment_analyzer(path, parameters, classifier, tfidf_parms, X_train=None, X_test=None, y_train=None, y_test=None, cross_validate=None):\n",
    "    # path A = the old path\n",
    "    # path B = the new path, no CountVectorizer at all\n",
    "    \n",
    "# run block of code and catch warnings\n",
    "  \n",
    "    if debug == True:\n",
    "        print_timestamp(BegTimeStamp+\" running with path={}\".format(path))\n",
    "    \n",
    "    global vectorized\n",
    "    vectorized = True\n",
    "    \n",
    "    pipeline_array = []\n",
    "   \n",
    "    if path == \"A\":\n",
    "        if classifier == 'bnb':\n",
    "            pipeline_array.append(Pipeline([\n",
    "                ('tfidf', TfidfVectorizer(**tfidf_parms)),\n",
    "                ('clf',   BernoulliNB(**parameters))\n",
    "            ]))\n",
    "        elif classifier == 'svc':\n",
    "            pipeline_array.append(Pipeline([\n",
    "                ('tfidf', TfidfVectorizer(**tfidf_parms)),\n",
    "                ('clf',   SVC(kernel = 'linear', **parameters))\n",
    "            ])) \n",
    "        elif classifier == 'mlb':\n",
    "            pipeline_array.append(Pipeline([\n",
    "                ('tfidf', TfidfVectorizer(**tfidf_parms)),\n",
    "                ('clf',   MultinomialNB(**parameters))\n",
    "            ]))\n",
    "        elif classifier == 'logit':\n",
    "            pipeline_array.append(Pipeline([\n",
    "                ('tfidf', TfidfVectorizer(**tfidf_parms)),\n",
    "                ('clf',   LogisticRegression(**parameters))\n",
    "            ]))\n",
    "        elif classifier == 'rfc':\n",
    "            pipeline_array.append(Pipeline([\n",
    "                ('tfidf', TfidfVectorizer(**tfidf_parms)),\n",
    "                ('clf',   ensemble.RandomForestClassifier(**parameters))\n",
    "            ]))  \n",
    "            \n",
    "    elif path == \"B\":\n",
    "        if classifier == 'bnb':\n",
    "            pipeline_array.append(Pipeline([\n",
    "                ('tfidf', TfidfVectorizer(**tfidf_parms)),\n",
    "                ('clf',   BernoulliNB(**parameters))\n",
    "            ]))\n",
    "        elif classifier == 'svc':\n",
    "            pipeline_array.append(Pipeline([\n",
    "                ('tfidf', TfidfVectorizer(**tfidf_parms)),\n",
    "                ('clf',   SVC(kernel = 'linear', **parameters))\n",
    "            ])) \n",
    "        elif classifier == 'mlb':\n",
    "            pipeline_array.append(Pipeline([\n",
    "                ('tfidf', TfidfVectorizer(**tfidf_parms)),\n",
    "                ('clf',   MultinomialNB(**parameters))\n",
    "            ]))\n",
    "        elif classifier == 'logit':\n",
    "            pipeline_array.append(Pipeline([\n",
    "                ('tfidf', TfidfVectorizer()),\n",
    "                ('clf',   LogisticRegression(**parameters))\n",
    "            ]))\n",
    "        elif classifier == 'rfc':\n",
    "            pipeline_array.append(Pipeline([\n",
    "                ('tfidf', TfidfVectorizer(**tfidf_parms)),\n",
    "                ('clf',   ensemble.RandomForestClassifier(**parameters))\n",
    "            ]))\n",
    "\n",
    "    pipe = pipeline_array[0]\n",
    "    \n",
    "    try:\n",
    "        vect_name_list = str(pipe.named_steps['vect']).split('(')\n",
    "        vect_name = \"vect = {}, \".format(vect_name_list[0])\n",
    "    except:\n",
    "        vect_name = ''\n",
    "\n",
    "    classifier_name_list=str(pipe.named_steps['clf']).split('(')\n",
    "    classifier_name=classifier_name_list[0]\n",
    "    tfidf_name_list = str(pipe.named_steps['tfidf']).split('(')\n",
    "    if len(tfidf_name_list) > 0:\n",
    "        tfidf_name = tfidf_name_list[0]\n",
    "    else:\n",
    "        tfidf_name = ''\n",
    "\n",
    "    printFormatted(\"###  Now running with: {} tfidf={} and clf={} {}\\nparameters={} \\n\\n tfidf_parms={}\".format( vect_name,\n",
    "                                                                                                tfidf_name,\n",
    "                                                                                                classifier_name,\n",
    "                                                                                                return_current_datetime(),\n",
    "                                                                                                parameters,\n",
    "                                                                                                tfidf_parms\n",
    "                                                                                                ))\n",
    "    pipefit = pipe.fit(X_train, y_train)\n",
    "\n",
    "#   Let's score it with the test data set    this is new 13-Aug-2019\n",
    "    print_training_and_test_scores(pipefit, X_train, X_test, y_train, y_test) # new on 13-Aug-2019\n",
    "    \n",
    "#   Let's produce the metrics scores\n",
    "    print_metrics_score(pipefit, X_train, X_test, y_train, y_test) # new on 13-Aug-2019\n",
    "    \n",
    "#   Let's run cross validation \n",
    "    if cross_validate == True:\n",
    "        print_cross_validation_scores(pipefit, X_train, X_test, y_train, y_test)\n",
    "        \n",
    "#   Let's run the confusion matrix\n",
    "    if confusion_matrix == True:\n",
    "        confusion_matrix_function(pipefit, X_train, X_test, y_train, y_test)\n",
    "            \n",
    "    if debug == True:\n",
    "        printFormatted(\"Steps information: {}\".format(pipe.steps))\n",
    "        print_timestamp(\"Finished running pipeline with:\\n{}: \".format(classifier_name))\n",
    "        \n",
    "    print_timestamp(EndTimeStamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1183,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def print_training_and_test_scores(model, X_train, X_test, y_train, y_test):\n",
    "    \n",
    "    ## Let's score it with the test data set    this is new 13-Aug-2019\n",
    "    training_score = model.score(X_train, y_train) \n",
    "    printFormatted(\"### Training score = {:.2%}\".format(training_score))\n",
    "    \n",
    "    ## Let's score it with the test data set  this is new 13-Aug-2019\n",
    "    test_score = model.score(X_test, y_test)\n",
    "    printFormatted(\"### Test score = {:.2%}\".format(test_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1184,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def print_metrics_score(fit_model, X_train, X_test, y_train, y_test):\n",
    "    y_pred_class  = fit_model.predict(X_test)\n",
    "    metrics_test_score =  metrics.accuracy_score(y_test, y_pred_class)\n",
    "    printFormatted('###  Metrics test accuracy score = {:.2%}'.format(metrics_test_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1185,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def print_cross_validation_scores(fit_model, X_train, X_test, y_train, y_test):\n",
    "    \n",
    "        accuracy = cross_val_score(fit_model, X_train, y_train, scoring='accuracy', cv = 5)\n",
    "        printFormatted(\"### Cross validation scores:  {}\".format(accuracy))\n",
    "        printFormatted(\"### Accuracy of Model with Cross Validation average is: {:.2%}\".format(accuracy.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1186,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def run_gradient_boosting():\n",
    "\n",
    "    print_timestamp('Begin')\n",
    "    \n",
    "    clf = ensemble.GradientBoostingClassifier(**params)\n",
    "\n",
    "    #Let's run cross validate score with the training data set\n",
    "    cross_val_score(clf, X_train, y_train, cv=5)\n",
    "\n",
    "    loss_function = 'deviance' # could be exponential\n",
    "    depth_value = 8\n",
    "    params = {'n_estimators': 500,\n",
    "              'max_depth': 8,\n",
    "              'loss_function': loss_function,\n",
    "              'max_leaf_nodes': depth_value, # 8 worked best...\n",
    "              'min_samples_leaf': depth_value * 3\n",
    "              ,'random_state' : random_state\n",
    "             }\n",
    "\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    predict_train = clf.predict(X_train)\n",
    "    predict_test = clf.predict(X_test)\n",
    "    \n",
    "    print_timestamp('End')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1187,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def run_svc(X_train=None, X_test=None, y_train=None, y_test=None, cross_validate=None, params=None):\n",
    "\n",
    "    print_timestamp('\\n'*3+'Begin run_svc')\n",
    "    \n",
    "    # Let's do a linear Support Vector Classifier\n",
    "    print_timestamp('Running SVC(kernel=linear')\n",
    "    svm = SVC(kernel = 'linear')\n",
    "    \n",
    "    if params == True:\n",
    "        print(\"In run_rf, params = {}\".format(params)) \n",
    "        svm.set_params(**params)\n",
    "    \n",
    "    # Let's fit the training model\n",
    "    print_timestamp('Running svm.fit')\n",
    "    svm.fit(X_train, y_train)\n",
    "    \n",
    "    # Let's score the training set\n",
    "    print_timestamp('Running svm.score for the training set')\n",
    "    svm_training_score = svm.score(X_train, y_train)\n",
    "    printFormatted(\"###  SVM Training score={:.2%}\".format(svm_training_score))\n",
    "\n",
    "    # Let's score the test set\n",
    "    print_timestamp('Running svm.fit for the test set')\n",
    "    svm_test_score = svm.score(X_test, y_test)\n",
    "    printFormatted(\"###  SVM Test score={:.2%}\".format(svm_test_score))\n",
    "\n",
    "    if cross_validate == True:\n",
    "        accuracy = cross_val_score(svm, X_train, y_train, scoring='accuracy', cv = 5)\n",
    "        printFormatted(\"### Cross validation scores:  {}\".format(accuracy))\n",
    "        printFormatted(\"### Accuracy of Model with Cross Validation average is: {:.2%}\".format(accuracy.mean()))\n",
    "\n",
    "    print_timestamp('\\n'*3+'End run_svc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1188,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def run_logistic_regression():\n",
    "    print_timestamp('\\n'*3+'Begin')\n",
    "\n",
    "    lr = LogisticRegression(C=1e20, solver='lbfgs', max_iter=1000)\n",
    "\n",
    "    print_timestamp('Running lr.fit for the training set')\n",
    "    lr.fit(X_train, y_train)\n",
    "    \n",
    "    print_timestamp('Running lr.fit for the training set')\n",
    "    print('\\nR-squared simple model training set yields:')\n",
    "    print(lr.score(X_train, y_train))\n",
    "    print(\"here comes the test set\")\n",
    "    lrscore = lr.score(X_test, y_test)\n",
    "    printFormatted(\"###  Logistic Regression score={:.2%}\".format(lrscore))\n",
    "    \n",
    "    print_timestamp('\\n'*3+'End')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1189,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def run_linear_regression():\n",
    "\n",
    "    print_timestamp('\\n'*3+'Begin')\n",
    "\n",
    "    regr = linear_model.LinearRegression()\n",
    "\n",
    "    print_timestamp('Running regr.fit for the training set')\n",
    "    regr.fit(X_train, y_train)\n",
    "    \n",
    "    print(\"\\nCoeffecients: \\n\", regr.coef_)\n",
    "    print(\"\\nIntercept: \\n\", regr.intercept_)\n",
    "    print(\"\\nR-squared for training data set:\")\n",
    "    print(regr.score(X_train, y_train))\n",
    "    \n",
    "    print(\"\\nR-squared for test data set:\")\n",
    "    print(regr.score(X_test, y_test))\n",
    "    \n",
    "    print_timestamp('End run_linear_regression.\\n\\n')\n",
    "    \n",
    "    print_timestamp('\\n'*3+'End')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1190,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def run_ridge_regression():\n",
    "    # Fitting a ridge regression model. Alpha is the regularization\n",
    "    # parameter (usually called lambda). As alpha gets larger, parameter\n",
    "    # shrinkage grows more pronounced. Note that by convention, the\n",
    "    # intercept is not regularized. Since we standardized the data\n",
    "    # earlier, the intercept should be equal to zero and can be dropped.\n",
    "    print_timestamp('\\n'*3+'Begin')\n",
    "    \n",
    "    ridgeregr = linear_model.Ridge(alpha=10, fit_intercept=False) \n",
    "    ridgeregr.fit(X_train, y_train)\n",
    "    print(ridgeregr.score(X_train, y_train))\n",
    "\n",
    "    print_timestamp('\\n'*3+'End')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1191,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def run_affinity_propagation(data, parameters=None):\n",
    "      \n",
    "    # returns the prediction, and the number of clusters\n",
    "    \n",
    "    print_timestamp('\\n'*3+'starting AffinityPropagation')\n",
    "\n",
    "    print_timestamp('\\n'*3+'Begin')\n",
    "    \n",
    "    ap = AffinityPropagation(parameters)\n",
    "#     ap = AffinityPropagation(damping=0.5,\n",
    "#                          max_iter=200,\n",
    "#                          convergence_iter=15,\n",
    "#                          copy=True,\n",
    "#                          preference=None,\n",
    "#                          affinity='euclidean',\n",
    "#                          verbose=False) \n",
    "\n",
    "    model = ap.fit(data)\n",
    "    pred = ap.predict(data)\n",
    "\n",
    "#     Z = merge_predict_and_cluster(data, target, pred) # let's merge the data dataframe, prediction, and the cluster\n",
    "    \n",
    "    # Pull the number of clusters and cluster assignments for each data point.\n",
    "    cluster_centers_indices = ap.cluster_centers_indices_\n",
    "    n_clusters_ = len(cluster_centers_indices)\n",
    "    labels = ap.labels_\n",
    "    \n",
    "    print('Estimated number of clusters: {}'.format(n_clusters_))\n",
    "\n",
    "    labels = model.labels_\n",
    "    \n",
    "    print(\"from run_affinity_propagation {}\".format(metrics.silhouette_score(data, labels, metric='euclidean')))\n",
    "    \n",
    "    print_timestamp('\\n'*3+'finished with AffinityPropagation')\n",
    "    \n",
    "    return pred, n_clusters_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1192,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def run_kmeans(data, parameters=None):\n",
    "\n",
    "    print(\"inside run_kmeans, and the data parameter is a {} datatype.\".format(type(data)))\n",
    "    dataframe = pd.DataFrame(data)\n",
    "    print(\"in run_kmeans, the size of dataframe is {}\".format(dataframe.shape))\n",
    "    # returns the prediction for the cluster\n",
    "    print(\"here comes the null check for data inside run_kmeans\")\n",
    "    display(data.isnull().sum())\n",
    "    print(\"here comes data.head(50)\")\n",
    "    display(data.head(50))\n",
    "    \n",
    "    print_timestamp('\\n'*3+'Begin')\n",
    "    print(\"running with number of clusters = {}\".format(parameters['n_clusters']))\n",
    "    print(\"in run_kmeans, parameters = {}\".format(parameters))\n",
    "    km = KMeans(parameters)\n",
    "\n",
    "#     pred = KMeans(n_clusters=K, random_state=42).fit_predict(data)\n",
    "    pred = km.fit_predict(data)\n",
    "#     Z = pd.DataFrame()\n",
    "#     Z = merge_predict_and_cluster(data, target, pred) # let's merge the data dataframe, prediction, and the cluster\n",
    "#     Z = pd.merge(data, pd.DataFrame(pred), left_index=True, right_index=True)\n",
    "#     display_column_names('first Z values', Z)\n",
    "#     Z.rename(columns={Z.columns[-1]: 'cluster'}, inplace=True)\n",
    "#     display_column_names('second Z values', Z)\n",
    "#     Z = pd.merge(Z, target, left_index=True, right_index=True)\n",
    "#     display_column_names('third Z values', Z)\n",
    "#     print(\"z columns are {}\".format(Z.columns))\n",
    "\n",
    "#     if debug == True:  \n",
    "#         print(\"the shape of Kmeans_pred is {}, and the shape of X is {}, and the shape of Z is {}\".format(pred.shape,\n",
    "#                                                                                                       data.shape,\n",
    "#                                                                                                       Z.shape))\n",
    "#         display(Z.head(100))\n",
    "#         display_column_names('Z below values', Z)\n",
    "\n",
    "#         count = Z.groupby(['cluster']).count() \n",
    "#         display(\"Z: Count by clusters are this:\\n\", count) \n",
    "  \n",
    "    print_timestamp('\\n'*3+'End')\n",
    "    \n",
    "    return pred\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1193,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def merge_predict_and_cluster(dataframe, target, predict):\n",
    "    Z = pd.merge(dataframe, target, left_index=True, right_index=True)\n",
    "    Z = pd.merge(Z, pd.DataFrame(predict), left_index=True, right_index=True)\n",
    "    Z.rename(columns={Z.columns[-1]: 'cluster'}, inplace=True)\n",
    "    \n",
    "    return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1194,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def confusion_matrix_function(model_fit, X_train, X_test, y_train, y_test):\n",
    "    \n",
    "    y_pred_class  = model_fit.predict(X_test)\n",
    "    \n",
    "    conf_matrix = confusion_matrix_function(y_test, y_pred_class)\n",
    "    printFormatted(\"### Confusion Matrix:  {}\".format(conf_matrixscores))\n",
    "    \n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1195,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def run_spectral_clustering(data, parameters=None):\n",
    "    display_dataframe_shape('entering run_spectral_clustering, data has shape of:', data)\n",
    "#     display_dataframe_shape('entering run_spectral_clustering, target has shape of:', target)\n",
    "    print_timestamp('\\n'*3+'Begin')\n",
    "    \n",
    "#     for clusternum in range(2, K):\n",
    "    print_timestamp(\"Running spectral_clustering with {} clusters.\".format(parameters['n_clusters']))\n",
    "#     n_clusters=K\n",
    "\n",
    "    # Declare and fit the model.\n",
    "    sc = SpectralClustering(parameters)\n",
    "    sc.fit(data)\n",
    "\n",
    "    #Predicted clusters.\n",
    "    predict=sc.fit_predict(data)\n",
    "\n",
    "#     Z = merge_predict_and_cluster(data, target, predict) # let's merge the data dataframe, prediction, and the cluster\n",
    "\n",
    "    if debug == True:\n",
    "        pass\n",
    "#         display_dataframe_shape('in run_spectral_clustering, Z has shape of:', Z)\n",
    "#         display_dataframe_shape('in run_spectral_clustering, target has shape of:', target)\n",
    "#         display(\"the datatypes of Z are\", Z.dtypes)\n",
    "\n",
    "#     plt.scatter(Z['cluster'], Z[target_column], c=Z['cluster'])\n",
    "#     plt.show()\n",
    "\n",
    "    labels = sc.labels_\n",
    "    print(\"from spectral clustering {}\".format(metrics.silhouette_score(data, labels, metric='euclidean')))\n",
    "\n",
    "#     print('Comparing the assigned categories to the ones in the data:')\n",
    "#     print(pd.crosstab(target,predict))\n",
    "    \n",
    "    print_timestamp('\\n'*3+'End')\n",
    "    \n",
    "    return predict, parameters['n_clusters']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1196,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def do_the_elbow(X, maxK=11):\n",
    "    printFormatted(\"## We are plotting the elbow method!\")\n",
    "    print_timestamp('\\n'*3+'Begin')\n",
    "    # calculate distortion for a range of number of cluster\n",
    "    distortions = []\n",
    "    for i in range(1, maxK):\n",
    "        km = KMeans(\n",
    "            n_clusters=i, init='random',\n",
    "            n_init=10, max_iter=300,\n",
    "            tol=1e-04, random_state=0\n",
    "        )\n",
    "        km.fit(X)\n",
    "        distortions.append(km.inertia_)\n",
    "\n",
    "    # plot\n",
    "    plt.plot(range(1, maxK), distortions, marker='o')\n",
    "    plt.xlabel('Number of clusters')\n",
    "    plt.ylabel('Distortion')\n",
    "    plt.show()\n",
    "    \n",
    "    print_timestamp('\\n'*3+'End')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1197,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def plot_it_clusters(dataframe, xvalue, yvalue, title):\n",
    "    \n",
    "    if debug == True:\n",
    "        display_dataframe_shape('entry received in plot_it_clusters', dataframe)\n",
    "        display(dataframe.dtypes)\n",
    "\n",
    "    data_demographics(dataframe, 10)\n",
    "        \n",
    "    plt.rcParams['figure.figsize'] = [xvalue, yvalue]\n",
    "    plt.xlabel(xcolumnname)\n",
    "    plt.ylabel(ycolumnname)\n",
    "    \n",
    "    df0 = dataframe[dataframe.cluster == 0]\n",
    "    df1 = dataframe[dataframe.cluster == 1]\n",
    "    df2 = dataframe[dataframe.cluster == 2]\n",
    "    df3 = dataframe[dataframe.cluster == 3]\n",
    "    df4 = dataframe[dataframe.cluster == 4]\n",
    "    df5 = dataframe[dataframe.cluster == 5]\n",
    "    \n",
    "    plt.scatter(df0[xcolumnname], df0[ycolumnname], color='green')\n",
    "    plt.scatter(df1[xcolumnname], df1[ycolumnname], color='red')\n",
    "    plt.scatter(df2[xcolumnname], df2[ycolumnname], color='blue')\n",
    "    plt.scatter(df3[xcolumnname], df3[ycolumnname], color='black')\n",
    "    plt.scatter(df4[xcolumnname], df4[ycolumnname], color='magenta')\n",
    "    plt.scatter(df5[xcolumnname], df5[ycolumnname], color='orange')\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "        \n",
    "#     plt.scatter(km.cluster_centers_[:,0],km.cluster_centers_[:,1],color='purple',marker='*',label='centroid')\n",
    "    \n",
    "#     if type == 'KMeans':\n",
    "#         plt.xlabel('Age')\n",
    "#         plt.ylabel('Income ($)')\n",
    "#         plt.legend()\n",
    "#         plt.scatter(km.cluster_centers[:,0], \n",
    "#                     km.cluster_centers[:,1],\n",
    "#                     marker = '*',\n",
    "#                     label = 'centroid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1198,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def run_mean_shift(data, parameters=None):\n",
    "    \n",
    "    # returns the cluster prediction, and the number of clusters\n",
    "    \n",
    "    print_timestamp('\\n'*3+'Begin')  \n",
    "\n",
    "    X_train = data\n",
    "    \n",
    "    # Here we set the bandwidth. This function automatically derives a bandwidth\n",
    "    # number based on an inspection of the distances among points in the data.\n",
    "    bandwidth = estimate_bandwidth(X_train, quantile=0.2, n_samples=500)\n",
    "\n",
    "    # Declare and fit the model.\n",
    "#     ms = MeanShift(bandwidth=bandwidth, bin_seeding=True)\n",
    "    ms = MeanShift(parameters)\n",
    "    if debug == True:\n",
    "        display_dataframe_shape('this is the shape of data coming into run_mean_shift', data)\n",
    "    ms.fit(data)\n",
    "\n",
    "#     if debug == True:\n",
    "#         display_dataframe_shape('this is the shape of target coming into run_mean_shift', target)\n",
    "        \n",
    "    pred = ms.predict(data)\n",
    "    \n",
    "    if debug == True:\n",
    "        display_dataframe_shape('this is the shape of pred after predict in run_mean_shift', data)\n",
    "        \n",
    "#     Z = merge_predict_and_cluster(data, target, pred) # let's merge the data dataframe, prediction, and the cluster\n",
    "\n",
    "    # Extract cluster assignments for each data point.\n",
    "    labels = ms.labels_\n",
    "\n",
    "    print(\"from mean shift {}\".format(metrics.silhouette_score(data, labels, metric='euclidean')))\n",
    "    \n",
    "    # Coordinates of the cluster centers.\n",
    "    cluster_centers = ms.cluster_centers_\n",
    "\n",
    "    # Count our clusters.\n",
    "    n_clusters_ = len(np.unique(labels))\n",
    "\n",
    "    print(\"Number of estimated clusters: {}\".format(n_clusters_))\n",
    "    \n",
    "    print_timestamp('\\n'*3+'End')\n",
    "    \n",
    "    return pred, n_clusters_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1199,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def run_tfidf_vectorizer(df, max_df=0.5, min_df=2, stop_words='english', lowercase=True, use_idf=True, norm=u'l2', smooth_idf=True):\n",
    "    print_timestamp(\"in run_tfidf_vectorizer: df is a {} datatype.\".format(type(df)))\n",
    "    vectorizer = TfidfVectorizer(\n",
    "                             max_df=max_df, # drop words that occur in more than half the paragraphs\n",
    "                             min_df=min_df, # only use words that appear at least twice\n",
    "                             stop_words=stop_words, \n",
    "                             lowercase=lowercase, #convert everything to lower case (since Alice in Wonderland has the HABIT of CAPITALIZING WORDS for EMPHASIS)\n",
    "                             use_idf=use_idf,#we definitely want to use inverse document frequencies in our weighting\n",
    "                             norm=norm, #Applies a correction factor so that longer paragraphs and shorter paragraphs get treated equally\n",
    "                             smooth_idf=smooth_idf #Adds 1 to all document frequencies, as if an extra document existed that used every word once.  Prevents divide-by-zero errors\n",
    "                            )\n",
    "\n",
    "    #Applying the vectorizer\n",
    "    tfidf_df = vectorizer.fit(df)\n",
    "    \n",
    "    return tfidf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1200,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def run_tfidf_vectorizer2(df, parameters=None):\n",
    "    print_timestamp(\"in run_tfidf_vectorizer: df is a {} datatype.\".format(type(df)))\n",
    "    if parameters:\n",
    "        print(\"parms = {}\".format(parameters))\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    vectorizer.set_params(**parameters)\n",
    "\n",
    "    #Applying the vectorizer\n",
    "    tfidf_df = vectorizer.fit_transform(df)\n",
    "    print(\"tfidf_df is a {} datatype\".format(type(tfidf_df)))\n",
    "    \n",
    "    tfidf_df_dense = tfidf_df.toarray()\n",
    "    print(\"tfidf_df_dense is a {} datatype\".format(type(tfidf_df_dense)))\n",
    "\n",
    "    return tfidf_df_dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1201,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def text_cleaner(text):\n",
    "    # Visual inspection identifies a form of punctuation spaCy does not\n",
    "    # recognize: the double dash '--'.  Better get rid of it now!\n",
    "    text = re.sub(r'--',' ',text)\n",
    "    text = re.sub(\"[\\[].*?[\\]]\", \"\", text)\n",
    "    text = ' '.join(text.split())\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1202,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def bag_of_words(text):\n",
    "    \n",
    "    # Filter out punctuation and stop words.\n",
    "    allwords = [token.lemma_\n",
    "                for token in text\n",
    "                if not token.is_punct\n",
    "                and not token.is_stop]\n",
    "    \n",
    "    # Return the most common words.\n",
    "    return [item[0] for item in Counter(allwords).most_common(2000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1203,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def bow_features_dev(sentences, common_words):\n",
    "    display(sentences.head(10))\n",
    "    num_sentences_to_print = 10\n",
    "    print(\"inside bow_features: sentences is a {} datatype, of {} length,\\nand common_words is a {} datatype of {} length.\"\n",
    "          .format(type(sentences), sentences.shape[0], type(common_words), len(common_words)))\n",
    "    print(\"here come {} sentences: {}\".format(num_sentences_to_print, sentences[0:num_sentences_to_print]))\n",
    "    # Scaffold the data frame and initialize counts to zero.\n",
    "    df = pd.DataFrame(columns=common_words)\n",
    "    print(\"in bow_features: sentences.iloc[0] = {}\".format(sentences.iloc[0]))\n",
    "    df['text_sentence'] = sentences.iloc[0] # this could be the problem\n",
    "    df['text_source'] = sentences.iloc[1]   # this too could be the problem...\n",
    "    df.loc[:, common_words] = 0\n",
    "    \n",
    "    # Process each row, counting the occurrence of words in each sentence.\n",
    "    num_values = len(list(enumerate(df['text_sentence'])))\n",
    "    print(\"There are {} enumerated items for iterations.\".format(num_values))\n",
    "    for i, sentence in enumerate(df['text_sentence']):\n",
    "        \n",
    "        # Convert the sentence to lemmas, then filter out punctuation,\n",
    "        # stop words, and uncommon words.\n",
    "        words = [token.lemma_\n",
    "                 for token in sentence\n",
    "                 if (\n",
    "                     not token.is_punct\n",
    "                     and not token.is_stop\n",
    "                     and token.lemma_ in common_words\n",
    "                 )]\n",
    "        \n",
    "        # Populate the row with word counts.\n",
    "        for word in words:\n",
    "            df.loc[i, word] += 1\n",
    "        \n",
    "        # This counter is just to make sure the kernel didn't hang.\n",
    "        if i % 50 == 0:\n",
    "            print(\"Processing row {}\".format(i))\n",
    "            \n",
    "    return pd.DataFrame(df)\n",
    "\n",
    "#     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1204,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def bow_features(sentences, common_words):\n",
    "  \n",
    "    # Scaffold the data frame and initialize counts to zero.\n",
    "    df = pd.DataFrame(columns=common_words)\n",
    "    df['text_sentence'] = sentences[0] # this could be the problem\n",
    "    df['text_source'] = sentences[1]   # this too could be the problem...\n",
    "    df.loc[:, common_words] = 0\n",
    "    \n",
    "    # Process each row, counting the occurrence of words in each sentence.\n",
    "    for i, sentence in enumerate(df['text_sentence']):\n",
    "        \n",
    "        # Convert the sentence to lemmas, then filter out punctuation,\n",
    "        # stop words, and uncommon words.\n",
    "        words = [token.lemma_\n",
    "                 for token in sentence\n",
    "                 if (\n",
    "                     not token.is_punct\n",
    "                     and not token.is_stop\n",
    "                     and token.lemma_ in common_words\n",
    "                 )]\n",
    "        \n",
    "        # Populate the row with word counts.\n",
    "        for word in words:\n",
    "            df.loc[i, word] += 1\n",
    "        \n",
    "        # This counter is just to make sure the kernel didn't hang.\n",
    "        if i % 50 == 0:\n",
    "            print(\"Processing row {}\".format(i))\n",
    "            \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1205,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def run_mnb(X_train=None, X_test=None, y_train=None, y_test=None, cross_validate=None, params=None):\n",
    "\n",
    "    mnb = MultinomialNB()\n",
    "    \n",
    "    if params == True:\n",
    "        print(\"In run_rf, params = {}\".format(params)) \n",
    "        mnb.set_params(**params)\n",
    "        \n",
    "    mnb_fit = mnb.fit(X_train, y_train)\n",
    "        \n",
    "#     training_score = mnb.score(X_train, y_train) \n",
    "#     printFormatted(\"### Training score = {:.2%}\".format(training_score))\n",
    "    \n",
    "#      ## Let's score it with the test data set\n",
    "#     test_score = mnb.score(X_test, y_test)\n",
    "    \n",
    "    #   Let's score it with the test data set    this is new 13-Aug-2019\n",
    "    print_training_and_test_scores(mnb_fit, X_train, X_test, y_train, y_test) # new on 13-Aug-2019\n",
    "    \n",
    "#   Let's produce the metrics scores\n",
    "    print_metrics_score(mnb_fit, X_train, X_test, y_train, y_test) # new on 13-Aug-2019\n",
    "    \n",
    "    if cross_validate == True:\n",
    "        print_cross_validation_scores(mnb_fit, X_train, X_test, y_train, y_test)\n",
    "        \n",
    "    if confusion_matrix == True:\n",
    "        confusion_matrix_function(mnb_fit, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1206,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def word2vec_function(sentences, workers=4, min_count=10, window=6, sg=0, sample=1e-3, size=300, hs=1):\n",
    "# import gensim\n",
    "# from gensim.models import word2vec\n",
    "# original values from the curriculum:\n",
    "# workers = 4, min_count=10, window=6, sg=0, sample=1e-3, size=300, hs=1\n",
    "\n",
    "    model = word2vec.Word2Vec(\n",
    "        sentences,\n",
    "        workers=workers,     # Number of threads to run in parallel (if your computer does parallel processing).\n",
    "        min_count=min_count,  # Minimum word count threshold.\n",
    "        window=window,      # Number of words around target word to consider.\n",
    "        sg=sg,          # Use CBOW because our corpus is small.\n",
    "        sample=sample ,  # Penalize frequent words.\n",
    "        size=size,      # Word vector length.\n",
    "        hs=hs           # Use hierarchical softmax.\n",
    "    )\n",
    "\n",
    "    print('done!')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1207,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def vectorizer_nb(type_of_vectorizer):\n",
    "\n",
    "    print_timestamp(BegTimeStamp)\n",
    "    \n",
    "    # 1. import and instantiate CountVectorizer (with the default parameters)\n",
    "\n",
    "    # 2. instantiate CountVectorizer (vectorizer)\n",
    "\n",
    "#     X = df.message\n",
    "#     y = df.sentiment_label\n",
    "\n",
    "    # split X and y into training and testing sets\n",
    "    # by default, it splits 75% training and 25% test\n",
    "    # random_state=1 for reproducibility\n",
    "    \n",
    "#     X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n",
    "\n",
    "    # 3. fit & transform\n",
    "    if type_of_vectorizer == 'Count':\n",
    "        print(\"We are running with CountVectorizer\")\n",
    "        vectorizer = CountVectorizer()\n",
    "        vectorizer.fit(X_train)\n",
    "        vectorizer_method = 'CountVectorizer'\n",
    "    elif type_of_vectorizer == 'Tfidf':\n",
    "        print(\"We are running with TfidfVectorizer\")\n",
    "        vectorizer = TfidfVectorizer()\n",
    "        vectorizer.fit_transform(X_train)\n",
    "        vectorizer_method = 'TfidfVectorizer'\n",
    "    \n",
    "    # 4. transform training data\n",
    "    X_train_dtm = vectorizer.transform(X_train)\n",
    "\n",
    "    # equivalently: combine fit and transform into a single step\n",
    "    # this is faster and what most people would do\n",
    "    X_train_dtm = vectorizer.fit_transform(X_train)\n",
    "\n",
    "    # 4. transform testing data (using fitted vocabulary) into a document-term matrix\n",
    "    X_test_dtm = vectorizer.transform(X_test)\n",
    "\n",
    "    # 1. import\n",
    "\n",
    "    # 2. instantiate a Multinomial Naive Bayes model\n",
    "    nb = MultinomialNB()\n",
    "\n",
    "    # 3. train the model \n",
    "    # using X_train_dtm (timing it with an IPython \"magic command\")\n",
    "\n",
    "    nb.fit(X_train_dtm, y_train)\n",
    "    \n",
    "    \n",
    "    # 4. make class predictions for X_test_dtm\n",
    "    y_pred_class = nb.predict(X_test_dtm)\n",
    "\n",
    "    # calculate accuracy of class predictions\n",
    "\n",
    "    met_test_score = metrics.accuracy_score(y_test, y_pred_class)\n",
    "    printFormatted('###  With {} vectorizer, the metrics accuracy score = {:.2%}'.format(vectorizer_method,\n",
    "                                                                                         met_test_score))\n",
    "    \n",
    "    print_timestamp(EndTimeStamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1208,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def display_column_names(label, df):\n",
    "    display(\"Label: {}: Column names are:\".format(label), df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1209,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def display_datatype(var):\n",
    "    # this function just returns the data type of the variable var\n",
    "    return \"{}\".format(type(var))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1210,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def display_dataframe_shape(label, df):\n",
    "    display(\"Label: {}: Dataframe shape is:\".format(label), df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1211,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def run_it(X_train, X_test, y_train, y_test, y):\n",
    "    \n",
    "#     file_stuff()\n",
    "    \n",
    "#     data_cleanup()\n",
    "    \n",
    "    print_timestamp('\\n'*3+'Begin')\n",
    "    \n",
    "    if Regression == True:\n",
    "        print_timestamp(\"We are running with a Regression model\")\n",
    "    elif Regression == False:\n",
    "        print_timestamp(\"We are running with a Classifier model\")\n",
    "    else:\n",
    "        print_timestamp(\"We have failed to set the Regression variable\")\n",
    "        sys.exit(main())\n",
    "        \n",
    "\n",
    "    if flag_to_plot_them == True:\n",
    "        plot_them()\n",
    "\n",
    "    if flag_to_run_features_importance == True:\n",
    "        \n",
    "        number_of_features_to_consider = 50\n",
    "        params = {'n_estimators': 100}\n",
    "\n",
    "        if Regression == True:\n",
    "            print_timestamp('We are running RandomForestRegressor')\n",
    "            rf = ensemble.RandomForestRegressor(**params)\n",
    "            \n",
    "        else:\n",
    "            print_timestamp('We are running RandomForestClassifier')\n",
    "            rf = ensemble.RandomForestClassifier(**params)\n",
    "\n",
    "        run_features_importance(rf, number_of_features_to_consider)\n",
    "\n",
    "    if flag_to_run_correlation_matrix == True:\n",
    "        run_correlation_matrix()\n",
    "        \n",
    "    if flag_to_run_sentiment_analyzer == True:\n",
    "        path = \"B\"\n",
    "\n",
    "\n",
    "        for path in ['A']:\n",
    "            for vectorizer_iterator in ['logit', 'mlb', 'bnb']:\n",
    "                if vectorizer_iterator == 'rfc':\n",
    "                    sentiment_analyzer(path=path, parameters=params, classifier=vectorizer_iterator, tfidf_parms=tfidf_parms)\n",
    "\n",
    "                elif vectorizer_iterator == 'bnb':\n",
    "                    parameters = {}\n",
    "                    sentiment_analyzer(path=path, parameters=parameters, classifier=vectorizer_iterator, tfidf_parms=tfidf_parms)\n",
    "\n",
    "                elif vectorizer_iterator == 'mlb':\n",
    "                    parameters = {}\n",
    "                    sentiment_analyzer(path=path, parameters=parameters, classifier=vectorizer_iterator, tfidf_parms=tfidf_parms)\n",
    "\n",
    "                elif vectorizer_iterator == 'logit': # newton-cg took too long. sag and saga about the same as lbfgs.\n",
    "                    tfidf_parms = {'max_features' :  10000 } # determined this through iterative testing\n",
    "                    parameters = {'C' :1e20, 'solver': 'lbfgs', 'max_iter': 1000} # max_iter=100 reports warning, try 1000\n",
    "                    sentiment_analyzer(path=path, parameters=parameters, classifier=vectorizer_iterator, tfidf_parms=tfidf_parms)\n",
    "\n",
    "                elif vectorizer_iterator == 'svc':\n",
    "                    parameters = {}\n",
    "                    sentiment_analyzer(path=path, parameters=parameters, classifier=vectorizer_iterator, tfidf_parms=tfidf_parms)\n",
    "                    if confusion_matrix != None:\n",
    "                        confusion_matrix_function(y_test, y_pred_class)\n",
    "\n",
    "    if flag_to_run_rf == True:\n",
    "        #     params = {}\n",
    "        params = {'n_estimators': 100} \n",
    "\n",
    "        if Regression == True:\n",
    "            rf = ensemble.RandomForestRegressor(**params)\n",
    "            print_timestamp('We are running RandomForestRegressor')\n",
    "        else:\n",
    "            rf = ensemble.RandomForestClassifier(**params)\n",
    "            print_timestamp('We are running RandomForestClassifier')\n",
    "\n",
    "        run_rf(rf)\n",
    "\n",
    "    if flag_to_run_gradient_boosting  == True:\n",
    "        run_gradient_boosting()\n",
    "\n",
    "    if flag_to_run_linear_regression  == True:\n",
    "        run_linear_regression()\n",
    "\n",
    "    if flag_to_run_logistic_regression == True:\n",
    "        run_logistic_regression()\n",
    "\n",
    "    if flag_to_run_svc == True:\n",
    "        run_svc() \n",
    "\n",
    "    if flag_to_run_ridge_regression == True:\n",
    "        run_ridge_regression()\n",
    "        \n",
    "    if flag_to_run_vectorizer_nb == True:\n",
    "        for vectorizer_iterator in ['Count', 'Tfidf']:\n",
    "            vectorizer_nb(vectorizer_iterator)\n",
    "        \n",
    "    if flag_to_run_kmeans == True:\n",
    "        method = KMeans(\n",
    "             n_clusters=num_clusters\n",
    "#                 ,random_state=42\n",
    "#                 ,init='random'\n",
    "#                 ,n_init=10\n",
    "#                 ,max_iter=300\n",
    "#                 ,tol=1e-04 \n",
    "        )\n",
    "        df1 = run_kmeans(X_train, y_train, num_clusters)\n",
    "        plot_it_clusters(df1, xvalue=16, yvalue=16, title=\"KMeans with number of clusters = {}\".format(num_clusters))\n",
    "        display(\"next plot please\")\n",
    "\n",
    "    if flag_to_run_affinity_propagation == True:\n",
    "        display_column_names('columns of X_train going into affinity_propagation: ', X_train)\n",
    "        df2, ap_num_clusters = run_affinity_propagation(X_train, y_train)\n",
    "        plot_it_clusters(df2, xvalue=16, yvalue=16, title=\"Affinity Propagation with number of clusters = {}\".format(ap_num_clusters))\n",
    "        \n",
    "    if flag_to_run_mean_shift == True:\n",
    "        df3, mean_shift_num_clusters = run_mean_shift(X_train, y_train)\n",
    "        plot_it_clusters(df3, xvalue=16, yvalue=16, title=\"Mean Shift with number of clusters = {}\".format(mean_shift_num_clusters))\n",
    "    \n",
    "    if flag_to_run_spectral_clustering == True:\n",
    "        df4 = run_spectral_clustering(X_train, y_train, K=num_clusters)\n",
    "        plot_it_clusters(df4, xvalue=16, yvalue=16,title=\"Spectral clustering with number of clusters = {}\".format(num_clusters) )\n",
    "\n",
    "    print_timestamp('End'+'\\n'*3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1212,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def main(entry_point):\n",
    "        \n",
    "    if entry_point == 0:\n",
    "        print_timestamp(\"Starting main()\")\n",
    "#         df = file_stuff()\n",
    "#         data_demographics(df, 5)\n",
    "#         display_column_names('post data_demographics of df', df)\n",
    "#         df, X, y = dataset_cleanup(df)\n",
    "#         display_column_names('post dataset_cleanup on X', X)\n",
    "#         data_demographics(df, 5)\n",
    "#         display_column_names('post data_demographics on X #2', X)\n",
    "# #         make_X_and_Y()\n",
    "#         X_train, X_test, y_train, y_test = training_test_set(X, y)\n",
    "#         display_column_names('after training_test_set: columns of X_train going into affinity_propagation: ', X_train)\n",
    "#         data_characteristics()\n",
    "#         plot_time_to_complete()\n",
    "#         plot_model_accuracy()\n",
    "#         plot_facet()\n",
    "\n",
    "    if flag_to_run_elbow_plot == True:   \n",
    "        do_the_elbow(X)\n",
    "    run_it(X_train, X_test, y_train, y_test, y)\n",
    "        \n",
    "    print_timestamp(\"Ending main()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1213,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def do_word_counts_from_text_files(gutenberg_books, regex=None, book_divisor=1 ):\n",
    "    print_timestamp(\"Starting do_word_counts()\")\n",
    "      \n",
    "    sentences = pd.DataFrame()\n",
    "    \n",
    "    \n",
    "    book_list = [('milton-paradise.txt',r'BOOK .*') , ('melville-moby_dick.txt',r'BOOK .*',)]\n",
    "#     gutenberg_books is a list of files on the computer\n",
    "    for book in book_list:\n",
    "#     paradise_book = 'milton-paradise.txt'\n",
    "#     moby_book = 'melville-moby_dick.txt'\n",
    "\n",
    "        book - gutenberg.raw(book)\n",
    "        book = re.sub(book[1],'', book)\n",
    "#     paradise = gutenberg.raw(paradise_book)\n",
    "#     paradise = re.sub(r'BOOK .*', '', paradise)\n",
    "\n",
    "#     moby = gutenberg.raw(moby_book)\n",
    "#     moby = re.sub(r'BOOK .*', '', moby)\n",
    "        book = text_cleaner(book[:int(len(book)/book_divisor)])\n",
    "\n",
    "\n",
    "    paradise = text_cleaner(paradise[:int(len(paradise)/paradise_divisor)])\n",
    "    moby     = text_cleaner(moby[:int(len(moby)/moby_divisor)])\n",
    "\n",
    "    if debug == True:\n",
    "        print(\"length of paradise is {}, and length of moby is {}.\".format(len(paradise), len(moby)))\n",
    "\n",
    "    paradise_doc = nlp(paradise)\n",
    "    moby_doc = nlp(moby)\n",
    "\n",
    "    paradise_sents = [[sent, \"Milton\"] for sent in paradise_doc.sents]\n",
    "    moby_sents = [[sent, \"Melville\"] for sent in moby_doc.sents]\n",
    "\n",
    "    print(\"paradise_sents is a {} datatype, and moby_sents is a {} datatype.\".format(display_datatype(paradise_sents), display_datatype(moby_sents)))\n",
    "    # a better way to do this is with pd.append... from https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.append.html\n",
    "#     >>> df = pd.DataFrame([[1, 2], [3, 4]], columns=list('AB'))\n",
    "#     >>> df\n",
    "#        A  B\n",
    "#     0  1  2\n",
    "#     1  3  4\n",
    "#     >>> df2 = pd.DataFrame([[5, 6], [7, 8]], columns=list('AB'))\n",
    "#     >>> df.append(df2, ignore_index=True)\n",
    "#            A  B\n",
    "#         0  1  2\n",
    "#         1  3  4\n",
    "#         2  5  6\n",
    "#         3  7  8\n",
    "    sentences = pd.DataFrame(paradise_sents + moby_sents) \n",
    "    sentences = pd.DataFrame(paradise_sents)\n",
    "    sentences.append(moby_sents, ignore_index=True)\n",
    "    \n",
    "    if debug == True:\n",
    "        display(\"Here is a sample from sentences:\\n\", sentences.sample(20))  \n",
    "        print(\"sentences is a {} datatype.\".format(display_datatype(sentences)))\n",
    "        sentences.head(30)\n",
    "\n",
    "    paradisewords = bag_of_words(paradise_doc)\n",
    "    mobywords = bag_of_words(moby_doc)\n",
    "    common_words = list(set(paradisewords + mobywords))\n",
    "    word_counts = bow_features(sentences, common_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1214,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def main_nlp_old(entry_point):\n",
    "    \n",
    "    # this function was specifically designed for this NLP Challenge\n",
    "    \n",
    "    if entry_point == 0:\n",
    "        confusion_matrix = None\n",
    "        \n",
    "        paradise_divisor = 20 # was 20\n",
    "        moby_divisor     = 53 # was 53\n",
    "        \n",
    "        print_timestamp(\"Starting main_nlp()\")\n",
    "        \n",
    "        paradise_book = 'milton-paradise.txt'\n",
    "        moby_book = 'melville-moby_dick.txt'\n",
    "        \n",
    "        paradise = gutenberg.raw(paradise_book)\n",
    "        paradise = re.sub(r'BOOK .*', '', paradise)\n",
    "\n",
    "        moby = gutenberg.raw(moby_book)\n",
    "        moby = re.sub(r'BOOK .*', '', moby)\n",
    "        \n",
    "        paradise = text_cleaner(paradise[:int(len(paradise)/paradise_divisor)])\n",
    "        moby     = text_cleaner(moby[:int(len(moby)/moby_divisor)])\n",
    "        \n",
    "        if debug == True:\n",
    "            print(\"length of paradise is {}, and length of moby is {}.\".format(len(paradise), len(moby)))\n",
    "        \n",
    "        paradise_doc = nlp(paradise)\n",
    "        moby_doc = nlp(moby)\n",
    "        \n",
    "        paradise_sents = [[sent, \"Milton\"] for sent in paradise_doc.sents]\n",
    "        moby_sents = [[sent, \"Melville\"] for sent in moby_doc.sents]\n",
    "\n",
    "        print(\"paradise_sents is a {} datatype, and moby_sents is a {} datatype.\".format(display_datatype(paradise_sents), display_datatype(moby_sents)))\n",
    "        sentences = pd.DataFrame(paradise_sents + moby_sents) \n",
    "        if debug == True:\n",
    "            display(\"Here is a sample from sentences:\\n\", sentences.sample(20))  \n",
    "            print(\"sentences is a {} datatype.\".format(display_datatype(sentences)))\n",
    "            sentences.head(30)\n",
    "        \n",
    "        paradisewords = bag_of_words(paradise_doc)\n",
    "        mobywords = bag_of_words(moby_doc)\n",
    "        common_words = list(set(paradisewords + mobywords))\n",
    "        word_counts = bow_features(sentences, common_words)\n",
    "        \n",
    "        if debug == True:  display(\"words_counts sample:\\n\", word_counts.sample(20))\n",
    "        word_counts['author'] = np.where(word_counts['text_source'] == 'Milton', pd.to_numeric(1), pd.to_numeric(0))\n",
    "\n",
    "        X = np.array(word_counts.drop(['text_sentence', 'text_source', 'author'], 1))\n",
    "        Y = word_counts['author']\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.4, random_state=0)\n",
    "        \n",
    "        for runit in range(0,2):\n",
    "            if runit == 0:\n",
    "                \n",
    "                n_estimators = 100\n",
    "                \n",
    "                params = {'n_estimators': n_estimators}\n",
    "                printFormatted('## Running Random Forests with Word Counts')\n",
    "                run_rf(X_train=X_train, X_test=X_test, y_train=y_train, y_test=y_test, params=params, cross_validate=True)\n",
    "\n",
    "                printFormatted('## Running MultiNomial Naive Bayes with Word Counts')\n",
    "                params = {}\n",
    "                run_mnb(X_train=X_train, X_test=X_test, y_train=y_train, y_test=y_test, params=params, cross_validate=True)\n",
    "\n",
    "                printFormatted('## Running Bernoulli Naive Bayes with Word Counts')\n",
    "                run_BernoulliNB(X_train=X_train, X_test=X_test, y_train=y_train, y_test=y_test, params=params, cross_validate=True) \n",
    "       \n",
    "            else:\n",
    "            \n",
    "                author = []\n",
    "                all_paras = []\n",
    "                for paragraph in gutenberg.paras(paradise_book):\n",
    "                    para = paragraph[0]\n",
    "                    para = [re.sub(r'--', '', word) for word in para]\n",
    "                    all_paras.append(' '.join(para))\n",
    "                    author.append('Milton')\n",
    "\n",
    "                for paragraph in gutenberg.paras(moby_book):\n",
    "                    para = paragraph[0]\n",
    "                    para = [re.sub(r'--', '', word) for word in para]\n",
    "                    all_paras.append(' '.join(para))\n",
    "                    author.append('Melville')\n",
    "\n",
    "                paragraphs = pd.DataFrame()        \n",
    "                paragraphs['paragraphs'] = all_paras\n",
    "                paragraphs['author'] = author\n",
    "                paragraphs['author'] = np.where(paragraphs['author'] == 'Milton', pd.to_numeric(1), pd.to_numeric(0))\n",
    "                \n",
    "                if debug == True:\n",
    "                    display(\"paragraphs sampe:\\n\", paragraphs.sample(20))\n",
    "                    display(\"describe paragraphs\\n\", paragraphs.describe())\n",
    "\n",
    "                X = paragraphs['paragraphs']\n",
    "                Y = paragraphs['author']\n",
    "                printFormatted('## Running with Paragraph Counts and tfidf and others')\n",
    "\n",
    "                X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.4, random_state=0)\n",
    "                params = {}\n",
    "                tfidf_parms = {}\n",
    "                for vectorizer_iterator in ['rfc', 'bnb', 'mlb', 'svc']:\n",
    "\n",
    "                    path = 'A'\n",
    "                    if vectorizer_iterator == 'rfc':\n",
    "                        sentiment_analyzer(path=path, parameters=params, classifier=vectorizer_iterator, tfidf_parms=tfidf_parms, \n",
    "                                           X_train=X_train, X_test=X_test, y_train=y_train, y_test=y_test, cross_validate=True)\n",
    "\n",
    "                    elif vectorizer_iterator == 'bnb':\n",
    "                        parameters = {}\n",
    "                        sentiment_analyzer(path=path, parameters=params, classifier=vectorizer_iterator, tfidf_parms=tfidf_parms, \n",
    "                                           X_train=X_train, X_test=X_test, y_train=y_train, y_test=y_test, cross_validate=True)\n",
    "\n",
    "                    elif vectorizer_iterator == 'mlb':\n",
    "                        parameters = {}\n",
    "                        sentiment_analyzer(path=path, parameters=params, classifier=vectorizer_iterator, tfidf_parms=tfidf_parms, \n",
    "                                           X_train=X_train, X_test=X_test, y_train=y_train, y_test=y_test, cross_validate=True)\n",
    "                        \n",
    "                    elif vectorizer_iterator == 'logit': # newton-cg took too long. sag and saga about the same as lbfgs.\n",
    "                        tfidf_parms = {'max_features' : 10000 } # determined this through iterative testing\n",
    "                        \n",
    "                        parameters = {'C' :1e20, 'solver': 'lbfgs', 'max_iter': 1000} # max_iter=100 reports warning, try 1000\n",
    "                        sentiment_analyzer(path=path, parameters=parameters, classifier=vectorizer_iterator, tfidf_parms=tfidf_parms, \n",
    "                                           X_train=X_train, X_test=X_test, y_train=y_train, y_test=y_test, cross_validate=True)\n",
    "\n",
    "                    elif vectorizer_iterator == 'svc':\n",
    "                        parameters = {}\n",
    "                        sentiment_analyzer(path=path, parameters=parameters, classifier=vectorizer_iterator, tfidf_parms=tfidf_parms, \n",
    "                                           X_train=X_train, X_test=X_test, y_train=y_train, y_test=y_test, cross_validate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1215,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# main_nlp(0)\n",
    "# print(\"it ran ok, dude!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1216,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "if debug == 300:\n",
    "    import os\n",
    "    import pandas as pd\n",
    "    import nltk\n",
    "\n",
    "    from nltk.corpus.reader.plaintext import PlaintextCorpusReader\n",
    "\n",
    "    debug = 16\n",
    "    big_paras = []\n",
    "\n",
    "    corpusdir = '/Users/lou/GITHubProjects/Thinkful/Datafiles/UnsupervisedLearningCapstone/fiction_corpus/'\n",
    "    fiction_corpus = PlaintextCorpusReader(corpusdir, '.*.txt') \n",
    "    documents_stat = fiction_corpus.fileids()\n",
    "\n",
    "    paragraph_book = pd.DataFrame()\n",
    "    paragraph_list = []\n",
    "    chapter_heading = u\"^[C][Hh][aA][pP][tT][eE][Rr].*\"\n",
    "    chapter_first = u\"^[C][Hh][aA][pP][tT][eE][Rr] [I][O][F][1]\"\n",
    "\n",
    "    for book in documents_stat: #documents_stat:\n",
    "        print(\"Now processing book={}\".format(book))\n",
    "        paragraph_list = []\n",
    "        paragraph_number = -1 # used for page counting\n",
    "        EndDisplay = False\n",
    "        endparagraph = -1 # used for the end paragraph number\n",
    "        paragraphcounter = -1 # used to count the number of paragraphs in the book\n",
    "        chapter = 1\n",
    "        book_chapters = pd.DataFrame()\n",
    "        chapters = []\n",
    "        out_of_the_header = False\n",
    "        print(\"book={}\".format(book))\n",
    "        book_details=book.split('_')\n",
    "        print(\"book_details={}\".format(book_details))\n",
    "        author_number = int(int(book_details[0]) * 100)\n",
    "        book_number = int(int(book_details[2]) * 10)\n",
    "        author_booknumber = int(int(book_details[0]) * 100 + int(book_details[2]) * 10)\n",
    "        print(\"author_number={}, book_number={}, author_booknum={}\".format(author_number,\n",
    "                                                                           book_number,\n",
    "                                                                           author_booknumber))\n",
    "\n",
    "        # get the number of chapters in the book\n",
    "        for paras in fiction_corpus.paras(book): \n",
    "            paragraph = \" \".join(paras[0])\n",
    "            paragraphcounter += 1\n",
    "            if debug & 64:   \n",
    "                print(\"paragraphcounter={}\".format(paragraphcounter))\n",
    "                print(\"paragraph={}\".format(paragraph))\n",
    "\n",
    "            if u\"*** END\" in paragraph.upper():\n",
    "                endparagraph = paragraphcounter\n",
    "                print(\"*** THERE ARE {} chapters in the book {}\".format(endparagraph, book))\n",
    "                if debug:    \n",
    "                    print(\"endparagraph = {}\".format(endparagraph))\n",
    "                break\n",
    "\n",
    "        # iterate through all of the paragraphs in the current book\n",
    "        for paras in fiction_corpus.paras(book):\n",
    "    #         print(\"author={}, book={},chapter={}, len(chapters)={} at paras+1\".format(author_number, book_number, chapter, len(chapters)))\n",
    "            paragraph = \" \".join(paras[0])\n",
    "            paragraph_number += 1\n",
    "            if debug & 16:    print(\"debug 16:  paragraph_number now equals {}, paragraph={}\".format(paragraph_number, \n",
    "                                                                                                     paragraph.upper()))\n",
    "\n",
    "            if out_of_the_header is False and chapter_heading not in paragraph.upper():\n",
    "                if debug & 32:    print(\"xy23: paragraph_number={}, and paragraph={}\".format(paragraph_number, paragraph))\n",
    "                still_in_the_gutenburg_header = True\n",
    "                # ^[C][Hh][aA][pP][tT][eE][Rr]\n",
    "            elif (paragraph.upper() in u\"^[C][Hh][aA][pP][tT][eE][Rr] [I][1][O]\") or out_of_the_header is True: # first paragraph for the book\n",
    "                if u\"CHAPTER I\" in paragraph.upper():\n",
    "                    out_of_the_header = True\n",
    "                    if debug & 128:    print(\"We are out of chapter 0, here dude...\")\n",
    "\n",
    "    #             paragraph_number += 1 # we are on the next chapter....\n",
    "                if paragraph_number > endparagraph: # we have we gone past the end of the actual book,write to the book_chapters data frame\n",
    "                    print(\"\\n #### BREAK ENCOUNTERED !!!\\nAnd we are at paragraph_number={}, and we are in book:{}\\n\".format(paragraph_number,book))\n",
    "                    print(\"endparagraph = {}\".format(endparagraph))\n",
    "\n",
    "                    for chapter_item in chapters: \n",
    "                        book_chapters.append(chapter_item, ignore_index=True)\n",
    "                    print(\"we just processed {} chapter_items in chapters.\".format(len(chapters)))\n",
    "                    chapters = []\n",
    "    #                 break\n",
    "                else: # not in the last paragraph\n",
    "                    paragraph_list.append(paragraph)\n",
    "                    print(\"paragraph_list[0]={}\".format(paragraph_list[0]))\n",
    "                    if \"CHAPTER\" in paragraph.upper(): # this is the beginning of a new chapter - write the current chapter to the chapters list\n",
    "                        chapter += 1\n",
    "                        if debug:  print(\"chapter is now {}\".format(chapter))\n",
    "                        full_chapter_from_paragraph = ''\n",
    "                        for paragaph_item in paragraph_list:\n",
    "                            full_chapter_from_paragraph += \" \" + paragaph_item\n",
    "                        chapters = chapters.append([author_number, book_number, author_booknumber, chapter, full_chapter_from_paragraph ])\n",
    "                        print(\"aye just processed {} paragraph_list items.\".format(len(paragraph_list)))\n",
    "\n",
    "    print(\"The books have all\\nBeen loaded.\\nSir.\\nAnything else for the evening?\\n\")\n",
    "    paragraph_book.columns = ['author','book','author_book','chapter','chapter_text']\n",
    "    print(\"At the end, and there are {} lines in book_chapters\".format(book_chapters.shape))\n",
    "\n",
    "    display(book_chapters.head(30))\n",
    "    display(book_chapters.tail(30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1217,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(string): \n",
    "  \n",
    "    # punctuation marks \n",
    "    punctuations = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~'''\n",
    "\n",
    "    # traverse the given string and if any punctuation \n",
    "    # marks occur replace it with null \n",
    "    for x in string.lower(): \n",
    "        if x in punctuations: \n",
    "            string = string.replace(x, \"\")\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1218,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shoehorn_unicode_into_ascii(s):\n",
    "    return unicodedata.normalize('NFKD', s).encode('ascii','ignore').decode('utf-8', 'ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1219,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_whitespace(s):\n",
    "    s2 = str(s).replace(r'\\.|\\!|\\?|\\'|,|-|\\(|\\)',\" \")\n",
    "    s2.replace('\\n',' ').replace('\\r',' ')\n",
    "    s2 = [''.join(c for c in s if c not in string.punctuation) for s in s2]\n",
    "    return str(s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1220,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a module for Text Processing\n",
    "def text_process(tex):\n",
    "    # 1. Removal of Punctuation Marks\n",
    "    nopunct = [char for char in tex if char not in string.punctuation]\n",
    "    nopunct = ''.join(nopunct)\n",
    "    lemmatiser = WordNetLemmatizer()\n",
    "    # 2. Lemmatisation\n",
    "    a = ''\n",
    "    i = 0\n",
    "    for i in range(len(nopunct.split())):\n",
    "        b = lemmatiser.lemmatize(nopunct.split()[i], pos=\"v\")\n",
    "        a = a + b + ' '\n",
    "\n",
    "    #3 Removal of Stopwords\n",
    "    return [word for word in a.split() if word.lower() not in stopwords.words('english')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1221,
   "metadata": {},
   "outputs": [],
   "source": [
    "def w2vec_vectorize_text(sentences=None, size=None, min_count=1, sg=1, seed=57,\n",
    "                         workers=4):\n",
    "    \n",
    "    m = Word2Vec(sentences=sentences, size=size, min_count=min_count,\n",
    "                 sg=sg, seed=seed)\n",
    "    \n",
    "    def word_vectorizer3(sent, m):\n",
    "        vec = []\n",
    "        numw = 0\n",
    "        for w in sent:\n",
    "            try:\n",
    "                if numw == 0:\n",
    "                    vec = m[w]\n",
    "                else:\n",
    "                    vec = np.add(vec, m[w])\n",
    "                numw += 1\n",
    "            except:\n",
    "                pass\n",
    "            return np.asarray(vec) / numw\n",
    "    \n",
    "    l = []\n",
    "\n",
    "    for i in tqdm(sentences):\n",
    "        l.append(word_vectorizer3(i, m))\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1222,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_nlp(entry_point=0):\n",
    "    \n",
    "    print_timestamp('\\n'*3+'Begin')\n",
    "    \n",
    "    debug = 2\n",
    "    big_paras = []\n",
    "\n",
    "    corpusdir = '/Users/lou/GITHubProjects/Thinkful/Datafiles/UnsupervisedLearningCapstone/fiction_corpus/'\n",
    "    fiction_corpus = PlaintextCorpusReader(corpusdir, '.*.txt') \n",
    "    documents_stat = fiction_corpus.fileids()\n",
    "    if debug == 11:    print(\"documents_stat={} and is a {} datatype\".format(documents_stat, type(documents_stat)))\n",
    "    documents_stat_0 = []\n",
    "    documents_stat_0.append(documents_stat[0])\n",
    "\n",
    "    if debug == 12:    print(\"documents_stat_0 is a {} datatype\".format(type(documents_stat)))\n",
    "    item_num = 0\n",
    "\n",
    "    book_block = []\n",
    "    word_counts= {}\n",
    "\n",
    "    for book in documents_stat:\n",
    "        item_num += 1\n",
    "\n",
    "        if debug== 13:    print(\"item_num={}\".format(item_num))\n",
    "    #    big_fiction = fiction_corpus.raw(book)\n",
    "        big_fiction_words = fiction_corpus.words(book)\n",
    "\n",
    "        if debug == 14:    print(\"big_fiction is a {} datatype\".format(type(big_fiction_words)))\n",
    "    #    big_fiction = str(big_fiction, 'utf-8', 'ignore')\n",
    "        big_fiction_len= len(big_fiction_words)\n",
    "        book_items = book.split('_')\n",
    "        author = book_items[0]\n",
    "        book_name = book_items[3]\n",
    "        wordcount1 = len(big_fiction_words)\n",
    "        if debug == 15:    print(\"big_fiction is words={}\".format(len(big_fiction_words)))\n",
    "\n",
    "        num_blocks = 128\n",
    "#         t_list = []\n",
    "        length = 500\n",
    "        interblock = 250\n",
    "        \n",
    "        for i in range(1,num_blocks):\n",
    "            if i == 1:\n",
    "                start =  1000\n",
    "                end = start + length\n",
    "            else:\n",
    "                start =  interblock + end\n",
    "                end = start + length \n",
    "    #         t_list.append((start, start+length))\n",
    "\n",
    "            big_fictionwords0 = big_fiction_words\n",
    "            if debug == 16:    print(\"big_fictionwords0 is a {} datatype\".format(type(big_fictionwords0)))\n",
    "\n",
    "    #         print(\"words {} thru {} are: {}\".format(2000, 3000, \" \".join(big_fiction_words[2000:3000])))\n",
    "            if debug == 17:    print(\"start={}, end={}\".format(start, end))\n",
    "            book_text = \" \".join(big_fiction_words[start:end]).lower()\n",
    "            book_text = remove_punctuation(book_text)\n",
    "#             book_text = text_process(book_text)\n",
    "            book_text = str(shoehorn_unicode_into_ascii(str(book_text)))\n",
    "\n",
    "\n",
    "\n",
    "            book_block.append([int(author), book_name, int(i), str(book_text)])\n",
    "            if debug == 18:  print(\"the length of zooga book_block is now {}\".format(len(book_block)))\n",
    "            if debug == 1 and i == 5:\n",
    "                print(\"book_name={}, book_text={}\".format(book_name, book_text))\n",
    "\n",
    "    if debug == 19:    print(\"book_block length is {}\".format(len(book_block)))\n",
    "\n",
    "    if debug == 20:    print(\"by the way, there are {} book_blocks.\".format(len(book_block)))\n",
    "        \n",
    "#     for book_block_item in book_block:\n",
    "#         if debug == 21:    print(\"author = {}, book={}, block={}.\".format(book_block_item[0],\n",
    "#                                                        book_block_item[1],\n",
    "#                                                        book_block_item[2]))\n",
    "#         book_block_key = str(book_block_item[0])+str(book_block_item[1]+str(book_block_item[2]))\n",
    "#         word_counts[book_block_key] = len(book_block_item[3].split(' '))\n",
    "\n",
    "    if debug == 22:\n",
    "        for book, word_counted in word_counts.items():\n",
    "            print(\"book={}, word_count = {}\".format(book, word_counted))\n",
    "\n",
    "#         with open(corpusdir + 'corpus_test.TXT', 'w') as corpustext:\n",
    "#             for file_lines in book_block:\n",
    "#                 corpustext.writelines(str(file_lines[0])+','+str(file_lines[1])+','+str(file_lines[2])+','+str(file_lines[3]+'\\n'))  \n",
    "\n",
    "        print(\"there were {} words in the last big_fiction.\".format(len(big_fiction_words)))\n",
    "        print(\"words {} thru {} are: {}\".format(2000, 3000, \" \".join(big_fiction_words[2000:3000])))\n",
    "    if debug == 23:  display(\"the length of book_block was {}\".format(len(book_block)))\n",
    "    \n",
    "    df_book_blocks = pd.DataFrame(book_block, columns=['author', 'book', 'book_block', 'block_text'] )\n",
    "    df_book_blocks['block_text_length'] = df_book_blocks['block_text'].apply(len)\n",
    "    display(\"## book_block:\", df_book_blocks.head(10))\n",
    "    \n",
    "#     run_it(X_train, X_test, y_train, y_test)\n",
    "    \n",
    "    df_book_blocks['block_text'].replace('', np.nan, inplace=True)\n",
    "    df_book_blocks.dropna(axis=1)\n",
    "    df_book_blocks.dropna(how='any', inplace=True)\n",
    "    df_book_blocks.block_text = df_book_blocks.block_text.str.strip()\n",
    "    df_book_blocks.block_text = df_book_blocks.block_text.str.replace('^ +| +$', '_')\n",
    "    df_book_blocks = df_book_blocks[df_book_blocks.block_text != '...']\n",
    "    df_book_blocks['block_text'].sample(20)\n",
    "    display(\"null schtuff\\n:\", df_book_blocks.isnull().sum())\n",
    "    display(\"df_book_blocks.columns\", df_book_blocks.columns)\n",
    "    # tfidf_X = run_tfidf_vectorizer(paragraph_book['paragraph'])\n",
    "    # display(\"tfidf_X is a {} datatype.\".format(type(tfidf_X)))\n",
    "    # display(tfidf_X)\n",
    "    X = df_book_blocks[['book_block', 'block_text']]\n",
    "    if debug:  display(\"shape of X is\", X.shape)\n",
    "    y = df_book_blocks['author']\n",
    "    if debug:  display(\"shape of y is\", y.shape)\n",
    "    # y.sample(20)\n",
    "    # X_train, X_test = train_test_split(tfidf_X)\n",
    "\n",
    "    vectorizer = TfidfVectorizer(\n",
    "#                                     max_df=0.5, \n",
    "#                                     min_df=2, \n",
    "#                                     stop_words='english', \n",
    "#                                     lowercase=True, \n",
    "#                                     use_idf=True, \n",
    "#                                     norm=u'l2', \n",
    "#                                     smooth_idf=True, \n",
    "                                    analyzer=text_process,\n",
    "                                    max_features=5000\n",
    "    )\n",
    "    #                              max_df=max_df, # drop words that occur in more than half the paragraphs\n",
    "    #                              min_df=min_df, # only use words that appear at least twice\n",
    "    #                              stop_words=stop_words, \n",
    "    #                              lowercase=lowercase, #convert everything to lower case (since Alice in Wonderland has the HABIT of CAPITALIZING WORDS for EMPHASIS)\n",
    "    #                              use_idf=use_idf,#we definitely want to use inverse document frequencies in our weighting\n",
    "    #                              norm=norm, #Applies a correction factor so that longer paragraphs and shorter paragraphs get treated equally\n",
    "    #                              smooth_idf=smooth_idf #Adds 1 to all document frequencies, as if an extra document existed that used every word once.  Prevents divide-by-zero errors\n",
    "    #                             )\n",
    "\n",
    "    #Applying the vectorizer\n",
    "\n",
    "    # tfidf_df = vectorizer.fit(df)\n",
    "    display(X['block_text'].head(20))\n",
    "    \n",
    "    display(\"the length of X is {}\".format(X.size))\n",
    "    print(\"here comes {} sample rows from X\".format(10))\n",
    "    display(X.sample(10))\n",
    "    model = gensim.models.Word2Vec(X.block_text, min_count=1, size=50)\n",
    "    \n",
    "    X1 = model.wv.vectors\n",
    "    print(\"############### the length of X1 from model.wv.syn0 is {} ###################, and the type is {}, and shape is {}\".format(len(X1), type(X1), X1.shape))\n",
    "#     print(\"here comes X1\")\n",
    "#     display(X1)        \n",
    "            \n",
    "    NUM_CLUSTERS = 10\n",
    "#     kclusterer = KMeansClusterer(NUM_CLUSTERS, distance=nltk.cluster.util.cosine_distance, repeats=25)\n",
    "#     assigned_clusters = kclusterer.cluster(X, assign_clusters=True)\n",
    "#     print(\"assigned clusters:\\n\", assigned_clusters)\n",
    "    \n",
    "    kmeans = KMeans(n_clusters=NUM_CLUSTERS)\n",
    "#     kmeans_fit = kmeans.fit(X)\n",
    "    kmeans_fit_predict = kmeans.fit_predict(X1)\n",
    "    print(\"kmeans_fit_predict:\\n\", kmeans_fit_predict)\n",
    "    display(\"size of kmeans_fit_predict is {}\".format(len(kmeans_fit_predict)))\n",
    "    print(\"We will now exit.\")\n",
    "    exit(-1)\n",
    "#     X_train, X_test, y_train, y_test = training_test_set(X, y) # split the dataframe\n",
    "#     display(\"X.sample(20)\", X.sample(20))\n",
    "    if debug == 24:\n",
    "        display(\"shape of X_train() is\", X_train.shape)\n",
    "        display(\"shape of X_test is\", X_test.shape)\n",
    "        display(\"X_train.head(20):\", X_train.head(20))\n",
    "#     tfidf_train = vectorizer.fit_transform(X_train['block_text']) # 1A: run fit and transform on the training data set\n",
    "#     tfidf_test_trans = vectorizer.transform(X_test['block_text']) # 1B: now run transoform on the test data set\n",
    "\n",
    "#     tfidf_train_df = pd.DataFrame(tfidf_train.toarray())# , columns=vectorizer.get_feature_names())\n",
    "#     tfidf_test_df = pd.DataFrame(tfidf_test_trans.toarray())# , columns=vectorizer.get_feature_names())\n",
    "#     display(\"tfidf_train_df shape is {}\".format(tfidf_train_df.shape))\n",
    "\n",
    "    K = 10\n",
    "#     if debug:\n",
    "#         print(\"tfidf_train_df shape is {}\".format(tfidf_train_df.shape))\n",
    "\n",
    "#     bow_transformer = CountVectorizer(analyzer=text_process).fit(X_train)\n",
    "#     text_bow_train = bow_transformer.transform(X_train) # ONLY TRAINING DATA\n",
    "    \n",
    "#     text_bow_test = bow_transformer.transform(X_test) #TEST DATA only\n",
    "    \n",
    "    word2vec_transformer = vectorizer =  word2vec_function(X['block_text'], workers=4, min_count=10, window=6, sg=0, sample=1e-3, size=300, hs=1).fit(X_train) # ONLY TRAINING DATA\n",
    "    word2vec_test = word2vec_transformer.transformer(X) # TEST DATA only\n",
    "    chapters = X['block_text']\n",
    "    model - Word2Vec(chapters, min_count=1)\n",
    "    \n",
    "#     kmeans_cluster_test =  KMeans(n_clusters=K,max_iter=1000, random_state=42).fit_predict(tfidf_test_df)\n",
    "#     kmeans_cluster_train = KMeans(n_clusters=K,max_iter=1000, random_state=42).fit_predict(tfidf_train_df)\n",
    "#     print(\"length of text_bow_test is {}\".format(len(text_bow_test.toarray())))\n",
    "#     kmeans_cluster_test =  KMeans(n_clusters=K, random_state=42).fit_predict(text_bow_train.T) #\n",
    "    kmeans_cluster_test =  KMeans(n_clusters=K, random_state=42).fit_predict(word2vec_test) #\n",
    "    \n",
    "    if debug == 25:\n",
    "        print(\"tfidf_train_df is a {} datatype\".format(type(tfidf_train_df)))\n",
    "    cluster_series_test = pd.Series(kmeans_cluster_test)\n",
    "    cluster_series_train = pd.Series(kmeans_cluster_train)\n",
    "\n",
    "    if debug == 26:\n",
    "        print(\"the shape of cluster_series_test is {}\".format(cluster_series_test.shape))\n",
    "        print(\"the shape of cluster_series_train is {}\".format(cluster_series_train.shape))\n",
    "\n",
    "    # need to add in all of the other columns along with the new features, and the cluster.....\n",
    "#     display(\"cluster_series_test head(20)\", cluster_series_test.head(20))\n",
    "    Z_test = pd.merge(cluster_series_test.rename('cluster'),tfidf_test_df,left_index=True, right_index=True)\n",
    "\n",
    "    Z_test2 = pd.merge(X, Z_test,left_index=True, right_index=True)\n",
    "    Z_test2 = pd.merge(y, Z_test2, left_index=True, right_index=True)\n",
    "    Z_test.rename(columns={Z_test.columns[0]: 'cluster'}, inplace=True)\n",
    "    if debug == 27:\n",
    "        print(\"shape of {} is {}, shape of {} is {}, shape of {} is {}\".format(\"tfidf_test_df\", \n",
    "                                                                           tfidf_test_df.shape, \n",
    "                                                                           \"kmeans_cluster_test\", \n",
    "                                                                           kmeans_cluster_test.shape,\n",
    "                                                                           \"Z_test\", Z_test.shape))\n",
    "\n",
    "    Z_train = pd.merge(cluster_series_train.rename('cluster'),tfidf_train_df,left_index=True, right_index=True)\n",
    "    Z_train.rename(columns={Z_train.columns[0]: 'cluster'}, inplace=True)\n",
    "    if debug == 28:\n",
    "        print(\"shape of {} is {}, shape of {} is {}, shape of {} is {}\".format(\"tfidf_train_df\", \n",
    "                                                                           tfidf_train_df.shape, \n",
    "                                                                           \"kmeans_cluster_train\", \n",
    "                                                                           kmeans_cluster_train.shape,\n",
    "                                                                           \"Z_train\", Z_train.shape))\n",
    "    # Z2 = Z.join(X)\n",
    "    # Z2 = pd.merge(Z, X, left_index=True, right_index=True)\n",
    "    # display(Z.sample(20))\n",
    "    if debug == 29:\n",
    "        display(\"shape of Z_test is {}, shape of X is {}\".format(Z_test.shape, X.shape))\n",
    "        display(Z_test.sample(20))\n",
    "    if flag_to_run_elbow_plot == True:\n",
    "        do_the_elbow(Z_test, maxK=20) # skip the author column\n",
    "\n",
    "    if debug == 30:\n",
    "        display(\"shape of Z_train is {}, shape of X is {}\".format(Z_train.shape, X.shape))\n",
    "        display(Z_train.sample(20))\n",
    "\n",
    "    # X_trainZ, X_testZ, y_trainZ, y_testZ = train_test_split(Z.loc[:, ~Z.columns.isin(['author', ''])) # split the dataframe\n",
    "    # df.loc[:, df.columns != 'b']\n",
    "    # df.loc[:, ~df.columns.isin(['col1', 'col2'])]\n",
    "\n",
    "                # display(Z[['cluster']].sample(20))\n",
    "    # Z = pd.merge(Z, pd.DataFrame(predict), left_index=True, right_index=True)\n",
    "    # Z.rename(columns={Z.columns[-1]: 'cluster'}, inplace=True)\n",
    "    # predicted = cluster.predict(Z['cluster'])\n",
    "    # predicted = cluster_series(X_test)\n",
    "    # predicted = cluster.predict(Z)\n",
    "    # display(predicted.head(20))\n",
    "    print(\"### NOW GET OUT OF HERE, YOU KNUCKLEHEAD!!! ###\")\n",
    "    display(Z_test2.head(40))\n",
    "    print_timestamp('\\n'*3+'End')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1223,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_nlp_new(entry_point=0):\n",
    "    \n",
    "    print_timestamp('\\n'*3+'Begin')\n",
    "    num_features = 50\n",
    "    \n",
    "    debug = 2\n",
    "    big_paras = []\n",
    "\n",
    "    corpusdir = '/Users/lou/GITHubProjects/Thinkful/Datafiles/UnsupervisedLearningCapstone/fiction_corpus/'\n",
    "    fiction_corpus = PlaintextCorpusReader(corpusdir, '.*.txt') \n",
    "    documents_stat = fiction_corpus.fileids()\n",
    "    if debug == 11:    print(\"documents_stat={} and is a {} datatype\".format(documents_stat, type(documents_stat)))\n",
    "    documents_stat_0 = []\n",
    "    documents_stat_0.append(documents_stat[0])\n",
    "\n",
    "    if debug == 12:    print(\"documents_stat_0 is a {} datatype\".format(type(documents_stat)))\n",
    "    item_num = 0\n",
    "\n",
    "    book_block = []\n",
    "    word_counts= {}\n",
    "\n",
    "    for book in documents_stat:\n",
    "        item_num += 1\n",
    "\n",
    "        if debug== 13:    print(\"item_num={}\".format(item_num))\n",
    "    #    big_fiction = fiction_corpus.raw(book)\n",
    "        big_fiction_words = fiction_corpus.words(book)\n",
    "\n",
    "        big_fiction_len= len(big_fiction_words)\n",
    "        book_items = book.split('_')\n",
    "        author = book_items[0]\n",
    "        book_name = book_items[3]\n",
    "        wordcount1 = len(big_fiction_words)\n",
    "        if debug == 15:    print(\"big_fiction is words={}\".format(len(big_fiction_words)))\n",
    "\n",
    "        num_blocks = 128\n",
    "#         t_list = []\n",
    "        length = 500\n",
    "        interblock = 250\n",
    "        \n",
    "        for i in range(1,num_blocks):\n",
    "            if i == 1:\n",
    "                start =  1000\n",
    "                end = start + length\n",
    "            else:\n",
    "                start =  interblock + end\n",
    "                end = start + length \n",
    "    #         t_list.append((start, start+length))\n",
    "\n",
    "            big_fictionwords0 = big_fiction_words\n",
    "            if debug == 16:    print(\"big_fictionwords0 is a {} datatype\".format(type(big_fictionwords0)))\n",
    "\n",
    "    #         print(\"words {} thru {} are: {}\".format(2000, 3000, \" \".join(big_fiction_words[2000:3000])))\n",
    "            if debug == 17:    print(\"start={}, end={}\".format(start, end))\n",
    "            book_text = \" \".join(big_fiction_words[start:end]).lower()\n",
    "            book_text = remove_punctuation(book_text)\n",
    "#             book_text = text_process(book_text)\n",
    "            book_text = str(shoehorn_unicode_into_ascii(str(book_text)))\n",
    "\n",
    "\n",
    "\n",
    "            book_block.append([int(author), book_name, int(i), str(book_text)])\n",
    "    \n",
    "    df_book_blocks = pd.DataFrame(book_block, columns=['author', 'book', 'book_block', 'block_text'] )\n",
    "    df_book_blocks['block_text_length'] = df_book_blocks['block_text'].apply(len)\n",
    "    display(\"## book_block:\", df_book_blocks.head(10))\n",
    "    \n",
    "#     run_it(X_train, X_test, y_train, y_test)\n",
    "    \n",
    "    df_book_blocks['block_text'].replace('', np.nan, inplace=True)\n",
    "    df_book_blocks.dropna(axis=1)\n",
    "    df_book_blocks.dropna(how='any', inplace=True)\n",
    "    df_book_blocks.block_text = df_book_blocks.block_text.str.strip()\n",
    "    df_book_blocks.block_text = df_book_blocks.block_text.str.replace('^ +| +$', '_')\n",
    "    df_book_blocks = df_book_blocks[df_book_blocks.block_text != '...']\n",
    "    df_book_blocks['block_text'].sample(20)\n",
    "    display(\"null schtuff\\n:\", df_book_blocks.isnull().sum())\n",
    "    display(\"df_book_blocks.columns\", df_book_blocks.columns)\n",
    "    # tfidf_X = run_tfidf_vectorizer(paragraph_book['paragraph'])\n",
    "    # display(\"tfidf_X is a {} datatype.\".format(type(tfidf_X)))\n",
    "    # display(tfidf_X)\n",
    "    X = df_book_blocks[['book_block', 'block_text']]\n",
    "    if debug:  display(\"shape of X is\", X.shape)\n",
    "    y = df_book_blocks['author']\n",
    "    if debug:  display(\"shape of y is\", y.shape)\n",
    "    # y.sample(20)\n",
    "    # X_train, X_test = train_test_split(tfidf_X)\n",
    "    display(\"Here come the book blocks check, dude!!!\")\n",
    "    display(df_book_blocks.isnull().sum())\n",
    "    \n",
    "    print_timestamp('\\n'*3+'Starting w2vec_vectorize_text...')\n",
    "    l3 = w2vec_vectorize_text(sentences=df_book_blocks['block_text'], size=num_features, min_count=1,sg=1, seed=57, workers=4)\n",
    "    print_timestamp('\\n'*3+'Done with w2vec_vectorize_text...')\n",
    "\n",
    "    X1 = np.array(l3)\n",
    "    print(\"the dimensions of X are {}\".format(X1.shape))\n",
    "    \n",
    "    df_book_blocks = pd.merge(df_book_blocks, pd.DataFrame(X1), left_index=True, right_index=True)\n",
    "\n",
    "    column_names_new = {}\n",
    "    for i in range(0,num_features):\n",
    "        column_names_new[i]='w2v_{}'.format(i)\n",
    "    df_book_blocks.rename(columns=column_names_new, inplace=True)\n",
    "    \n",
    "    print(\"columns for df_book_blocks are:\")\n",
    "    print(df_book_blocks.columns)\n",
    "    \n",
    "    print_timestamp('\\n'*3+'Starting tfidf_vectorize_text...')\n",
    "    \n",
    "    parms1 = {}\n",
    "    parms1[\"max_features\"] = num_features\n",
    "    l4 = run_tfidf_vectorizer2(df_book_blocks['block_text'], parameters=parms1)\n",
    "    \n",
    "    df4 = pd.DataFrame(l4)\n",
    "    \n",
    "#     print(\"l4 is a {} datatype.\".format(type(l4)))\n",
    "\n",
    "    column_names_new = {}\n",
    "    for i in range(0,num_features):\n",
    "        column_names_new[i]='tfidf_{}'.format(i)\n",
    "    df4.rename(columns=column_names_new, inplace=True)\n",
    "    print(\"columns for df4 are:\")\n",
    "    print(df4.columns)\n",
    "    \n",
    "#     print(\"here is df4.head(20)\")\n",
    "#     display(df4.head(20))\n",
    "    \n",
    "    print_timestamp('\\n'*3+'Done with tfidf_vectorize_text...')\n",
    "#     X2 = np.array(l4)\n",
    "#     print(\"the dimensions of X are {}\".format(X2.shape))\n",
    "    \n",
    "    df_book_blocks = pd.merge(df_book_blocks, df4, left_index=True, right_index=True)\n",
    "    display(df_book_blocks.loc[15:30, 'tfidf_22' : 'tfidf_27']) # a subset of rows and columns...\n",
    "\n",
    "    if debug == True:\n",
    "        print(\"columns for df_book_blocks are:\")\n",
    "        print(df_book_blocks.columns)\n",
    "        \n",
    "    print(\"#### HERE ARE THE dtypes of df_book_blocks...\")\n",
    "    display(df_book_blocks.dtypes)\n",
    "    print(\"here are the nulls and nan's in df_book_blocks.iloc[:,5:5004]\".format(num_features+4))\n",
    "    display(df_book_blocks.iloc[:, 5 : num_features + 4].isnull().sum())\n",
    "    print(\"Now we are getting the cluster from tfidf...\")\n",
    "\n",
    "    df_w2v = df_book_blocks.iloc[:, 5 : num_features + 4]\n",
    "    cluster_w2v = run_kmeans(df_w2v, parameters={\"n_clusters\": 10})\n",
    "    \n",
    "    print(\"Now we are getting the cluster from w2v...\")\n",
    "    cluster_tfidf, return_K = run_kmeans(df_book_blocks.iloc[:, num_features + 5 : num_features*2 + 4], parameters={\"n_clusters\": 10})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "2019-09-24 21:29:07: In: main_nlp_new \n",
       "\n",
       "\n",
       "Begin "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'## book_block:'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>book</th>\n",
       "      <th>book_block</th>\n",
       "      <th>block_text</th>\n",
       "      <th>block_text_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>ThirtyNineSteps.txt</td>\n",
       "      <td>1</td>\n",
       "      <td>dressed  dined at the cafa  royal  and turned...</td>\n",
       "      <td>2099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>ThirtyNineSteps.txt</td>\n",
       "      <td>2</td>\n",
       "      <td>i want to know if i can count you in  a  a  ge...</td>\n",
       "      <td>2245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>ThirtyNineSteps.txt</td>\n",
       "      <td>3</td>\n",
       "      <td>have got left behind a little  a  yes and no  ...</td>\n",
       "      <td>2069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>ThirtyNineSteps.txt</td>\n",
       "      <td>4</td>\n",
       "      <td>but ita  s not going to come off if therea  s...</td>\n",
       "      <td>2136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>ThirtyNineSteps.txt</td>\n",
       "      <td>5</td>\n",
       "      <td>tomorrow to swear to having heard a shot  but ...</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>ThirtyNineSteps.txt</td>\n",
       "      <td>6</td>\n",
       "      <td>next morning to hear my man  paddock  making t...</td>\n",
       "      <td>2172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>ThirtyNineSteps.txt</td>\n",
       "      <td>7</td>\n",
       "      <td>he had taken on a fairly stiff job  it was not...</td>\n",
       "      <td>2182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>ThirtyNineSteps.txt</td>\n",
       "      <td>8</td>\n",
       "      <td>coming back to me  and i could think again  it...</td>\n",
       "      <td>2199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>ThirtyNineSteps.txt</td>\n",
       "      <td>9</td>\n",
       "      <td>queer how the prospect comforted me  i had bee...</td>\n",
       "      <td>2286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>ThirtyNineSteps.txt</td>\n",
       "      <td>10</td>\n",
       "      <td>hunted out a well  used tweed suit  a pair of ...</td>\n",
       "      <td>2178</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   author                 book  book_block                                         block_text  block_text_length\n",
       "0       1  ThirtyNineSteps.txt           1   dressed  dined at the cafa  royal  and turned...               2099\n",
       "1       1  ThirtyNineSteps.txt           2  i want to know if i can count you in  a  a  ge...               2245\n",
       "2       1  ThirtyNineSteps.txt           3  have got left behind a little  a  yes and no  ...               2069\n",
       "3       1  ThirtyNineSteps.txt           4   but ita  s not going to come off if therea  s...               2136\n",
       "4       1  ThirtyNineSteps.txt           5  tomorrow to swear to having heard a shot  but ...               2017\n",
       "5       1  ThirtyNineSteps.txt           6  next morning to hear my man  paddock  making t...               2172\n",
       "6       1  ThirtyNineSteps.txt           7  he had taken on a fairly stiff job  it was not...               2182\n",
       "7       1  ThirtyNineSteps.txt           8  coming back to me  and i could think again  it...               2199\n",
       "8       1  ThirtyNineSteps.txt           9  queer how the prospect comforted me  i had bee...               2286\n",
       "9       1  ThirtyNineSteps.txt          10  hunted out a well  used tweed suit  a pair of ...               2178"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'null schtuff\\n:'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "author               0\n",
       "book                 0\n",
       "book_block           0\n",
       "block_text           0\n",
       "block_text_length    0\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'df_book_blocks.columns'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Index(['author', 'book', 'book_block', 'block_text', 'block_text_length'], dtype='object')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'shape of X is'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(2309, 2)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'shape of y is'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(2309,)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Here come the book blocks check, dude!!!'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "author               0\n",
       "book                 0\n",
       "book_block           0\n",
       "block_text           0\n",
       "block_text_length    0\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "2019-09-24 21:29:12: In: main_nlp_new \n",
       "\n",
       "\n",
       "Starting w2vec_vectorize_text... "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2309 [00:00<?, ?it/s]/Users/lou/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:13: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  del sys.path[0]\n",
      "100%|| 2309/2309 [00:00<00:00, 114512.29it/s]\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "2019-09-24 21:29:22: In: main_nlp_new \n",
       "\n",
       "\n",
       "Done with w2vec_vectorize_text... "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the dimensions of X are (2309, 50)\n",
      "columns for df_book_blocks are:\n",
      "Index(['author', 'book', 'book_block', 'block_text', 'block_text_length', 'w2v_0', 'w2v_1', 'w2v_2', 'w2v_3', 'w2v_4', 'w2v_5', 'w2v_6', 'w2v_7', 'w2v_8', 'w2v_9', 'w2v_10', 'w2v_11', 'w2v_12', 'w2v_13', 'w2v_14', 'w2v_15', 'w2v_16', 'w2v_17', 'w2v_18', 'w2v_19', 'w2v_20', 'w2v_21', 'w2v_22', 'w2v_23', 'w2v_24', 'w2v_25', 'w2v_26', 'w2v_27', 'w2v_28', 'w2v_29', 'w2v_30', 'w2v_31', 'w2v_32', 'w2v_33', 'w2v_34', 'w2v_35', 'w2v_36', 'w2v_37', 'w2v_38', 'w2v_39', 'w2v_40', 'w2v_41', 'w2v_42', 'w2v_43', 'w2v_44', 'w2v_45', 'w2v_46', 'w2v_47', 'w2v_48', 'w2v_49'], dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "2019-09-24 21:29:22: In: main_nlp_new \n",
       "\n",
       "\n",
       "Starting tfidf_vectorize_text... "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "2019-09-24 21:29:22: In: run_tfidf_vectorizer2 in run_tfidf_vectorizer: df is a <class 'pandas.core.series.Series'> datatype. "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parms = {'max_features': 50}\n",
      "tfidf_df is a <class 'scipy.sparse.csr.csr_matrix'> datatype\n",
      "tfidf_df_dense is a <class 'numpy.ndarray'> datatype\n",
      "columns for df4 are:\n",
      "Index(['tfidf_0', 'tfidf_1', 'tfidf_2', 'tfidf_3', 'tfidf_4', 'tfidf_5', 'tfidf_6', 'tfidf_7', 'tfidf_8', 'tfidf_9', 'tfidf_10', 'tfidf_11', 'tfidf_12', 'tfidf_13', 'tfidf_14', 'tfidf_15', 'tfidf_16', 'tfidf_17', 'tfidf_18', 'tfidf_19', 'tfidf_20', 'tfidf_21', 'tfidf_22', 'tfidf_23', 'tfidf_24', 'tfidf_25', 'tfidf_26', 'tfidf_27', 'tfidf_28', 'tfidf_29', 'tfidf_30', 'tfidf_31', 'tfidf_32', 'tfidf_33', 'tfidf_34', 'tfidf_35', 'tfidf_36', 'tfidf_37', 'tfidf_38', 'tfidf_39', 'tfidf_40', 'tfidf_41', 'tfidf_42', 'tfidf_43', 'tfidf_44', 'tfidf_45', 'tfidf_46', 'tfidf_47', 'tfidf_48', 'tfidf_49'], dtype='object')\n",
      "here is df4.head(20)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tfidf_0</th>\n",
       "      <th>tfidf_1</th>\n",
       "      <th>tfidf_2</th>\n",
       "      <th>tfidf_3</th>\n",
       "      <th>tfidf_4</th>\n",
       "      <th>tfidf_5</th>\n",
       "      <th>tfidf_6</th>\n",
       "      <th>tfidf_7</th>\n",
       "      <th>tfidf_8</th>\n",
       "      <th>tfidf_9</th>\n",
       "      <th>tfidf_10</th>\n",
       "      <th>tfidf_11</th>\n",
       "      <th>tfidf_12</th>\n",
       "      <th>tfidf_13</th>\n",
       "      <th>tfidf_14</th>\n",
       "      <th>tfidf_15</th>\n",
       "      <th>tfidf_16</th>\n",
       "      <th>tfidf_17</th>\n",
       "      <th>tfidf_18</th>\n",
       "      <th>tfidf_19</th>\n",
       "      <th>tfidf_20</th>\n",
       "      <th>tfidf_21</th>\n",
       "      <th>tfidf_22</th>\n",
       "      <th>tfidf_23</th>\n",
       "      <th>tfidf_24</th>\n",
       "      <th>tfidf_25</th>\n",
       "      <th>tfidf_26</th>\n",
       "      <th>tfidf_27</th>\n",
       "      <th>tfidf_28</th>\n",
       "      <th>tfidf_29</th>\n",
       "      <th>tfidf_30</th>\n",
       "      <th>tfidf_31</th>\n",
       "      <th>tfidf_32</th>\n",
       "      <th>tfidf_33</th>\n",
       "      <th>tfidf_34</th>\n",
       "      <th>tfidf_35</th>\n",
       "      <th>tfidf_36</th>\n",
       "      <th>tfidf_37</th>\n",
       "      <th>tfidf_38</th>\n",
       "      <th>tfidf_39</th>\n",
       "      <th>tfidf_40</th>\n",
       "      <th>tfidf_41</th>\n",
       "      <th>tfidf_42</th>\n",
       "      <th>tfidf_43</th>\n",
       "      <th>tfidf_44</th>\n",
       "      <th>tfidf_45</th>\n",
       "      <th>tfidf_46</th>\n",
       "      <th>tfidf_47</th>\n",
       "      <th>tfidf_48</th>\n",
       "      <th>tfidf_49</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.058</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.436</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.149</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.119</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.155</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.269</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.116</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.138</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.103</td>\n",
       "      <td>0.136</td>\n",
       "      <td>0.266</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.092</td>\n",
       "      <td>0.105</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.573</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.062</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.183</td>\n",
       "      <td>0.289</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.067</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.119</td>\n",
       "      <td>0.101</td>\n",
       "      <td>0.096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.415</td>\n",
       "      <td>0.096</td>\n",
       "      <td>0.071</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.110</td>\n",
       "      <td>0.091</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.172</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.419</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.175</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.203</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.097</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.060</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.218</td>\n",
       "      <td>0.150</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.199</td>\n",
       "      <td>0.480</td>\n",
       "      <td>0.106</td>\n",
       "      <td>0.092</td>\n",
       "      <td>0.059</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.240</td>\n",
       "      <td>0.115</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.121</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.096</td>\n",
       "      <td>0.091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.064</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.354</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.092</td>\n",
       "      <td>0.164</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.211</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.086</td>\n",
       "      <td>0.129</td>\n",
       "      <td>0.269</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.191</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.152</td>\n",
       "      <td>0.127</td>\n",
       "      <td>0.169</td>\n",
       "      <td>0.104</td>\n",
       "      <td>0.114</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.073</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.202</td>\n",
       "      <td>0.058</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.282</td>\n",
       "      <td>0.404</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.217</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.329</td>\n",
       "      <td>0.106</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.097</td>\n",
       "      <td>0.306</td>\n",
       "      <td>0.078</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.098</td>\n",
       "      <td>0.117</td>\n",
       "      <td>0.053</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.126</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.119</td>\n",
       "      <td>0.103</td>\n",
       "      <td>0.071</td>\n",
       "      <td>0.449</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.073</td>\n",
       "      <td>0.177</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.445</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.212</td>\n",
       "      <td>0.108</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.119</td>\n",
       "      <td>0.424</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.066</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.092</td>\n",
       "      <td>0.236</td>\n",
       "      <td>0.198</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.106</td>\n",
       "      <td>0.283</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.167</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.188</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.232</td>\n",
       "      <td>0.065</td>\n",
       "      <td>0.164</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.116</td>\n",
       "      <td>0.129</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.133</td>\n",
       "      <td>0.077</td>\n",
       "      <td>0.191</td>\n",
       "      <td>0.373</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.147</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.071</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.073</td>\n",
       "      <td>0.078</td>\n",
       "      <td>0.386</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.109</td>\n",
       "      <td>0.104</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.360</td>\n",
       "      <td>0.081</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.134</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.174</td>\n",
       "      <td>0.457</td>\n",
       "      <td>0.168</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.062</td>\n",
       "      <td>0.082</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.229</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.243</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.192</td>\n",
       "      <td>0.139</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.052</td>\n",
       "      <td>0.076</td>\n",
       "      <td>0.151</td>\n",
       "      <td>0.111</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.254</td>\n",
       "      <td>0.087</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.077</td>\n",
       "      <td>0.482</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.071</td>\n",
       "      <td>0.068</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.355</td>\n",
       "      <td>0.133</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.106</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.061</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.217</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.059</td>\n",
       "      <td>0.052</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0.060</td>\n",
       "      <td>0.163</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.411</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.183</td>\n",
       "      <td>0.238</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.242</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.109</td>\n",
       "      <td>0.072</td>\n",
       "      <td>0.105</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.289</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.067</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.220</td>\n",
       "      <td>0.434</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.338</td>\n",
       "      <td>0.178</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.067</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.126</td>\n",
       "      <td>0.142</td>\n",
       "      <td>0.067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.085</td>\n",
       "      <td>0.309</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.067</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.111</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.107</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.255</td>\n",
       "      <td>0.052</td>\n",
       "      <td>0.154</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.052</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.062</td>\n",
       "      <td>0.206</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.106</td>\n",
       "      <td>0.062</td>\n",
       "      <td>0.306</td>\n",
       "      <td>0.149</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.069</td>\n",
       "      <td>0.247</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.123</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.167</td>\n",
       "      <td>0.473</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.088</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.247</td>\n",
       "      <td>0.303</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.085</td>\n",
       "      <td>0.124</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.060</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.182</td>\n",
       "      <td>0.029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.111</td>\n",
       "      <td>0.243</td>\n",
       "      <td>0.067</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.173</td>\n",
       "      <td>0.088</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.190</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.343</td>\n",
       "      <td>0.052</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.067</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.244</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.105</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.295</td>\n",
       "      <td>0.140</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.203</td>\n",
       "      <td>0.093</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.164</td>\n",
       "      <td>0.527</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.082</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.264</td>\n",
       "      <td>0.256</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.149</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.089</td>\n",
       "      <td>0.064</td>\n",
       "      <td>0.373</td>\n",
       "      <td>0.052</td>\n",
       "      <td>0.101</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.059</td>\n",
       "      <td>0.122</td>\n",
       "      <td>0.202</td>\n",
       "      <td>0.158</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.124</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.164</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.104</td>\n",
       "      <td>0.406</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.233</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.094</td>\n",
       "      <td>0.583</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.099</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.061</td>\n",
       "      <td>0.187</td>\n",
       "      <td>0.196</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.064</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.097</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.376</td>\n",
       "      <td>0.162</td>\n",
       "      <td>0.136</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.141</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.178</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.053</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.105</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.217</td>\n",
       "      <td>0.394</td>\n",
       "      <td>0.115</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.105</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.064</td>\n",
       "      <td>0.606</td>\n",
       "      <td>0.068</td>\n",
       "      <td>0.059</td>\n",
       "      <td>0.112</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.251</td>\n",
       "      <td>0.198</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.086</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.409</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.148</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.074</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.071</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.128</td>\n",
       "      <td>0.087</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.099</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.159</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.068</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.409</td>\n",
       "      <td>0.078</td>\n",
       "      <td>0.093</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.096</td>\n",
       "      <td>0.069</td>\n",
       "      <td>0.590</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.064</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.065</td>\n",
       "      <td>0.059</td>\n",
       "      <td>0.159</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.094</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.062</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.189</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.052</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.349</td>\n",
       "      <td>0.091</td>\n",
       "      <td>0.067</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.104</td>\n",
       "      <td>0.171</td>\n",
       "      <td>0.102</td>\n",
       "      <td>0.139</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.206</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.085</td>\n",
       "      <td>0.062</td>\n",
       "      <td>0.122</td>\n",
       "      <td>0.209</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.308</td>\n",
       "      <td>0.071</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.077</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.062</td>\n",
       "      <td>0.658</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.117</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.226</td>\n",
       "      <td>0.195</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.113</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.061</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.299</td>\n",
       "      <td>0.150</td>\n",
       "      <td>0.088</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.088</td>\n",
       "      <td>0.069</td>\n",
       "      <td>0.085</td>\n",
       "      <td>0.101</td>\n",
       "      <td>0.276</td>\n",
       "      <td>0.104</td>\n",
       "      <td>0.203</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.069</td>\n",
       "      <td>0.089</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.136</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.084</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.162</td>\n",
       "      <td>0.158</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.082</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.112</td>\n",
       "      <td>0.081</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.193</td>\n",
       "      <td>0.598</td>\n",
       "      <td>0.088</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.109</td>\n",
       "      <td>0.117</td>\n",
       "      <td>0.071</td>\n",
       "      <td>0.136</td>\n",
       "      <td>0.286</td>\n",
       "      <td>0.087</td>\n",
       "      <td>0.188</td>\n",
       "      <td>0.082</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.119</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.283</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.069</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.094</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.167</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.103</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.173</td>\n",
       "      <td>0.072</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.112</td>\n",
       "      <td>0.316</td>\n",
       "      <td>0.114</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.782</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.067</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.117</td>\n",
       "      <td>0.158</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.092</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.052</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.145</td>\n",
       "      <td>0.395</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.086</td>\n",
       "      <td>0.064</td>\n",
       "      <td>0.085</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.165</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.059</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.196</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.211</td>\n",
       "      <td>0.141</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.079</td>\n",
       "      <td>0.078</td>\n",
       "      <td>0.153</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.089</td>\n",
       "      <td>0.158</td>\n",
       "      <td>0.060</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.079</td>\n",
       "      <td>0.218</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.107</td>\n",
       "      <td>0.605</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.071</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.263</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.079</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.086</td>\n",
       "      <td>0.291</td>\n",
       "      <td>0.092</td>\n",
       "      <td>0.068</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.195</td>\n",
       "      <td>0.052</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.222</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.079</td>\n",
       "      <td>0.114</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.167</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.247</td>\n",
       "      <td>0.181</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.312</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.059</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.645</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.112</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.104</td>\n",
       "      <td>0.263</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.058</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.087</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.451</td>\n",
       "      <td>0.111</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.081</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.104</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.170</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.107</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.151</td>\n",
       "      <td>0.126</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.103</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.186</td>\n",
       "      <td>0.109</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.226</td>\n",
       "      <td>0.115</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.069</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.476</td>\n",
       "      <td>0.162</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.179</td>\n",
       "      <td>0.130</td>\n",
       "      <td>0.301</td>\n",
       "      <td>0.211</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.089</td>\n",
       "      <td>0.078</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.133</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.315</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.068</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.109</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.213</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.134</td>\n",
       "      <td>0.074</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.138</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.147</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.130</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.218</td>\n",
       "      <td>0.061</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.315</td>\n",
       "      <td>0.120</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.119</td>\n",
       "      <td>0.149</td>\n",
       "      <td>0.608</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.059</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.126</td>\n",
       "      <td>0.287</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.058</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.115</td>\n",
       "      <td>0.092</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.066</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.393</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.071</td>\n",
       "      <td>0.213</td>\n",
       "      <td>0.118</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.091</td>\n",
       "      <td>0.081</td>\n",
       "      <td>0.099</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.307</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.097</td>\n",
       "      <td>0.159</td>\n",
       "      <td>0.060</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.240</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.060</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.199</td>\n",
       "      <td>0.481</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.062</td>\n",
       "      <td>0.059</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.114</td>\n",
       "      <td>0.284</td>\n",
       "      <td>0.322</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.060</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.064</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.068</td>\n",
       "      <td>0.193</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    tfidf_0  tfidf_1  tfidf_2  tfidf_3  tfidf_4  tfidf_5  tfidf_6  tfidf_7  tfidf_8  tfidf_9  tfidf_10  tfidf_11  tfidf_12  tfidf_13  tfidf_14  tfidf_15  tfidf_16  tfidf_17  tfidf_18  tfidf_19  tfidf_20  tfidf_21  tfidf_22  tfidf_23  tfidf_24  tfidf_25  tfidf_26  tfidf_27  tfidf_28  tfidf_29  tfidf_30  tfidf_31  tfidf_32  tfidf_33  tfidf_34  tfidf_35  tfidf_36  tfidf_37  tfidf_38  tfidf_39  tfidf_40  tfidf_41  tfidf_42  tfidf_43  tfidf_44  tfidf_45  tfidf_46  tfidf_47  tfidf_48  tfidf_49\n",
       "0     0.058    0.031    0.436    0.051    0.149    0.000    0.050    0.029    0.119    0.028     0.155     0.000     0.269     0.000     0.116     0.075     0.034     0.138     0.031     0.024     0.103     0.136     0.266     0.063     0.051     0.092     0.105     0.000     0.034     0.063     0.000     0.032     0.046     0.573     0.037     0.032     0.062     0.000     0.030     0.183     0.289     0.000     0.000     0.000     0.000     0.067     0.000     0.119     0.101     0.096\n",
       "1     0.028    0.030    0.415    0.096    0.071    0.027    0.047    0.110    0.091    0.054     0.172     0.028     0.419     0.000     0.083     0.000     0.033     0.175     0.029     0.203     0.033     0.097     0.000     0.060     0.000     0.218     0.150     0.030     0.033     0.090     0.000     0.000     0.199     0.480     0.106     0.092     0.059     0.031     0.000     0.240     0.115     0.000     0.121     0.033     0.000     0.063     0.000     0.045     0.096     0.091\n",
       "2     0.064    0.035    0.354    0.028    0.055    0.092    0.164    0.032    0.211    0.000     0.086     0.129     0.269     0.000     0.191     0.055     0.152     0.127     0.169     0.104     0.114     0.000     0.073     0.035     0.028     0.202     0.058     0.035     0.000     0.070     0.000     0.000     0.282     0.404     0.041     0.000     0.000     0.217     0.033     0.329     0.106     0.000     0.070     0.000     0.000     0.000     0.045     0.000     0.037     0.282\n",
       "3     0.030    0.097    0.306    0.078    0.000    0.029    0.051    0.030    0.098    0.117     0.053     0.000     0.126     0.000     0.119     0.103     0.071     0.449     0.032     0.073     0.177     0.070     0.445     0.000     0.026     0.212     0.108     0.000     0.035     0.000     0.000     0.000     0.119     0.424     0.000     0.066     0.032     0.000     0.092     0.236     0.198     0.000     0.000     0.035     0.000     0.034     0.000     0.049     0.000     0.066\n",
       "4     0.033    0.106    0.283    0.057    0.028    0.063    0.167    0.032    0.188    0.032     0.232     0.065     0.164     0.000     0.000     0.028     0.116     0.129     0.000     0.133     0.077     0.191     0.373     0.035     0.029     0.180     0.147     0.035     0.038     0.071     0.000     0.073     0.078     0.386     0.000     0.109     0.104     0.000     0.100     0.360     0.081     0.000     0.000     0.000     0.035     0.037     0.000     0.134     0.038     0.323\n",
       "5     0.000    0.174    0.457    0.168    0.083    0.062    0.082    0.032    0.026    0.063     0.229     0.000     0.243     0.000     0.192     0.139     0.038     0.051     0.000     0.052     0.076     0.151     0.111     0.070     0.028     0.254     0.087     0.000     0.038     0.035     0.000     0.000     0.077     0.482     0.000     0.071     0.068     0.036     0.033     0.355     0.133     0.000     0.035     0.000     0.000     0.037     0.000     0.106     0.075     0.000\n",
       "6     0.061    0.033    0.217    0.080    0.026    0.059    0.052    0.000    0.075    0.060     0.163     0.000     0.411     0.000     0.183     0.238     0.000     0.242     0.032     0.025     0.109     0.072     0.105     0.033     0.135     0.289     0.083     0.033     0.000     0.067     0.045     0.034     0.220     0.434     0.000     0.000     0.032     0.000     0.031     0.338     0.178     0.000     0.067     0.036     0.033     0.035     0.000     0.126     0.142     0.067\n",
       "7     0.026    0.085    0.309    0.045    0.067    0.125    0.111    0.026    0.107    0.000     0.255     0.052     0.154     0.000     0.052     0.135     0.062     0.206     0.000     0.106     0.062     0.306     0.149     0.028     0.069     0.247     0.024     0.028     0.123     0.000     0.000     0.029     0.167     0.473     0.033     0.029     0.000     0.088     0.027     0.247     0.303     0.000     0.085     0.124     0.028     0.060     0.000     0.021     0.182     0.029\n",
       "8     0.000    0.111    0.243    0.067    0.044    0.173    0.088    0.026    0.190    0.050     0.343     0.052     0.022     0.000     0.051     0.067     0.030     0.244     0.000     0.105     0.031     0.090     0.295     0.140     0.046     0.203     0.093     0.000     0.000     0.000     0.000     0.029     0.164     0.527     0.033     0.029     0.082     0.029     0.000     0.264     0.256     0.000     0.028     0.031     0.028     0.029     0.000     0.042     0.149     0.000\n",
       "9     0.089    0.064    0.373    0.052    0.101    0.028    0.050    0.059    0.122    0.202     0.158     0.000     0.124     0.000     0.029     0.026     0.000     0.164     0.000     0.048     0.070     0.104     0.406     0.000     0.026     0.233     0.080     0.032     0.000     0.032     0.000     0.000     0.094     0.583     0.000     0.099     0.063     0.000     0.061     0.187     0.196     0.000     0.032     0.000     0.064     0.000     0.000     0.097     0.000     0.065\n",
       "10    0.000    0.029    0.376    0.162    0.136    0.000    0.023    0.026    0.022    0.026     0.141     0.000     0.178     0.037     0.053     0.023     0.031     0.105     0.000     0.043     0.000     0.217     0.394     0.115     0.047     0.105     0.048     0.029     0.000     0.029     0.000     0.030     0.064     0.606     0.068     0.059     0.112     0.000     0.054     0.251     0.198     0.033     0.029     0.000     0.086     0.000     0.000     0.022     0.000     0.000\n",
       "11    0.029    0.125    0.409    0.100    0.148    0.000    0.074    0.000    0.071    0.028     0.128     0.087     0.024     0.000     0.000     0.099     0.034     0.159     0.030     0.070     0.068     0.135     0.132     0.031     0.000     0.409     0.078     0.093     0.034     0.000     0.000     0.096     0.069     0.590     0.000     0.064     0.030     0.065     0.059     0.159     0.143     0.036     0.094     0.000     0.062     0.033     0.000     0.189     0.033     0.095\n",
       "12    0.052    0.000    0.349    0.091    0.067    0.025    0.022    0.104    0.171    0.102     0.139     0.000     0.000     0.036     0.026     0.022     0.000     0.206     0.000     0.085     0.062     0.122     0.209     0.028     0.023     0.308     0.071     0.028     0.031     0.028     0.077     0.000     0.062     0.658     0.000     0.000     0.000     0.117     0.000     0.226     0.195     0.000     0.057     0.000     0.113     0.090     0.000     0.021     0.061     0.000\n",
       "13    0.034    0.000    0.299    0.150    0.088    0.033    0.088    0.069    0.085    0.101     0.276     0.104     0.203     0.000     0.069     0.089     0.000     0.136     0.000     0.084     0.000     0.162     0.158     0.037     0.031     0.082     0.031     0.112     0.081     0.038     0.000     0.000     0.193     0.598     0.088     0.000     0.109     0.117     0.071     0.136     0.286     0.087     0.188     0.082     0.037     0.119     0.000     0.028     0.000     0.000\n",
       "14    0.021    0.023    0.283    0.055    0.000    0.000    0.054    0.000    0.069    0.041     0.094     0.042     0.000     0.000     0.000     0.000     0.025     0.167     0.000     0.103     0.000     0.173     0.072     0.023     0.112     0.316     0.114     0.000     0.050     0.000     0.000     0.000     0.135     0.782     0.000     0.070     0.067     0.048     0.000     0.117     0.158     0.000     0.000     0.000     0.092     0.024     0.000     0.052     0.025     0.023\n",
       "15    0.000    0.145    0.395    0.000    0.086    0.064    0.085    0.000    0.165    0.033     0.059     0.033     0.196     0.000     0.000     0.029     0.000     0.211     0.141     0.054     0.079     0.078     0.153     0.036     0.089     0.158     0.060     0.000     0.079     0.218     0.000     0.000     0.107     0.605     0.000     0.037     0.071     0.000     0.034     0.263     0.028     0.000     0.000     0.079     0.036     0.038     0.047     0.055     0.039     0.257\n",
       "16    0.026    0.086    0.291    0.092    0.068    0.000    0.090    0.026    0.195    0.052     0.070     0.026     0.222     0.000     0.079     0.114     0.031     0.167     0.028     0.043     0.031     0.247     0.181     0.029     0.000     0.312     0.000     0.000     0.000     0.029     0.039     0.059     0.042     0.645     0.000     0.029     0.112     0.000     0.000     0.104     0.263     0.000     0.058     0.031     0.057     0.000     0.000     0.087     0.000     0.116\n",
       "17    0.032    0.000    0.451    0.111    0.027    0.031    0.081    0.000    0.104    0.031     0.170     0.000     0.107     0.000     0.095     0.055     0.151     0.126     0.033     0.103     0.000     0.186     0.109     0.000     0.056     0.226     0.115     0.137     0.000     0.069     0.000     0.035     0.051     0.476     0.162     0.000     0.000     0.179     0.130     0.301     0.211     0.040     0.035     0.075     0.000     0.000     0.089     0.078     0.000     0.245\n",
       "18    0.133    0.029    0.315    0.023    0.045    0.026    0.068    0.000    0.109    0.026     0.213     0.027     0.134     0.074     0.026     0.138     0.031     0.147     0.000     0.130     0.095     0.218     0.061     0.000     0.024     0.315     0.120     0.000     0.000     0.000     0.039     0.119     0.149     0.608     0.034     0.059     0.056     0.000     0.000     0.126     0.287     0.000     0.058     0.032     0.115     0.092     0.000     0.066     0.031     0.117\n",
       "19    0.083    0.000    0.393    0.024    0.071    0.213    0.118    0.028    0.091    0.081     0.099     0.028     0.023     0.000     0.000     0.000     0.033     0.307     0.000     0.090     0.000     0.097     0.159     0.060     0.025     0.240     0.100     0.060     0.033     0.000     0.041     0.000     0.199     0.481     0.035     0.062     0.059     0.031     0.114     0.284     0.322     0.035     0.060     0.033     0.030     0.064     0.000     0.068     0.193     0.000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "2019-09-24 21:29:23: In: main_nlp_new \n",
       "\n",
       "\n",
       "Done with tfidf_vectorize_text... "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tfidf_22</th>\n",
       "      <th>tfidf_23</th>\n",
       "      <th>tfidf_24</th>\n",
       "      <th>tfidf_25</th>\n",
       "      <th>tfidf_26</th>\n",
       "      <th>tfidf_27</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.153</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.089</td>\n",
       "      <td>0.158</td>\n",
       "      <td>0.060</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.181</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.312</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.109</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.226</td>\n",
       "      <td>0.115</td>\n",
       "      <td>0.137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.061</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.315</td>\n",
       "      <td>0.120</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.159</td>\n",
       "      <td>0.060</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.240</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.175</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.241</td>\n",
       "      <td>0.092</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.459</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.343</td>\n",
       "      <td>0.091</td>\n",
       "      <td>0.036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.262</td>\n",
       "      <td>0.107</td>\n",
       "      <td>0.058</td>\n",
       "      <td>0.181</td>\n",
       "      <td>0.118</td>\n",
       "      <td>0.035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.220</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.243</td>\n",
       "      <td>0.104</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.291</td>\n",
       "      <td>0.158</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.172</td>\n",
       "      <td>0.066</td>\n",
       "      <td>0.078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.150</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.289</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.061</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.293</td>\n",
       "      <td>0.096</td>\n",
       "      <td>0.057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.326</td>\n",
       "      <td>0.155</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.084</td>\n",
       "      <td>0.129</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.327</td>\n",
       "      <td>0.093</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.225</td>\n",
       "      <td>0.206</td>\n",
       "      <td>0.031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.263</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.151</td>\n",
       "      <td>0.243</td>\n",
       "      <td>0.166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.320</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.147</td>\n",
       "      <td>0.140</td>\n",
       "      <td>0.034</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    tfidf_22  tfidf_23  tfidf_24  tfidf_25  tfidf_26  tfidf_27\n",
       "15     0.153     0.036     0.089     0.158     0.060     0.000\n",
       "16     0.181     0.029     0.000     0.312     0.000     0.000\n",
       "17     0.109     0.000     0.056     0.226     0.115     0.137\n",
       "18     0.061     0.000     0.024     0.315     0.120     0.000\n",
       "19     0.159     0.060     0.025     0.240     0.100     0.060\n",
       "20     0.175     0.055     0.000     0.241     0.092     0.000\n",
       "21     0.459     0.036     0.030     0.343     0.091     0.036\n",
       "22     0.262     0.107     0.058     0.181     0.118     0.035\n",
       "23     0.220     0.083     0.000     0.243     0.104     0.000\n",
       "24     0.291     0.158     0.032     0.172     0.066     0.078\n",
       "25     0.150     0.028     0.000     0.289     0.095     0.000\n",
       "26     0.061     0.000     0.047     0.293     0.096     0.057\n",
       "27     0.326     0.155     0.000     0.084     0.129     0.000\n",
       "28     0.327     0.093     0.025     0.225     0.206     0.031\n",
       "29     0.263     0.000     0.034     0.151     0.243     0.166\n",
       "30     0.320     0.034     0.000     0.147     0.140     0.034"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### HERE ARE THE dtypes of df_book_blocks...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "author                 int64\n",
       "book                  object\n",
       "book_block             int64\n",
       "block_text            object\n",
       "block_text_length      int64\n",
       "w2v_0                float32\n",
       "w2v_1                float32\n",
       "w2v_2                float32\n",
       "w2v_3                float32\n",
       "w2v_4                float32\n",
       "w2v_5                float32\n",
       "w2v_6                float32\n",
       "w2v_7                float32\n",
       "w2v_8                float32\n",
       "w2v_9                float32\n",
       "w2v_10               float32\n",
       "w2v_11               float32\n",
       "w2v_12               float32\n",
       "w2v_13               float32\n",
       "w2v_14               float32\n",
       "w2v_15               float32\n",
       "w2v_16               float32\n",
       "w2v_17               float32\n",
       "w2v_18               float32\n",
       "w2v_19               float32\n",
       "w2v_20               float32\n",
       "w2v_21               float32\n",
       "w2v_22               float32\n",
       "w2v_23               float32\n",
       "w2v_24               float32\n",
       "w2v_25               float32\n",
       "w2v_26               float32\n",
       "w2v_27               float32\n",
       "w2v_28               float32\n",
       "w2v_29               float32\n",
       "w2v_30               float32\n",
       "w2v_31               float32\n",
       "w2v_32               float32\n",
       "w2v_33               float32\n",
       "w2v_34               float32\n",
       "w2v_35               float32\n",
       "w2v_36               float32\n",
       "w2v_37               float32\n",
       "w2v_38               float32\n",
       "w2v_39               float32\n",
       "w2v_40               float32\n",
       "w2v_41               float32\n",
       "w2v_42               float32\n",
       "w2v_43               float32\n",
       "w2v_44               float32\n",
       "w2v_45               float32\n",
       "w2v_46               float32\n",
       "w2v_47               float32\n",
       "w2v_48               float32\n",
       "w2v_49               float32\n",
       "tfidf_0              float64\n",
       "tfidf_1              float64\n",
       "tfidf_2              float64\n",
       "tfidf_3              float64\n",
       "tfidf_4              float64\n",
       "tfidf_5              float64\n",
       "tfidf_6              float64\n",
       "tfidf_7              float64\n",
       "tfidf_8              float64\n",
       "tfidf_9              float64\n",
       "tfidf_10             float64\n",
       "tfidf_11             float64\n",
       "tfidf_12             float64\n",
       "tfidf_13             float64\n",
       "tfidf_14             float64\n",
       "tfidf_15             float64\n",
       "tfidf_16             float64\n",
       "tfidf_17             float64\n",
       "tfidf_18             float64\n",
       "tfidf_19             float64\n",
       "tfidf_20             float64\n",
       "tfidf_21             float64\n",
       "tfidf_22             float64\n",
       "tfidf_23             float64\n",
       "tfidf_24             float64\n",
       "tfidf_25             float64\n",
       "tfidf_26             float64\n",
       "tfidf_27             float64\n",
       "tfidf_28             float64\n",
       "tfidf_29             float64\n",
       "tfidf_30             float64\n",
       "tfidf_31             float64\n",
       "tfidf_32             float64\n",
       "tfidf_33             float64\n",
       "tfidf_34             float64\n",
       "tfidf_35             float64\n",
       "tfidf_36             float64\n",
       "tfidf_37             float64\n",
       "tfidf_38             float64\n",
       "tfidf_39             float64\n",
       "tfidf_40             float64\n",
       "tfidf_41             float64\n",
       "tfidf_42             float64\n",
       "tfidf_43             float64\n",
       "tfidf_44             float64\n",
       "tfidf_45             float64\n",
       "tfidf_46             float64\n",
       "tfidf_47             float64\n",
       "tfidf_48             float64\n",
       "tfidf_49             float64\n",
       "dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here are the nulls and nan's in df_book_blocks.iloc[:,5:5004]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "w2v_0     0\n",
       "w2v_1     0\n",
       "w2v_2     0\n",
       "w2v_3     0\n",
       "w2v_4     0\n",
       "w2v_5     0\n",
       "w2v_6     0\n",
       "w2v_7     0\n",
       "w2v_8     0\n",
       "w2v_9     0\n",
       "w2v_10    0\n",
       "w2v_11    0\n",
       "w2v_12    0\n",
       "w2v_13    0\n",
       "w2v_14    0\n",
       "w2v_15    0\n",
       "w2v_16    0\n",
       "w2v_17    0\n",
       "w2v_18    0\n",
       "w2v_19    0\n",
       "w2v_20    0\n",
       "w2v_21    0\n",
       "w2v_22    0\n",
       "w2v_23    0\n",
       "w2v_24    0\n",
       "w2v_25    0\n",
       "w2v_26    0\n",
       "w2v_27    0\n",
       "w2v_28    0\n",
       "w2v_29    0\n",
       "w2v_30    0\n",
       "w2v_31    0\n",
       "w2v_32    0\n",
       "w2v_33    0\n",
       "w2v_34    0\n",
       "w2v_35    0\n",
       "w2v_36    0\n",
       "w2v_37    0\n",
       "w2v_38    0\n",
       "w2v_39    0\n",
       "w2v_40    0\n",
       "w2v_41    0\n",
       "w2v_42    0\n",
       "w2v_43    0\n",
       "w2v_44    0\n",
       "w2v_45    0\n",
       "w2v_46    0\n",
       "w2v_47    0\n",
       "w2v_48    0\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now we are getting the cluster from tfidf...\n",
      "inside run_kmeans, and the data parameter is a <class 'pandas.core.frame.DataFrame'> datatype.\n",
      "in run_kmeans, the size of dataframe is (1983, 49)\n",
      "here comes the null check for data inside run_kmeans\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "w2v_0     0\n",
       "w2v_1     0\n",
       "w2v_2     0\n",
       "w2v_3     0\n",
       "w2v_4     0\n",
       "w2v_5     0\n",
       "w2v_6     0\n",
       "w2v_7     0\n",
       "w2v_8     0\n",
       "w2v_9     0\n",
       "w2v_10    0\n",
       "w2v_11    0\n",
       "w2v_12    0\n",
       "w2v_13    0\n",
       "w2v_14    0\n",
       "w2v_15    0\n",
       "w2v_16    0\n",
       "w2v_17    0\n",
       "w2v_18    0\n",
       "w2v_19    0\n",
       "w2v_20    0\n",
       "w2v_21    0\n",
       "w2v_22    0\n",
       "w2v_23    0\n",
       "w2v_24    0\n",
       "w2v_25    0\n",
       "w2v_26    0\n",
       "w2v_27    0\n",
       "w2v_28    0\n",
       "w2v_29    0\n",
       "w2v_30    0\n",
       "w2v_31    0\n",
       "w2v_32    0\n",
       "w2v_33    0\n",
       "w2v_34    0\n",
       "w2v_35    0\n",
       "w2v_36    0\n",
       "w2v_37    0\n",
       "w2v_38    0\n",
       "w2v_39    0\n",
       "w2v_40    0\n",
       "w2v_41    0\n",
       "w2v_42    0\n",
       "w2v_43    0\n",
       "w2v_44    0\n",
       "w2v_45    0\n",
       "w2v_46    0\n",
       "w2v_47    0\n",
       "w2v_48    0\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here comes data.head(50)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>w2v_0</th>\n",
       "      <th>w2v_1</th>\n",
       "      <th>w2v_2</th>\n",
       "      <th>w2v_3</th>\n",
       "      <th>w2v_4</th>\n",
       "      <th>w2v_5</th>\n",
       "      <th>w2v_6</th>\n",
       "      <th>w2v_7</th>\n",
       "      <th>w2v_8</th>\n",
       "      <th>w2v_9</th>\n",
       "      <th>w2v_10</th>\n",
       "      <th>w2v_11</th>\n",
       "      <th>w2v_12</th>\n",
       "      <th>w2v_13</th>\n",
       "      <th>w2v_14</th>\n",
       "      <th>w2v_15</th>\n",
       "      <th>w2v_16</th>\n",
       "      <th>w2v_17</th>\n",
       "      <th>w2v_18</th>\n",
       "      <th>w2v_19</th>\n",
       "      <th>w2v_20</th>\n",
       "      <th>w2v_21</th>\n",
       "      <th>w2v_22</th>\n",
       "      <th>w2v_23</th>\n",
       "      <th>w2v_24</th>\n",
       "      <th>w2v_25</th>\n",
       "      <th>w2v_26</th>\n",
       "      <th>w2v_27</th>\n",
       "      <th>w2v_28</th>\n",
       "      <th>w2v_29</th>\n",
       "      <th>w2v_30</th>\n",
       "      <th>w2v_31</th>\n",
       "      <th>w2v_32</th>\n",
       "      <th>w2v_33</th>\n",
       "      <th>w2v_34</th>\n",
       "      <th>w2v_35</th>\n",
       "      <th>w2v_36</th>\n",
       "      <th>w2v_37</th>\n",
       "      <th>w2v_38</th>\n",
       "      <th>w2v_39</th>\n",
       "      <th>w2v_40</th>\n",
       "      <th>w2v_41</th>\n",
       "      <th>w2v_42</th>\n",
       "      <th>w2v_43</th>\n",
       "      <th>w2v_44</th>\n",
       "      <th>w2v_45</th>\n",
       "      <th>w2v_46</th>\n",
       "      <th>w2v_47</th>\n",
       "      <th>w2v_48</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>-0.157</td>\n",
       "      <td>-0.057</td>\n",
       "      <td>-0.118</td>\n",
       "      <td>0.153</td>\n",
       "      <td>-0.099</td>\n",
       "      <td>0.079</td>\n",
       "      <td>0.054</td>\n",
       "      <td>-0.016</td>\n",
       "      <td>0.073</td>\n",
       "      <td>0.048</td>\n",
       "      <td>-0.059</td>\n",
       "      <td>0.079</td>\n",
       "      <td>0.258</td>\n",
       "      <td>0.160</td>\n",
       "      <td>-0.130</td>\n",
       "      <td>0.094</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.098</td>\n",
       "      <td>0.193</td>\n",
       "      <td>0.023</td>\n",
       "      <td>-0.145</td>\n",
       "      <td>0.133</td>\n",
       "      <td>-0.051</td>\n",
       "      <td>-0.142</td>\n",
       "      <td>-0.134</td>\n",
       "      <td>-0.265</td>\n",
       "      <td>-0.149</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>-0.062</td>\n",
       "      <td>0.073</td>\n",
       "      <td>-0.101</td>\n",
       "      <td>0.148</td>\n",
       "      <td>0.062</td>\n",
       "      <td>-0.120</td>\n",
       "      <td>0.338</td>\n",
       "      <td>-0.077</td>\n",
       "      <td>0.092</td>\n",
       "      <td>-0.061</td>\n",
       "      <td>-0.166</td>\n",
       "      <td>-0.065</td>\n",
       "      <td>0.044</td>\n",
       "      <td>-0.061</td>\n",
       "      <td>-0.168</td>\n",
       "      <td>0.089</td>\n",
       "      <td>0.040</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>0.180</td>\n",
       "      <td>-0.129</td>\n",
       "      <td>-0.185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.138</td>\n",
       "      <td>0.105</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>0.014</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>-0.072</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.070</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>0.094</td>\n",
       "      <td>-0.028</td>\n",
       "      <td>0.242</td>\n",
       "      <td>0.284</td>\n",
       "      <td>-0.144</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.182</td>\n",
       "      <td>0.107</td>\n",
       "      <td>0.215</td>\n",
       "      <td>0.014</td>\n",
       "      <td>-0.082</td>\n",
       "      <td>0.077</td>\n",
       "      <td>0.010</td>\n",
       "      <td>-0.169</td>\n",
       "      <td>-0.093</td>\n",
       "      <td>-0.360</td>\n",
       "      <td>-0.074</td>\n",
       "      <td>0.094</td>\n",
       "      <td>-0.156</td>\n",
       "      <td>0.110</td>\n",
       "      <td>-0.138</td>\n",
       "      <td>0.124</td>\n",
       "      <td>0.101</td>\n",
       "      <td>-0.094</td>\n",
       "      <td>0.252</td>\n",
       "      <td>-0.120</td>\n",
       "      <td>0.127</td>\n",
       "      <td>-0.092</td>\n",
       "      <td>-0.186</td>\n",
       "      <td>-0.014</td>\n",
       "      <td>0.004</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>-0.111</td>\n",
       "      <td>0.093</td>\n",
       "      <td>0.026</td>\n",
       "      <td>-0.164</td>\n",
       "      <td>0.242</td>\n",
       "      <td>-0.158</td>\n",
       "      <td>-0.277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>-0.119</td>\n",
       "      <td>0.082</td>\n",
       "      <td>-0.057</td>\n",
       "      <td>-0.097</td>\n",
       "      <td>0.053</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.055</td>\n",
       "      <td>-0.049</td>\n",
       "      <td>0.065</td>\n",
       "      <td>0.303</td>\n",
       "      <td>0.214</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>0.060</td>\n",
       "      <td>0.172</td>\n",
       "      <td>0.106</td>\n",
       "      <td>0.183</td>\n",
       "      <td>0.076</td>\n",
       "      <td>-0.121</td>\n",
       "      <td>0.134</td>\n",
       "      <td>-0.116</td>\n",
       "      <td>-0.087</td>\n",
       "      <td>-0.041</td>\n",
       "      <td>-0.191</td>\n",
       "      <td>-0.234</td>\n",
       "      <td>-0.117</td>\n",
       "      <td>-0.217</td>\n",
       "      <td>0.107</td>\n",
       "      <td>-0.099</td>\n",
       "      <td>0.188</td>\n",
       "      <td>0.136</td>\n",
       "      <td>-0.108</td>\n",
       "      <td>0.339</td>\n",
       "      <td>0.065</td>\n",
       "      <td>0.046</td>\n",
       "      <td>-0.203</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.046</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>0.038</td>\n",
       "      <td>-0.086</td>\n",
       "      <td>0.124</td>\n",
       "      <td>0.029</td>\n",
       "      <td>-0.209</td>\n",
       "      <td>0.186</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.016</td>\n",
       "      <td>-0.032</td>\n",
       "      <td>-0.110</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.093</td>\n",
       "      <td>0.097</td>\n",
       "      <td>0.171</td>\n",
       "      <td>0.059</td>\n",
       "      <td>0.090</td>\n",
       "      <td>-0.220</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>0.129</td>\n",
       "      <td>0.317</td>\n",
       "      <td>0.211</td>\n",
       "      <td>-0.014</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.144</td>\n",
       "      <td>0.353</td>\n",
       "      <td>0.032</td>\n",
       "      <td>-0.110</td>\n",
       "      <td>0.121</td>\n",
       "      <td>-0.161</td>\n",
       "      <td>-0.087</td>\n",
       "      <td>-0.102</td>\n",
       "      <td>-0.341</td>\n",
       "      <td>-0.077</td>\n",
       "      <td>-0.056</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.011</td>\n",
       "      <td>-0.074</td>\n",
       "      <td>0.202</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>-0.108</td>\n",
       "      <td>0.243</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.080</td>\n",
       "      <td>-0.047</td>\n",
       "      <td>-0.088</td>\n",
       "      <td>-0.038</td>\n",
       "      <td>0.082</td>\n",
       "      <td>-0.074</td>\n",
       "      <td>-0.140</td>\n",
       "      <td>0.092</td>\n",
       "      <td>0.192</td>\n",
       "      <td>-0.097</td>\n",
       "      <td>0.182</td>\n",
       "      <td>-0.108</td>\n",
       "      <td>-0.080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.008</td>\n",
       "      <td>-0.119</td>\n",
       "      <td>-0.050</td>\n",
       "      <td>-0.066</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.105</td>\n",
       "      <td>0.137</td>\n",
       "      <td>-0.089</td>\n",
       "      <td>0.192</td>\n",
       "      <td>0.267</td>\n",
       "      <td>0.111</td>\n",
       "      <td>-0.243</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.117</td>\n",
       "      <td>0.072</td>\n",
       "      <td>0.252</td>\n",
       "      <td>0.019</td>\n",
       "      <td>-0.160</td>\n",
       "      <td>0.142</td>\n",
       "      <td>-0.048</td>\n",
       "      <td>-0.060</td>\n",
       "      <td>-0.056</td>\n",
       "      <td>-0.233</td>\n",
       "      <td>-0.162</td>\n",
       "      <td>-0.013</td>\n",
       "      <td>-0.059</td>\n",
       "      <td>0.009</td>\n",
       "      <td>-0.127</td>\n",
       "      <td>0.175</td>\n",
       "      <td>0.017</td>\n",
       "      <td>-0.116</td>\n",
       "      <td>0.319</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>0.092</td>\n",
       "      <td>0.022</td>\n",
       "      <td>-0.150</td>\n",
       "      <td>-0.110</td>\n",
       "      <td>0.050</td>\n",
       "      <td>-0.022</td>\n",
       "      <td>-0.168</td>\n",
       "      <td>0.095</td>\n",
       "      <td>-0.016</td>\n",
       "      <td>-0.127</td>\n",
       "      <td>0.260</td>\n",
       "      <td>-0.153</td>\n",
       "      <td>-0.090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.045</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>-0.142</td>\n",
       "      <td>-0.197</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.106</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.053</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>0.133</td>\n",
       "      <td>0.301</td>\n",
       "      <td>0.081</td>\n",
       "      <td>-0.201</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.162</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.232</td>\n",
       "      <td>0.012</td>\n",
       "      <td>-0.172</td>\n",
       "      <td>0.210</td>\n",
       "      <td>-0.024</td>\n",
       "      <td>-0.109</td>\n",
       "      <td>0.080</td>\n",
       "      <td>-0.216</td>\n",
       "      <td>-0.054</td>\n",
       "      <td>-0.044</td>\n",
       "      <td>-0.162</td>\n",
       "      <td>0.061</td>\n",
       "      <td>-0.120</td>\n",
       "      <td>0.212</td>\n",
       "      <td>0.061</td>\n",
       "      <td>-0.224</td>\n",
       "      <td>0.274</td>\n",
       "      <td>0.155</td>\n",
       "      <td>0.008</td>\n",
       "      <td>-0.171</td>\n",
       "      <td>-0.042</td>\n",
       "      <td>-0.028</td>\n",
       "      <td>0.060</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>-0.037</td>\n",
       "      <td>0.138</td>\n",
       "      <td>0.117</td>\n",
       "      <td>-0.202</td>\n",
       "      <td>0.167</td>\n",
       "      <td>-0.258</td>\n",
       "      <td>-0.132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>-0.119</td>\n",
       "      <td>0.082</td>\n",
       "      <td>-0.057</td>\n",
       "      <td>-0.097</td>\n",
       "      <td>0.053</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.055</td>\n",
       "      <td>-0.049</td>\n",
       "      <td>0.065</td>\n",
       "      <td>0.303</td>\n",
       "      <td>0.214</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>0.060</td>\n",
       "      <td>0.172</td>\n",
       "      <td>0.106</td>\n",
       "      <td>0.183</td>\n",
       "      <td>0.076</td>\n",
       "      <td>-0.121</td>\n",
       "      <td>0.134</td>\n",
       "      <td>-0.116</td>\n",
       "      <td>-0.087</td>\n",
       "      <td>-0.041</td>\n",
       "      <td>-0.191</td>\n",
       "      <td>-0.234</td>\n",
       "      <td>-0.117</td>\n",
       "      <td>-0.217</td>\n",
       "      <td>0.107</td>\n",
       "      <td>-0.099</td>\n",
       "      <td>0.188</td>\n",
       "      <td>0.136</td>\n",
       "      <td>-0.108</td>\n",
       "      <td>0.339</td>\n",
       "      <td>0.065</td>\n",
       "      <td>0.046</td>\n",
       "      <td>-0.203</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.046</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>0.038</td>\n",
       "      <td>-0.086</td>\n",
       "      <td>0.124</td>\n",
       "      <td>0.029</td>\n",
       "      <td>-0.209</td>\n",
       "      <td>0.186</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>-0.022</td>\n",
       "      <td>-0.032</td>\n",
       "      <td>-0.102</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>-0.053</td>\n",
       "      <td>0.113</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.216</td>\n",
       "      <td>-0.077</td>\n",
       "      <td>-0.066</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.343</td>\n",
       "      <td>0.073</td>\n",
       "      <td>-0.220</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.130</td>\n",
       "      <td>0.103</td>\n",
       "      <td>0.258</td>\n",
       "      <td>0.024</td>\n",
       "      <td>-0.157</td>\n",
       "      <td>0.215</td>\n",
       "      <td>0.026</td>\n",
       "      <td>-0.072</td>\n",
       "      <td>-0.044</td>\n",
       "      <td>-0.275</td>\n",
       "      <td>-0.057</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>-0.135</td>\n",
       "      <td>0.053</td>\n",
       "      <td>-0.170</td>\n",
       "      <td>0.101</td>\n",
       "      <td>0.034</td>\n",
       "      <td>-0.126</td>\n",
       "      <td>0.376</td>\n",
       "      <td>-0.022</td>\n",
       "      <td>0.069</td>\n",
       "      <td>-0.051</td>\n",
       "      <td>-0.187</td>\n",
       "      <td>-0.031</td>\n",
       "      <td>0.003</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>-0.112</td>\n",
       "      <td>0.096</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>-0.110</td>\n",
       "      <td>0.093</td>\n",
       "      <td>-0.197</td>\n",
       "      <td>-0.076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>-0.274</td>\n",
       "      <td>-0.463</td>\n",
       "      <td>-0.166</td>\n",
       "      <td>-0.291</td>\n",
       "      <td>0.267</td>\n",
       "      <td>0.133</td>\n",
       "      <td>0.061</td>\n",
       "      <td>-0.195</td>\n",
       "      <td>0.216</td>\n",
       "      <td>-0.279</td>\n",
       "      <td>-0.148</td>\n",
       "      <td>0.232</td>\n",
       "      <td>0.483</td>\n",
       "      <td>0.245</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.128</td>\n",
       "      <td>-0.073</td>\n",
       "      <td>0.354</td>\n",
       "      <td>0.393</td>\n",
       "      <td>0.308</td>\n",
       "      <td>-0.358</td>\n",
       "      <td>0.077</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.041</td>\n",
       "      <td>-0.233</td>\n",
       "      <td>-0.392</td>\n",
       "      <td>-0.039</td>\n",
       "      <td>-0.084</td>\n",
       "      <td>-0.013</td>\n",
       "      <td>0.196</td>\n",
       "      <td>-0.261</td>\n",
       "      <td>0.172</td>\n",
       "      <td>0.185</td>\n",
       "      <td>0.208</td>\n",
       "      <td>0.216</td>\n",
       "      <td>-0.250</td>\n",
       "      <td>-0.025</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>0.076</td>\n",
       "      <td>-0.028</td>\n",
       "      <td>0.139</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>-0.163</td>\n",
       "      <td>-0.031</td>\n",
       "      <td>0.325</td>\n",
       "      <td>0.167</td>\n",
       "      <td>-0.015</td>\n",
       "      <td>-0.092</td>\n",
       "      <td>0.033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>-0.119</td>\n",
       "      <td>0.082</td>\n",
       "      <td>-0.057</td>\n",
       "      <td>-0.097</td>\n",
       "      <td>0.053</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.055</td>\n",
       "      <td>-0.049</td>\n",
       "      <td>0.065</td>\n",
       "      <td>0.303</td>\n",
       "      <td>0.214</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>0.060</td>\n",
       "      <td>0.172</td>\n",
       "      <td>0.106</td>\n",
       "      <td>0.183</td>\n",
       "      <td>0.076</td>\n",
       "      <td>-0.121</td>\n",
       "      <td>0.134</td>\n",
       "      <td>-0.116</td>\n",
       "      <td>-0.087</td>\n",
       "      <td>-0.041</td>\n",
       "      <td>-0.191</td>\n",
       "      <td>-0.234</td>\n",
       "      <td>-0.117</td>\n",
       "      <td>-0.217</td>\n",
       "      <td>0.107</td>\n",
       "      <td>-0.099</td>\n",
       "      <td>0.188</td>\n",
       "      <td>0.136</td>\n",
       "      <td>-0.108</td>\n",
       "      <td>0.339</td>\n",
       "      <td>0.065</td>\n",
       "      <td>0.046</td>\n",
       "      <td>-0.203</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.046</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>0.038</td>\n",
       "      <td>-0.086</td>\n",
       "      <td>0.124</td>\n",
       "      <td>0.029</td>\n",
       "      <td>-0.209</td>\n",
       "      <td>0.186</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.016</td>\n",
       "      <td>-0.032</td>\n",
       "      <td>-0.110</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.093</td>\n",
       "      <td>0.097</td>\n",
       "      <td>0.171</td>\n",
       "      <td>0.059</td>\n",
       "      <td>0.090</td>\n",
       "      <td>-0.220</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>0.129</td>\n",
       "      <td>0.317</td>\n",
       "      <td>0.211</td>\n",
       "      <td>-0.014</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.144</td>\n",
       "      <td>0.353</td>\n",
       "      <td>0.032</td>\n",
       "      <td>-0.110</td>\n",
       "      <td>0.121</td>\n",
       "      <td>-0.161</td>\n",
       "      <td>-0.087</td>\n",
       "      <td>-0.102</td>\n",
       "      <td>-0.341</td>\n",
       "      <td>-0.077</td>\n",
       "      <td>-0.056</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.011</td>\n",
       "      <td>-0.074</td>\n",
       "      <td>0.202</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>-0.108</td>\n",
       "      <td>0.243</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.080</td>\n",
       "      <td>-0.047</td>\n",
       "      <td>-0.088</td>\n",
       "      <td>-0.038</td>\n",
       "      <td>0.082</td>\n",
       "      <td>-0.074</td>\n",
       "      <td>-0.140</td>\n",
       "      <td>0.092</td>\n",
       "      <td>0.192</td>\n",
       "      <td>-0.097</td>\n",
       "      <td>0.182</td>\n",
       "      <td>-0.108</td>\n",
       "      <td>-0.080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>-0.243</td>\n",
       "      <td>-0.159</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.052</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.102</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.055</td>\n",
       "      <td>-0.140</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>0.112</td>\n",
       "      <td>0.149</td>\n",
       "      <td>0.033</td>\n",
       "      <td>-0.082</td>\n",
       "      <td>-0.057</td>\n",
       "      <td>0.194</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.196</td>\n",
       "      <td>-0.016</td>\n",
       "      <td>-0.051</td>\n",
       "      <td>0.081</td>\n",
       "      <td>-0.074</td>\n",
       "      <td>-0.102</td>\n",
       "      <td>-0.130</td>\n",
       "      <td>-0.329</td>\n",
       "      <td>-0.056</td>\n",
       "      <td>-0.026</td>\n",
       "      <td>-0.115</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>-0.105</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.069</td>\n",
       "      <td>-0.147</td>\n",
       "      <td>0.336</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.121</td>\n",
       "      <td>-0.062</td>\n",
       "      <td>-0.230</td>\n",
       "      <td>-0.066</td>\n",
       "      <td>0.049</td>\n",
       "      <td>-0.014</td>\n",
       "      <td>-0.133</td>\n",
       "      <td>0.154</td>\n",
       "      <td>-0.073</td>\n",
       "      <td>-0.171</td>\n",
       "      <td>0.287</td>\n",
       "      <td>0.007</td>\n",
       "      <td>-0.158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.138</td>\n",
       "      <td>0.105</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>0.014</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>-0.072</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.070</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>0.094</td>\n",
       "      <td>-0.028</td>\n",
       "      <td>0.242</td>\n",
       "      <td>0.284</td>\n",
       "      <td>-0.144</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.182</td>\n",
       "      <td>0.107</td>\n",
       "      <td>0.215</td>\n",
       "      <td>0.014</td>\n",
       "      <td>-0.082</td>\n",
       "      <td>0.077</td>\n",
       "      <td>0.010</td>\n",
       "      <td>-0.169</td>\n",
       "      <td>-0.093</td>\n",
       "      <td>-0.360</td>\n",
       "      <td>-0.074</td>\n",
       "      <td>0.094</td>\n",
       "      <td>-0.156</td>\n",
       "      <td>0.110</td>\n",
       "      <td>-0.138</td>\n",
       "      <td>0.124</td>\n",
       "      <td>0.101</td>\n",
       "      <td>-0.094</td>\n",
       "      <td>0.252</td>\n",
       "      <td>-0.120</td>\n",
       "      <td>0.127</td>\n",
       "      <td>-0.092</td>\n",
       "      <td>-0.186</td>\n",
       "      <td>-0.014</td>\n",
       "      <td>0.004</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>-0.111</td>\n",
       "      <td>0.093</td>\n",
       "      <td>0.026</td>\n",
       "      <td>-0.164</td>\n",
       "      <td>0.242</td>\n",
       "      <td>-0.158</td>\n",
       "      <td>-0.277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.138</td>\n",
       "      <td>0.105</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>0.014</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>-0.072</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.070</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>0.094</td>\n",
       "      <td>-0.028</td>\n",
       "      <td>0.242</td>\n",
       "      <td>0.284</td>\n",
       "      <td>-0.144</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.182</td>\n",
       "      <td>0.107</td>\n",
       "      <td>0.215</td>\n",
       "      <td>0.014</td>\n",
       "      <td>-0.082</td>\n",
       "      <td>0.077</td>\n",
       "      <td>0.010</td>\n",
       "      <td>-0.169</td>\n",
       "      <td>-0.093</td>\n",
       "      <td>-0.360</td>\n",
       "      <td>-0.074</td>\n",
       "      <td>0.094</td>\n",
       "      <td>-0.156</td>\n",
       "      <td>0.110</td>\n",
       "      <td>-0.138</td>\n",
       "      <td>0.124</td>\n",
       "      <td>0.101</td>\n",
       "      <td>-0.094</td>\n",
       "      <td>0.252</td>\n",
       "      <td>-0.120</td>\n",
       "      <td>0.127</td>\n",
       "      <td>-0.092</td>\n",
       "      <td>-0.186</td>\n",
       "      <td>-0.014</td>\n",
       "      <td>0.004</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>-0.111</td>\n",
       "      <td>0.093</td>\n",
       "      <td>0.026</td>\n",
       "      <td>-0.164</td>\n",
       "      <td>0.242</td>\n",
       "      <td>-0.158</td>\n",
       "      <td>-0.277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>-0.081</td>\n",
       "      <td>-0.126</td>\n",
       "      <td>-0.097</td>\n",
       "      <td>-0.084</td>\n",
       "      <td>-0.074</td>\n",
       "      <td>0.058</td>\n",
       "      <td>0.016</td>\n",
       "      <td>-0.014</td>\n",
       "      <td>0.041</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>-0.055</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.239</td>\n",
       "      <td>0.139</td>\n",
       "      <td>-0.201</td>\n",
       "      <td>0.111</td>\n",
       "      <td>0.149</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.204</td>\n",
       "      <td>0.031</td>\n",
       "      <td>-0.226</td>\n",
       "      <td>0.166</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>-0.089</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>-0.279</td>\n",
       "      <td>-0.147</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>-0.145</td>\n",
       "      <td>0.030</td>\n",
       "      <td>-0.134</td>\n",
       "      <td>0.093</td>\n",
       "      <td>0.096</td>\n",
       "      <td>-0.143</td>\n",
       "      <td>0.342</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>0.115</td>\n",
       "      <td>0.003</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>-0.066</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.007</td>\n",
       "      <td>-0.130</td>\n",
       "      <td>0.090</td>\n",
       "      <td>-0.042</td>\n",
       "      <td>-0.134</td>\n",
       "      <td>0.195</td>\n",
       "      <td>-0.180</td>\n",
       "      <td>-0.040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>-0.032</td>\n",
       "      <td>-0.155</td>\n",
       "      <td>-0.175</td>\n",
       "      <td>0.044</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>-0.050</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.095</td>\n",
       "      <td>-0.176</td>\n",
       "      <td>0.088</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.206</td>\n",
       "      <td>-0.026</td>\n",
       "      <td>-0.102</td>\n",
       "      <td>0.116</td>\n",
       "      <td>0.064</td>\n",
       "      <td>0.053</td>\n",
       "      <td>0.190</td>\n",
       "      <td>0.033</td>\n",
       "      <td>-0.084</td>\n",
       "      <td>0.108</td>\n",
       "      <td>-0.108</td>\n",
       "      <td>-0.081</td>\n",
       "      <td>-0.077</td>\n",
       "      <td>-0.204</td>\n",
       "      <td>-0.130</td>\n",
       "      <td>-0.054</td>\n",
       "      <td>-0.093</td>\n",
       "      <td>0.150</td>\n",
       "      <td>-0.063</td>\n",
       "      <td>0.177</td>\n",
       "      <td>0.063</td>\n",
       "      <td>-0.128</td>\n",
       "      <td>0.442</td>\n",
       "      <td>-0.093</td>\n",
       "      <td>0.032</td>\n",
       "      <td>-0.105</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.099</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.010</td>\n",
       "      <td>-0.200</td>\n",
       "      <td>0.131</td>\n",
       "      <td>0.175</td>\n",
       "      <td>-0.253</td>\n",
       "      <td>0.267</td>\n",
       "      <td>-0.210</td>\n",
       "      <td>-0.148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.016</td>\n",
       "      <td>-0.032</td>\n",
       "      <td>-0.110</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.093</td>\n",
       "      <td>0.097</td>\n",
       "      <td>0.171</td>\n",
       "      <td>0.059</td>\n",
       "      <td>0.090</td>\n",
       "      <td>-0.220</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>0.129</td>\n",
       "      <td>0.317</td>\n",
       "      <td>0.211</td>\n",
       "      <td>-0.014</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.144</td>\n",
       "      <td>0.353</td>\n",
       "      <td>0.032</td>\n",
       "      <td>-0.110</td>\n",
       "      <td>0.121</td>\n",
       "      <td>-0.161</td>\n",
       "      <td>-0.087</td>\n",
       "      <td>-0.102</td>\n",
       "      <td>-0.341</td>\n",
       "      <td>-0.077</td>\n",
       "      <td>-0.056</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.011</td>\n",
       "      <td>-0.074</td>\n",
       "      <td>0.202</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>-0.108</td>\n",
       "      <td>0.243</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.080</td>\n",
       "      <td>-0.047</td>\n",
       "      <td>-0.088</td>\n",
       "      <td>-0.038</td>\n",
       "      <td>0.082</td>\n",
       "      <td>-0.074</td>\n",
       "      <td>-0.140</td>\n",
       "      <td>0.092</td>\n",
       "      <td>0.192</td>\n",
       "      <td>-0.097</td>\n",
       "      <td>0.182</td>\n",
       "      <td>-0.108</td>\n",
       "      <td>-0.080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>-0.119</td>\n",
       "      <td>0.082</td>\n",
       "      <td>-0.057</td>\n",
       "      <td>-0.097</td>\n",
       "      <td>0.053</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.055</td>\n",
       "      <td>-0.049</td>\n",
       "      <td>0.065</td>\n",
       "      <td>0.303</td>\n",
       "      <td>0.214</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>0.060</td>\n",
       "      <td>0.172</td>\n",
       "      <td>0.106</td>\n",
       "      <td>0.183</td>\n",
       "      <td>0.076</td>\n",
       "      <td>-0.121</td>\n",
       "      <td>0.134</td>\n",
       "      <td>-0.116</td>\n",
       "      <td>-0.087</td>\n",
       "      <td>-0.041</td>\n",
       "      <td>-0.191</td>\n",
       "      <td>-0.234</td>\n",
       "      <td>-0.117</td>\n",
       "      <td>-0.217</td>\n",
       "      <td>0.107</td>\n",
       "      <td>-0.099</td>\n",
       "      <td>0.188</td>\n",
       "      <td>0.136</td>\n",
       "      <td>-0.108</td>\n",
       "      <td>0.339</td>\n",
       "      <td>0.065</td>\n",
       "      <td>0.046</td>\n",
       "      <td>-0.203</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.046</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>0.038</td>\n",
       "      <td>-0.086</td>\n",
       "      <td>0.124</td>\n",
       "      <td>0.029</td>\n",
       "      <td>-0.209</td>\n",
       "      <td>0.186</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.096</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.011</td>\n",
       "      <td>-0.050</td>\n",
       "      <td>-0.057</td>\n",
       "      <td>0.071</td>\n",
       "      <td>0.111</td>\n",
       "      <td>0.076</td>\n",
       "      <td>-0.108</td>\n",
       "      <td>-0.013</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.148</td>\n",
       "      <td>0.272</td>\n",
       "      <td>-0.142</td>\n",
       "      <td>0.166</td>\n",
       "      <td>0.226</td>\n",
       "      <td>0.120</td>\n",
       "      <td>0.333</td>\n",
       "      <td>0.098</td>\n",
       "      <td>-0.182</td>\n",
       "      <td>0.151</td>\n",
       "      <td>-0.111</td>\n",
       "      <td>-0.059</td>\n",
       "      <td>-0.062</td>\n",
       "      <td>-0.242</td>\n",
       "      <td>-0.141</td>\n",
       "      <td>-0.036</td>\n",
       "      <td>-0.204</td>\n",
       "      <td>0.058</td>\n",
       "      <td>-0.119</td>\n",
       "      <td>0.196</td>\n",
       "      <td>0.110</td>\n",
       "      <td>-0.062</td>\n",
       "      <td>0.374</td>\n",
       "      <td>-0.187</td>\n",
       "      <td>0.038</td>\n",
       "      <td>-0.025</td>\n",
       "      <td>-0.056</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.002</td>\n",
       "      <td>-0.082</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.070</td>\n",
       "      <td>-0.097</td>\n",
       "      <td>0.191</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.008</td>\n",
       "      <td>-0.119</td>\n",
       "      <td>-0.050</td>\n",
       "      <td>-0.066</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.105</td>\n",
       "      <td>0.137</td>\n",
       "      <td>-0.089</td>\n",
       "      <td>0.192</td>\n",
       "      <td>0.267</td>\n",
       "      <td>0.111</td>\n",
       "      <td>-0.243</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.117</td>\n",
       "      <td>0.072</td>\n",
       "      <td>0.252</td>\n",
       "      <td>0.019</td>\n",
       "      <td>-0.160</td>\n",
       "      <td>0.142</td>\n",
       "      <td>-0.048</td>\n",
       "      <td>-0.060</td>\n",
       "      <td>-0.056</td>\n",
       "      <td>-0.233</td>\n",
       "      <td>-0.162</td>\n",
       "      <td>-0.013</td>\n",
       "      <td>-0.059</td>\n",
       "      <td>0.009</td>\n",
       "      <td>-0.127</td>\n",
       "      <td>0.175</td>\n",
       "      <td>0.017</td>\n",
       "      <td>-0.116</td>\n",
       "      <td>0.319</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>0.092</td>\n",
       "      <td>0.022</td>\n",
       "      <td>-0.150</td>\n",
       "      <td>-0.110</td>\n",
       "      <td>0.050</td>\n",
       "      <td>-0.022</td>\n",
       "      <td>-0.168</td>\n",
       "      <td>0.095</td>\n",
       "      <td>-0.016</td>\n",
       "      <td>-0.127</td>\n",
       "      <td>0.260</td>\n",
       "      <td>-0.153</td>\n",
       "      <td>-0.090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.036</td>\n",
       "      <td>-0.025</td>\n",
       "      <td>-0.022</td>\n",
       "      <td>-0.047</td>\n",
       "      <td>-0.057</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>0.159</td>\n",
       "      <td>0.092</td>\n",
       "      <td>-0.193</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.084</td>\n",
       "      <td>0.261</td>\n",
       "      <td>0.211</td>\n",
       "      <td>-0.212</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.197</td>\n",
       "      <td>0.059</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.030</td>\n",
       "      <td>-0.137</td>\n",
       "      <td>0.111</td>\n",
       "      <td>-0.045</td>\n",
       "      <td>-0.132</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.273</td>\n",
       "      <td>-0.057</td>\n",
       "      <td>0.069</td>\n",
       "      <td>-0.162</td>\n",
       "      <td>0.110</td>\n",
       "      <td>-0.122</td>\n",
       "      <td>0.160</td>\n",
       "      <td>0.126</td>\n",
       "      <td>-0.183</td>\n",
       "      <td>0.228</td>\n",
       "      <td>-0.105</td>\n",
       "      <td>0.157</td>\n",
       "      <td>-0.037</td>\n",
       "      <td>-0.052</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>0.046</td>\n",
       "      <td>-0.048</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.043</td>\n",
       "      <td>-0.245</td>\n",
       "      <td>0.287</td>\n",
       "      <td>-0.074</td>\n",
       "      <td>-0.148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>-0.243</td>\n",
       "      <td>-0.159</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.052</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.102</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.055</td>\n",
       "      <td>-0.140</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>0.112</td>\n",
       "      <td>0.149</td>\n",
       "      <td>0.033</td>\n",
       "      <td>-0.082</td>\n",
       "      <td>-0.057</td>\n",
       "      <td>0.194</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.196</td>\n",
       "      <td>-0.016</td>\n",
       "      <td>-0.051</td>\n",
       "      <td>0.081</td>\n",
       "      <td>-0.074</td>\n",
       "      <td>-0.102</td>\n",
       "      <td>-0.130</td>\n",
       "      <td>-0.329</td>\n",
       "      <td>-0.056</td>\n",
       "      <td>-0.026</td>\n",
       "      <td>-0.115</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>-0.105</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.069</td>\n",
       "      <td>-0.147</td>\n",
       "      <td>0.336</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.121</td>\n",
       "      <td>-0.062</td>\n",
       "      <td>-0.230</td>\n",
       "      <td>-0.066</td>\n",
       "      <td>0.049</td>\n",
       "      <td>-0.014</td>\n",
       "      <td>-0.133</td>\n",
       "      <td>0.154</td>\n",
       "      <td>-0.073</td>\n",
       "      <td>-0.171</td>\n",
       "      <td>0.287</td>\n",
       "      <td>0.007</td>\n",
       "      <td>-0.158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.207</td>\n",
       "      <td>-0.077</td>\n",
       "      <td>0.050</td>\n",
       "      <td>-0.103</td>\n",
       "      <td>0.072</td>\n",
       "      <td>0.256</td>\n",
       "      <td>0.049</td>\n",
       "      <td>-0.070</td>\n",
       "      <td>0.042</td>\n",
       "      <td>-0.147</td>\n",
       "      <td>-0.120</td>\n",
       "      <td>0.148</td>\n",
       "      <td>0.343</td>\n",
       "      <td>0.253</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.128</td>\n",
       "      <td>0.073</td>\n",
       "      <td>0.216</td>\n",
       "      <td>0.452</td>\n",
       "      <td>0.077</td>\n",
       "      <td>-0.033</td>\n",
       "      <td>0.117</td>\n",
       "      <td>-0.261</td>\n",
       "      <td>-0.141</td>\n",
       "      <td>-0.087</td>\n",
       "      <td>-0.282</td>\n",
       "      <td>-0.111</td>\n",
       "      <td>-0.109</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.141</td>\n",
       "      <td>0.064</td>\n",
       "      <td>0.168</td>\n",
       "      <td>0.105</td>\n",
       "      <td>-0.095</td>\n",
       "      <td>0.212</td>\n",
       "      <td>-0.085</td>\n",
       "      <td>0.052</td>\n",
       "      <td>-0.119</td>\n",
       "      <td>0.024</td>\n",
       "      <td>-0.024</td>\n",
       "      <td>0.096</td>\n",
       "      <td>-0.105</td>\n",
       "      <td>-0.219</td>\n",
       "      <td>0.162</td>\n",
       "      <td>0.376</td>\n",
       "      <td>-0.171</td>\n",
       "      <td>0.183</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.036</td>\n",
       "      <td>-0.025</td>\n",
       "      <td>-0.022</td>\n",
       "      <td>-0.047</td>\n",
       "      <td>-0.057</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>0.159</td>\n",
       "      <td>0.092</td>\n",
       "      <td>-0.193</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.084</td>\n",
       "      <td>0.261</td>\n",
       "      <td>0.211</td>\n",
       "      <td>-0.212</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.197</td>\n",
       "      <td>0.059</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.030</td>\n",
       "      <td>-0.137</td>\n",
       "      <td>0.111</td>\n",
       "      <td>-0.045</td>\n",
       "      <td>-0.132</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.273</td>\n",
       "      <td>-0.057</td>\n",
       "      <td>0.069</td>\n",
       "      <td>-0.162</td>\n",
       "      <td>0.110</td>\n",
       "      <td>-0.122</td>\n",
       "      <td>0.160</td>\n",
       "      <td>0.126</td>\n",
       "      <td>-0.183</td>\n",
       "      <td>0.228</td>\n",
       "      <td>-0.105</td>\n",
       "      <td>0.157</td>\n",
       "      <td>-0.037</td>\n",
       "      <td>-0.052</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>0.046</td>\n",
       "      <td>-0.048</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.043</td>\n",
       "      <td>-0.245</td>\n",
       "      <td>0.287</td>\n",
       "      <td>-0.074</td>\n",
       "      <td>-0.148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.138</td>\n",
       "      <td>0.105</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>0.014</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>-0.072</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.070</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>0.094</td>\n",
       "      <td>-0.028</td>\n",
       "      <td>0.242</td>\n",
       "      <td>0.284</td>\n",
       "      <td>-0.144</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.182</td>\n",
       "      <td>0.107</td>\n",
       "      <td>0.215</td>\n",
       "      <td>0.014</td>\n",
       "      <td>-0.082</td>\n",
       "      <td>0.077</td>\n",
       "      <td>0.010</td>\n",
       "      <td>-0.169</td>\n",
       "      <td>-0.093</td>\n",
       "      <td>-0.360</td>\n",
       "      <td>-0.074</td>\n",
       "      <td>0.094</td>\n",
       "      <td>-0.156</td>\n",
       "      <td>0.110</td>\n",
       "      <td>-0.138</td>\n",
       "      <td>0.124</td>\n",
       "      <td>0.101</td>\n",
       "      <td>-0.094</td>\n",
       "      <td>0.252</td>\n",
       "      <td>-0.120</td>\n",
       "      <td>0.127</td>\n",
       "      <td>-0.092</td>\n",
       "      <td>-0.186</td>\n",
       "      <td>-0.014</td>\n",
       "      <td>0.004</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>-0.111</td>\n",
       "      <td>0.093</td>\n",
       "      <td>0.026</td>\n",
       "      <td>-0.164</td>\n",
       "      <td>0.242</td>\n",
       "      <td>-0.158</td>\n",
       "      <td>-0.277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>-0.243</td>\n",
       "      <td>-0.159</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.052</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.102</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.055</td>\n",
       "      <td>-0.140</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>0.112</td>\n",
       "      <td>0.149</td>\n",
       "      <td>0.033</td>\n",
       "      <td>-0.082</td>\n",
       "      <td>-0.057</td>\n",
       "      <td>0.194</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.196</td>\n",
       "      <td>-0.016</td>\n",
       "      <td>-0.051</td>\n",
       "      <td>0.081</td>\n",
       "      <td>-0.074</td>\n",
       "      <td>-0.102</td>\n",
       "      <td>-0.130</td>\n",
       "      <td>-0.329</td>\n",
       "      <td>-0.056</td>\n",
       "      <td>-0.026</td>\n",
       "      <td>-0.115</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>-0.105</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.069</td>\n",
       "      <td>-0.147</td>\n",
       "      <td>0.336</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.121</td>\n",
       "      <td>-0.062</td>\n",
       "      <td>-0.230</td>\n",
       "      <td>-0.066</td>\n",
       "      <td>0.049</td>\n",
       "      <td>-0.014</td>\n",
       "      <td>-0.133</td>\n",
       "      <td>0.154</td>\n",
       "      <td>-0.073</td>\n",
       "      <td>-0.171</td>\n",
       "      <td>0.287</td>\n",
       "      <td>0.007</td>\n",
       "      <td>-0.158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>-0.048</td>\n",
       "      <td>-0.148</td>\n",
       "      <td>-0.175</td>\n",
       "      <td>0.075</td>\n",
       "      <td>-0.050</td>\n",
       "      <td>0.068</td>\n",
       "      <td>0.060</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.085</td>\n",
       "      <td>-0.063</td>\n",
       "      <td>-0.091</td>\n",
       "      <td>0.085</td>\n",
       "      <td>0.220</td>\n",
       "      <td>0.139</td>\n",
       "      <td>-0.147</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.203</td>\n",
       "      <td>0.099</td>\n",
       "      <td>0.182</td>\n",
       "      <td>0.060</td>\n",
       "      <td>-0.148</td>\n",
       "      <td>0.243</td>\n",
       "      <td>-0.033</td>\n",
       "      <td>-0.120</td>\n",
       "      <td>-0.070</td>\n",
       "      <td>-0.232</td>\n",
       "      <td>-0.136</td>\n",
       "      <td>-0.128</td>\n",
       "      <td>-0.167</td>\n",
       "      <td>0.044</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>0.214</td>\n",
       "      <td>0.070</td>\n",
       "      <td>-0.109</td>\n",
       "      <td>0.323</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>-0.026</td>\n",
       "      <td>-0.105</td>\n",
       "      <td>-0.129</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.069</td>\n",
       "      <td>-0.155</td>\n",
       "      <td>-0.091</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.042</td>\n",
       "      <td>-0.087</td>\n",
       "      <td>0.148</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>-0.077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>-0.157</td>\n",
       "      <td>-0.057</td>\n",
       "      <td>-0.118</td>\n",
       "      <td>0.153</td>\n",
       "      <td>-0.099</td>\n",
       "      <td>0.079</td>\n",
       "      <td>0.054</td>\n",
       "      <td>-0.016</td>\n",
       "      <td>0.073</td>\n",
       "      <td>0.048</td>\n",
       "      <td>-0.059</td>\n",
       "      <td>0.079</td>\n",
       "      <td>0.258</td>\n",
       "      <td>0.160</td>\n",
       "      <td>-0.130</td>\n",
       "      <td>0.094</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.098</td>\n",
       "      <td>0.193</td>\n",
       "      <td>0.023</td>\n",
       "      <td>-0.145</td>\n",
       "      <td>0.133</td>\n",
       "      <td>-0.051</td>\n",
       "      <td>-0.142</td>\n",
       "      <td>-0.134</td>\n",
       "      <td>-0.265</td>\n",
       "      <td>-0.149</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>-0.062</td>\n",
       "      <td>0.073</td>\n",
       "      <td>-0.101</td>\n",
       "      <td>0.148</td>\n",
       "      <td>0.062</td>\n",
       "      <td>-0.120</td>\n",
       "      <td>0.338</td>\n",
       "      <td>-0.077</td>\n",
       "      <td>0.092</td>\n",
       "      <td>-0.061</td>\n",
       "      <td>-0.166</td>\n",
       "      <td>-0.065</td>\n",
       "      <td>0.044</td>\n",
       "      <td>-0.061</td>\n",
       "      <td>-0.168</td>\n",
       "      <td>0.089</td>\n",
       "      <td>0.040</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>0.180</td>\n",
       "      <td>-0.129</td>\n",
       "      <td>-0.185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.008</td>\n",
       "      <td>-0.119</td>\n",
       "      <td>-0.050</td>\n",
       "      <td>-0.066</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.105</td>\n",
       "      <td>0.137</td>\n",
       "      <td>-0.089</td>\n",
       "      <td>0.192</td>\n",
       "      <td>0.267</td>\n",
       "      <td>0.111</td>\n",
       "      <td>-0.243</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.117</td>\n",
       "      <td>0.072</td>\n",
       "      <td>0.252</td>\n",
       "      <td>0.019</td>\n",
       "      <td>-0.160</td>\n",
       "      <td>0.142</td>\n",
       "      <td>-0.048</td>\n",
       "      <td>-0.060</td>\n",
       "      <td>-0.056</td>\n",
       "      <td>-0.233</td>\n",
       "      <td>-0.162</td>\n",
       "      <td>-0.013</td>\n",
       "      <td>-0.059</td>\n",
       "      <td>0.009</td>\n",
       "      <td>-0.127</td>\n",
       "      <td>0.175</td>\n",
       "      <td>0.017</td>\n",
       "      <td>-0.116</td>\n",
       "      <td>0.319</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>0.092</td>\n",
       "      <td>0.022</td>\n",
       "      <td>-0.150</td>\n",
       "      <td>-0.110</td>\n",
       "      <td>0.050</td>\n",
       "      <td>-0.022</td>\n",
       "      <td>-0.168</td>\n",
       "      <td>0.095</td>\n",
       "      <td>-0.016</td>\n",
       "      <td>-0.127</td>\n",
       "      <td>0.260</td>\n",
       "      <td>-0.153</td>\n",
       "      <td>-0.090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>-0.081</td>\n",
       "      <td>-0.126</td>\n",
       "      <td>-0.097</td>\n",
       "      <td>-0.084</td>\n",
       "      <td>-0.074</td>\n",
       "      <td>0.058</td>\n",
       "      <td>0.016</td>\n",
       "      <td>-0.014</td>\n",
       "      <td>0.041</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>-0.055</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.239</td>\n",
       "      <td>0.139</td>\n",
       "      <td>-0.201</td>\n",
       "      <td>0.111</td>\n",
       "      <td>0.149</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.204</td>\n",
       "      <td>0.031</td>\n",
       "      <td>-0.226</td>\n",
       "      <td>0.166</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>-0.089</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>-0.279</td>\n",
       "      <td>-0.147</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>-0.145</td>\n",
       "      <td>0.030</td>\n",
       "      <td>-0.134</td>\n",
       "      <td>0.093</td>\n",
       "      <td>0.096</td>\n",
       "      <td>-0.143</td>\n",
       "      <td>0.342</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>0.115</td>\n",
       "      <td>0.003</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>-0.066</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.007</td>\n",
       "      <td>-0.130</td>\n",
       "      <td>0.090</td>\n",
       "      <td>-0.042</td>\n",
       "      <td>-0.134</td>\n",
       "      <td>0.195</td>\n",
       "      <td>-0.180</td>\n",
       "      <td>-0.040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>-0.119</td>\n",
       "      <td>0.082</td>\n",
       "      <td>-0.057</td>\n",
       "      <td>-0.097</td>\n",
       "      <td>0.053</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.055</td>\n",
       "      <td>-0.049</td>\n",
       "      <td>0.065</td>\n",
       "      <td>0.303</td>\n",
       "      <td>0.214</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>0.060</td>\n",
       "      <td>0.172</td>\n",
       "      <td>0.106</td>\n",
       "      <td>0.183</td>\n",
       "      <td>0.076</td>\n",
       "      <td>-0.121</td>\n",
       "      <td>0.134</td>\n",
       "      <td>-0.116</td>\n",
       "      <td>-0.087</td>\n",
       "      <td>-0.041</td>\n",
       "      <td>-0.191</td>\n",
       "      <td>-0.234</td>\n",
       "      <td>-0.117</td>\n",
       "      <td>-0.217</td>\n",
       "      <td>0.107</td>\n",
       "      <td>-0.099</td>\n",
       "      <td>0.188</td>\n",
       "      <td>0.136</td>\n",
       "      <td>-0.108</td>\n",
       "      <td>0.339</td>\n",
       "      <td>0.065</td>\n",
       "      <td>0.046</td>\n",
       "      <td>-0.203</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.046</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>0.038</td>\n",
       "      <td>-0.086</td>\n",
       "      <td>0.124</td>\n",
       "      <td>0.029</td>\n",
       "      <td>-0.209</td>\n",
       "      <td>0.186</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.045</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>-0.142</td>\n",
       "      <td>-0.197</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.106</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.053</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>0.133</td>\n",
       "      <td>0.301</td>\n",
       "      <td>0.081</td>\n",
       "      <td>-0.201</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.162</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.232</td>\n",
       "      <td>0.012</td>\n",
       "      <td>-0.172</td>\n",
       "      <td>0.210</td>\n",
       "      <td>-0.024</td>\n",
       "      <td>-0.109</td>\n",
       "      <td>0.080</td>\n",
       "      <td>-0.216</td>\n",
       "      <td>-0.054</td>\n",
       "      <td>-0.044</td>\n",
       "      <td>-0.162</td>\n",
       "      <td>0.061</td>\n",
       "      <td>-0.120</td>\n",
       "      <td>0.212</td>\n",
       "      <td>0.061</td>\n",
       "      <td>-0.224</td>\n",
       "      <td>0.274</td>\n",
       "      <td>0.155</td>\n",
       "      <td>0.008</td>\n",
       "      <td>-0.171</td>\n",
       "      <td>-0.042</td>\n",
       "      <td>-0.028</td>\n",
       "      <td>0.060</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>-0.037</td>\n",
       "      <td>0.138</td>\n",
       "      <td>0.117</td>\n",
       "      <td>-0.202</td>\n",
       "      <td>0.167</td>\n",
       "      <td>-0.258</td>\n",
       "      <td>-0.132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>-0.243</td>\n",
       "      <td>-0.159</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.052</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.102</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.055</td>\n",
       "      <td>-0.140</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>0.112</td>\n",
       "      <td>0.149</td>\n",
       "      <td>0.033</td>\n",
       "      <td>-0.082</td>\n",
       "      <td>-0.057</td>\n",
       "      <td>0.194</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.196</td>\n",
       "      <td>-0.016</td>\n",
       "      <td>-0.051</td>\n",
       "      <td>0.081</td>\n",
       "      <td>-0.074</td>\n",
       "      <td>-0.102</td>\n",
       "      <td>-0.130</td>\n",
       "      <td>-0.329</td>\n",
       "      <td>-0.056</td>\n",
       "      <td>-0.026</td>\n",
       "      <td>-0.115</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>-0.105</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.069</td>\n",
       "      <td>-0.147</td>\n",
       "      <td>0.336</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.121</td>\n",
       "      <td>-0.062</td>\n",
       "      <td>-0.230</td>\n",
       "      <td>-0.066</td>\n",
       "      <td>0.049</td>\n",
       "      <td>-0.014</td>\n",
       "      <td>-0.133</td>\n",
       "      <td>0.154</td>\n",
       "      <td>-0.073</td>\n",
       "      <td>-0.171</td>\n",
       "      <td>0.287</td>\n",
       "      <td>0.007</td>\n",
       "      <td>-0.158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.158</td>\n",
       "      <td>-0.027</td>\n",
       "      <td>-0.093</td>\n",
       "      <td>0.034</td>\n",
       "      <td>-0.141</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.053</td>\n",
       "      <td>0.136</td>\n",
       "      <td>0.091</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.176</td>\n",
       "      <td>0.263</td>\n",
       "      <td>0.302</td>\n",
       "      <td>-0.163</td>\n",
       "      <td>0.067</td>\n",
       "      <td>0.163</td>\n",
       "      <td>-0.022</td>\n",
       "      <td>0.217</td>\n",
       "      <td>0.088</td>\n",
       "      <td>-0.215</td>\n",
       "      <td>0.226</td>\n",
       "      <td>-0.085</td>\n",
       "      <td>-0.120</td>\n",
       "      <td>0.050</td>\n",
       "      <td>-0.287</td>\n",
       "      <td>-0.215</td>\n",
       "      <td>-0.116</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>0.091</td>\n",
       "      <td>-0.162</td>\n",
       "      <td>0.170</td>\n",
       "      <td>0.004</td>\n",
       "      <td>-0.051</td>\n",
       "      <td>0.284</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>0.073</td>\n",
       "      <td>-0.137</td>\n",
       "      <td>-0.053</td>\n",
       "      <td>-0.033</td>\n",
       "      <td>0.024</td>\n",
       "      <td>-0.070</td>\n",
       "      <td>-0.027</td>\n",
       "      <td>0.189</td>\n",
       "      <td>0.008</td>\n",
       "      <td>-0.119</td>\n",
       "      <td>0.164</td>\n",
       "      <td>-0.103</td>\n",
       "      <td>0.020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.158</td>\n",
       "      <td>-0.027</td>\n",
       "      <td>-0.093</td>\n",
       "      <td>0.034</td>\n",
       "      <td>-0.141</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.053</td>\n",
       "      <td>0.136</td>\n",
       "      <td>0.091</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.176</td>\n",
       "      <td>0.263</td>\n",
       "      <td>0.302</td>\n",
       "      <td>-0.163</td>\n",
       "      <td>0.067</td>\n",
       "      <td>0.163</td>\n",
       "      <td>-0.022</td>\n",
       "      <td>0.217</td>\n",
       "      <td>0.088</td>\n",
       "      <td>-0.215</td>\n",
       "      <td>0.226</td>\n",
       "      <td>-0.085</td>\n",
       "      <td>-0.120</td>\n",
       "      <td>0.050</td>\n",
       "      <td>-0.287</td>\n",
       "      <td>-0.215</td>\n",
       "      <td>-0.116</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>0.091</td>\n",
       "      <td>-0.162</td>\n",
       "      <td>0.170</td>\n",
       "      <td>0.004</td>\n",
       "      <td>-0.051</td>\n",
       "      <td>0.284</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>0.073</td>\n",
       "      <td>-0.137</td>\n",
       "      <td>-0.053</td>\n",
       "      <td>-0.033</td>\n",
       "      <td>0.024</td>\n",
       "      <td>-0.070</td>\n",
       "      <td>-0.027</td>\n",
       "      <td>0.189</td>\n",
       "      <td>0.008</td>\n",
       "      <td>-0.119</td>\n",
       "      <td>0.164</td>\n",
       "      <td>-0.103</td>\n",
       "      <td>0.020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>-0.012</td>\n",
       "      <td>0.030</td>\n",
       "      <td>-0.028</td>\n",
       "      <td>-0.140</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>0.165</td>\n",
       "      <td>0.060</td>\n",
       "      <td>0.138</td>\n",
       "      <td>-0.108</td>\n",
       "      <td>-0.121</td>\n",
       "      <td>0.240</td>\n",
       "      <td>0.296</td>\n",
       "      <td>0.206</td>\n",
       "      <td>-0.100</td>\n",
       "      <td>-0.038</td>\n",
       "      <td>0.246</td>\n",
       "      <td>0.154</td>\n",
       "      <td>0.183</td>\n",
       "      <td>-0.015</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.170</td>\n",
       "      <td>-0.025</td>\n",
       "      <td>-0.105</td>\n",
       "      <td>-0.025</td>\n",
       "      <td>-0.186</td>\n",
       "      <td>-0.039</td>\n",
       "      <td>-0.112</td>\n",
       "      <td>-0.116</td>\n",
       "      <td>-0.039</td>\n",
       "      <td>-0.166</td>\n",
       "      <td>0.163</td>\n",
       "      <td>0.173</td>\n",
       "      <td>-0.101</td>\n",
       "      <td>0.401</td>\n",
       "      <td>0.088</td>\n",
       "      <td>0.003</td>\n",
       "      <td>-0.113</td>\n",
       "      <td>-0.149</td>\n",
       "      <td>-0.106</td>\n",
       "      <td>-0.039</td>\n",
       "      <td>0.033</td>\n",
       "      <td>-0.258</td>\n",
       "      <td>0.190</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>-0.098</td>\n",
       "      <td>0.100</td>\n",
       "      <td>-0.113</td>\n",
       "      <td>-0.032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>-0.022</td>\n",
       "      <td>-0.032</td>\n",
       "      <td>-0.102</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>-0.053</td>\n",
       "      <td>0.113</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.216</td>\n",
       "      <td>-0.077</td>\n",
       "      <td>-0.066</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.343</td>\n",
       "      <td>0.073</td>\n",
       "      <td>-0.220</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.130</td>\n",
       "      <td>0.103</td>\n",
       "      <td>0.258</td>\n",
       "      <td>0.024</td>\n",
       "      <td>-0.157</td>\n",
       "      <td>0.215</td>\n",
       "      <td>0.026</td>\n",
       "      <td>-0.072</td>\n",
       "      <td>-0.044</td>\n",
       "      <td>-0.275</td>\n",
       "      <td>-0.057</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>-0.135</td>\n",
       "      <td>0.053</td>\n",
       "      <td>-0.170</td>\n",
       "      <td>0.101</td>\n",
       "      <td>0.034</td>\n",
       "      <td>-0.126</td>\n",
       "      <td>0.376</td>\n",
       "      <td>-0.022</td>\n",
       "      <td>0.069</td>\n",
       "      <td>-0.051</td>\n",
       "      <td>-0.187</td>\n",
       "      <td>-0.031</td>\n",
       "      <td>0.003</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>-0.112</td>\n",
       "      <td>0.096</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>-0.110</td>\n",
       "      <td>0.093</td>\n",
       "      <td>-0.197</td>\n",
       "      <td>-0.076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>-0.119</td>\n",
       "      <td>0.082</td>\n",
       "      <td>-0.057</td>\n",
       "      <td>-0.097</td>\n",
       "      <td>0.053</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.055</td>\n",
       "      <td>-0.049</td>\n",
       "      <td>0.065</td>\n",
       "      <td>0.303</td>\n",
       "      <td>0.214</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>0.060</td>\n",
       "      <td>0.172</td>\n",
       "      <td>0.106</td>\n",
       "      <td>0.183</td>\n",
       "      <td>0.076</td>\n",
       "      <td>-0.121</td>\n",
       "      <td>0.134</td>\n",
       "      <td>-0.116</td>\n",
       "      <td>-0.087</td>\n",
       "      <td>-0.041</td>\n",
       "      <td>-0.191</td>\n",
       "      <td>-0.234</td>\n",
       "      <td>-0.117</td>\n",
       "      <td>-0.217</td>\n",
       "      <td>0.107</td>\n",
       "      <td>-0.099</td>\n",
       "      <td>0.188</td>\n",
       "      <td>0.136</td>\n",
       "      <td>-0.108</td>\n",
       "      <td>0.339</td>\n",
       "      <td>0.065</td>\n",
       "      <td>0.046</td>\n",
       "      <td>-0.203</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.046</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>0.038</td>\n",
       "      <td>-0.086</td>\n",
       "      <td>0.124</td>\n",
       "      <td>0.029</td>\n",
       "      <td>-0.209</td>\n",
       "      <td>0.186</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>-0.243</td>\n",
       "      <td>-0.159</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.052</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.102</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.055</td>\n",
       "      <td>-0.140</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>0.112</td>\n",
       "      <td>0.149</td>\n",
       "      <td>0.033</td>\n",
       "      <td>-0.082</td>\n",
       "      <td>-0.057</td>\n",
       "      <td>0.194</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.196</td>\n",
       "      <td>-0.016</td>\n",
       "      <td>-0.051</td>\n",
       "      <td>0.081</td>\n",
       "      <td>-0.074</td>\n",
       "      <td>-0.102</td>\n",
       "      <td>-0.130</td>\n",
       "      <td>-0.329</td>\n",
       "      <td>-0.056</td>\n",
       "      <td>-0.026</td>\n",
       "      <td>-0.115</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>-0.105</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.069</td>\n",
       "      <td>-0.147</td>\n",
       "      <td>0.336</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.121</td>\n",
       "      <td>-0.062</td>\n",
       "      <td>-0.230</td>\n",
       "      <td>-0.066</td>\n",
       "      <td>0.049</td>\n",
       "      <td>-0.014</td>\n",
       "      <td>-0.133</td>\n",
       "      <td>0.154</td>\n",
       "      <td>-0.073</td>\n",
       "      <td>-0.171</td>\n",
       "      <td>0.287</td>\n",
       "      <td>0.007</td>\n",
       "      <td>-0.158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>0.048</td>\n",
       "      <td>-0.129</td>\n",
       "      <td>0.076</td>\n",
       "      <td>-0.074</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.071</td>\n",
       "      <td>-0.040</td>\n",
       "      <td>-0.049</td>\n",
       "      <td>-0.200</td>\n",
       "      <td>0.085</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.166</td>\n",
       "      <td>-0.247</td>\n",
       "      <td>0.130</td>\n",
       "      <td>0.094</td>\n",
       "      <td>0.058</td>\n",
       "      <td>0.298</td>\n",
       "      <td>0.005</td>\n",
       "      <td>-0.253</td>\n",
       "      <td>0.227</td>\n",
       "      <td>-0.035</td>\n",
       "      <td>-0.147</td>\n",
       "      <td>-0.082</td>\n",
       "      <td>-0.317</td>\n",
       "      <td>-0.025</td>\n",
       "      <td>-0.046</td>\n",
       "      <td>-0.030</td>\n",
       "      <td>0.103</td>\n",
       "      <td>-0.133</td>\n",
       "      <td>0.252</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.142</td>\n",
       "      <td>0.293</td>\n",
       "      <td>-0.012</td>\n",
       "      <td>0.086</td>\n",
       "      <td>-0.136</td>\n",
       "      <td>-0.093</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0.106</td>\n",
       "      <td>-0.052</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.099</td>\n",
       "      <td>0.094</td>\n",
       "      <td>-0.234</td>\n",
       "      <td>0.172</td>\n",
       "      <td>-0.092</td>\n",
       "      <td>-0.030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.008</td>\n",
       "      <td>-0.119</td>\n",
       "      <td>-0.050</td>\n",
       "      <td>-0.066</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.105</td>\n",
       "      <td>0.137</td>\n",
       "      <td>-0.089</td>\n",
       "      <td>0.192</td>\n",
       "      <td>0.267</td>\n",
       "      <td>0.111</td>\n",
       "      <td>-0.243</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.117</td>\n",
       "      <td>0.072</td>\n",
       "      <td>0.252</td>\n",
       "      <td>0.019</td>\n",
       "      <td>-0.160</td>\n",
       "      <td>0.142</td>\n",
       "      <td>-0.048</td>\n",
       "      <td>-0.060</td>\n",
       "      <td>-0.056</td>\n",
       "      <td>-0.233</td>\n",
       "      <td>-0.162</td>\n",
       "      <td>-0.013</td>\n",
       "      <td>-0.059</td>\n",
       "      <td>0.009</td>\n",
       "      <td>-0.127</td>\n",
       "      <td>0.175</td>\n",
       "      <td>0.017</td>\n",
       "      <td>-0.116</td>\n",
       "      <td>0.319</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>0.092</td>\n",
       "      <td>0.022</td>\n",
       "      <td>-0.150</td>\n",
       "      <td>-0.110</td>\n",
       "      <td>0.050</td>\n",
       "      <td>-0.022</td>\n",
       "      <td>-0.168</td>\n",
       "      <td>0.095</td>\n",
       "      <td>-0.016</td>\n",
       "      <td>-0.127</td>\n",
       "      <td>0.260</td>\n",
       "      <td>-0.153</td>\n",
       "      <td>-0.090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>-0.243</td>\n",
       "      <td>-0.159</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.052</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.102</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.055</td>\n",
       "      <td>-0.140</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>0.112</td>\n",
       "      <td>0.149</td>\n",
       "      <td>0.033</td>\n",
       "      <td>-0.082</td>\n",
       "      <td>-0.057</td>\n",
       "      <td>0.194</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.196</td>\n",
       "      <td>-0.016</td>\n",
       "      <td>-0.051</td>\n",
       "      <td>0.081</td>\n",
       "      <td>-0.074</td>\n",
       "      <td>-0.102</td>\n",
       "      <td>-0.130</td>\n",
       "      <td>-0.329</td>\n",
       "      <td>-0.056</td>\n",
       "      <td>-0.026</td>\n",
       "      <td>-0.115</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>-0.105</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.069</td>\n",
       "      <td>-0.147</td>\n",
       "      <td>0.336</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.121</td>\n",
       "      <td>-0.062</td>\n",
       "      <td>-0.230</td>\n",
       "      <td>-0.066</td>\n",
       "      <td>0.049</td>\n",
       "      <td>-0.014</td>\n",
       "      <td>-0.133</td>\n",
       "      <td>0.154</td>\n",
       "      <td>-0.073</td>\n",
       "      <td>-0.171</td>\n",
       "      <td>0.287</td>\n",
       "      <td>0.007</td>\n",
       "      <td>-0.158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>-0.022</td>\n",
       "      <td>-0.032</td>\n",
       "      <td>-0.102</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>-0.053</td>\n",
       "      <td>0.113</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.216</td>\n",
       "      <td>-0.077</td>\n",
       "      <td>-0.066</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.343</td>\n",
       "      <td>0.073</td>\n",
       "      <td>-0.220</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.130</td>\n",
       "      <td>0.103</td>\n",
       "      <td>0.258</td>\n",
       "      <td>0.024</td>\n",
       "      <td>-0.157</td>\n",
       "      <td>0.215</td>\n",
       "      <td>0.026</td>\n",
       "      <td>-0.072</td>\n",
       "      <td>-0.044</td>\n",
       "      <td>-0.275</td>\n",
       "      <td>-0.057</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>-0.135</td>\n",
       "      <td>0.053</td>\n",
       "      <td>-0.170</td>\n",
       "      <td>0.101</td>\n",
       "      <td>0.034</td>\n",
       "      <td>-0.126</td>\n",
       "      <td>0.376</td>\n",
       "      <td>-0.022</td>\n",
       "      <td>0.069</td>\n",
       "      <td>-0.051</td>\n",
       "      <td>-0.187</td>\n",
       "      <td>-0.031</td>\n",
       "      <td>0.003</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>-0.112</td>\n",
       "      <td>0.096</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>-0.110</td>\n",
       "      <td>0.093</td>\n",
       "      <td>-0.197</td>\n",
       "      <td>-0.076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.036</td>\n",
       "      <td>-0.025</td>\n",
       "      <td>-0.022</td>\n",
       "      <td>-0.047</td>\n",
       "      <td>-0.057</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>0.159</td>\n",
       "      <td>0.092</td>\n",
       "      <td>-0.193</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.084</td>\n",
       "      <td>0.261</td>\n",
       "      <td>0.211</td>\n",
       "      <td>-0.212</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.197</td>\n",
       "      <td>0.059</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.030</td>\n",
       "      <td>-0.137</td>\n",
       "      <td>0.111</td>\n",
       "      <td>-0.045</td>\n",
       "      <td>-0.132</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.273</td>\n",
       "      <td>-0.057</td>\n",
       "      <td>0.069</td>\n",
       "      <td>-0.162</td>\n",
       "      <td>0.110</td>\n",
       "      <td>-0.122</td>\n",
       "      <td>0.160</td>\n",
       "      <td>0.126</td>\n",
       "      <td>-0.183</td>\n",
       "      <td>0.228</td>\n",
       "      <td>-0.105</td>\n",
       "      <td>0.157</td>\n",
       "      <td>-0.037</td>\n",
       "      <td>-0.052</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>0.046</td>\n",
       "      <td>-0.048</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.043</td>\n",
       "      <td>-0.245</td>\n",
       "      <td>0.287</td>\n",
       "      <td>-0.074</td>\n",
       "      <td>-0.148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>-0.081</td>\n",
       "      <td>-0.126</td>\n",
       "      <td>-0.097</td>\n",
       "      <td>-0.084</td>\n",
       "      <td>-0.074</td>\n",
       "      <td>0.058</td>\n",
       "      <td>0.016</td>\n",
       "      <td>-0.014</td>\n",
       "      <td>0.041</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>-0.055</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.239</td>\n",
       "      <td>0.139</td>\n",
       "      <td>-0.201</td>\n",
       "      <td>0.111</td>\n",
       "      <td>0.149</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.204</td>\n",
       "      <td>0.031</td>\n",
       "      <td>-0.226</td>\n",
       "      <td>0.166</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>-0.089</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>-0.279</td>\n",
       "      <td>-0.147</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>-0.145</td>\n",
       "      <td>0.030</td>\n",
       "      <td>-0.134</td>\n",
       "      <td>0.093</td>\n",
       "      <td>0.096</td>\n",
       "      <td>-0.143</td>\n",
       "      <td>0.342</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>0.115</td>\n",
       "      <td>0.003</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>-0.066</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.007</td>\n",
       "      <td>-0.130</td>\n",
       "      <td>0.090</td>\n",
       "      <td>-0.042</td>\n",
       "      <td>-0.134</td>\n",
       "      <td>0.195</td>\n",
       "      <td>-0.180</td>\n",
       "      <td>-0.040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.045</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>-0.142</td>\n",
       "      <td>-0.197</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.106</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.053</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>0.133</td>\n",
       "      <td>0.301</td>\n",
       "      <td>0.081</td>\n",
       "      <td>-0.201</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.162</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.232</td>\n",
       "      <td>0.012</td>\n",
       "      <td>-0.172</td>\n",
       "      <td>0.210</td>\n",
       "      <td>-0.024</td>\n",
       "      <td>-0.109</td>\n",
       "      <td>0.080</td>\n",
       "      <td>-0.216</td>\n",
       "      <td>-0.054</td>\n",
       "      <td>-0.044</td>\n",
       "      <td>-0.162</td>\n",
       "      <td>0.061</td>\n",
       "      <td>-0.120</td>\n",
       "      <td>0.212</td>\n",
       "      <td>0.061</td>\n",
       "      <td>-0.224</td>\n",
       "      <td>0.274</td>\n",
       "      <td>0.155</td>\n",
       "      <td>0.008</td>\n",
       "      <td>-0.171</td>\n",
       "      <td>-0.042</td>\n",
       "      <td>-0.028</td>\n",
       "      <td>0.060</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>-0.037</td>\n",
       "      <td>0.138</td>\n",
       "      <td>0.117</td>\n",
       "      <td>-0.202</td>\n",
       "      <td>0.167</td>\n",
       "      <td>-0.258</td>\n",
       "      <td>-0.132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.008</td>\n",
       "      <td>-0.119</td>\n",
       "      <td>-0.050</td>\n",
       "      <td>-0.066</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.105</td>\n",
       "      <td>0.137</td>\n",
       "      <td>-0.089</td>\n",
       "      <td>0.192</td>\n",
       "      <td>0.267</td>\n",
       "      <td>0.111</td>\n",
       "      <td>-0.243</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.117</td>\n",
       "      <td>0.072</td>\n",
       "      <td>0.252</td>\n",
       "      <td>0.019</td>\n",
       "      <td>-0.160</td>\n",
       "      <td>0.142</td>\n",
       "      <td>-0.048</td>\n",
       "      <td>-0.060</td>\n",
       "      <td>-0.056</td>\n",
       "      <td>-0.233</td>\n",
       "      <td>-0.162</td>\n",
       "      <td>-0.013</td>\n",
       "      <td>-0.059</td>\n",
       "      <td>0.009</td>\n",
       "      <td>-0.127</td>\n",
       "      <td>0.175</td>\n",
       "      <td>0.017</td>\n",
       "      <td>-0.116</td>\n",
       "      <td>0.319</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>0.092</td>\n",
       "      <td>0.022</td>\n",
       "      <td>-0.150</td>\n",
       "      <td>-0.110</td>\n",
       "      <td>0.050</td>\n",
       "      <td>-0.022</td>\n",
       "      <td>-0.168</td>\n",
       "      <td>0.095</td>\n",
       "      <td>-0.016</td>\n",
       "      <td>-0.127</td>\n",
       "      <td>0.260</td>\n",
       "      <td>-0.153</td>\n",
       "      <td>-0.090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>-0.081</td>\n",
       "      <td>-0.126</td>\n",
       "      <td>-0.097</td>\n",
       "      <td>-0.084</td>\n",
       "      <td>-0.074</td>\n",
       "      <td>0.058</td>\n",
       "      <td>0.016</td>\n",
       "      <td>-0.014</td>\n",
       "      <td>0.041</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>-0.055</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.239</td>\n",
       "      <td>0.139</td>\n",
       "      <td>-0.201</td>\n",
       "      <td>0.111</td>\n",
       "      <td>0.149</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.204</td>\n",
       "      <td>0.031</td>\n",
       "      <td>-0.226</td>\n",
       "      <td>0.166</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>-0.089</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>-0.279</td>\n",
       "      <td>-0.147</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>-0.145</td>\n",
       "      <td>0.030</td>\n",
       "      <td>-0.134</td>\n",
       "      <td>0.093</td>\n",
       "      <td>0.096</td>\n",
       "      <td>-0.143</td>\n",
       "      <td>0.342</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>0.115</td>\n",
       "      <td>0.003</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>-0.066</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.007</td>\n",
       "      <td>-0.130</td>\n",
       "      <td>0.090</td>\n",
       "      <td>-0.042</td>\n",
       "      <td>-0.134</td>\n",
       "      <td>0.195</td>\n",
       "      <td>-0.180</td>\n",
       "      <td>-0.040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>-0.157</td>\n",
       "      <td>-0.057</td>\n",
       "      <td>-0.118</td>\n",
       "      <td>0.153</td>\n",
       "      <td>-0.099</td>\n",
       "      <td>0.079</td>\n",
       "      <td>0.054</td>\n",
       "      <td>-0.016</td>\n",
       "      <td>0.073</td>\n",
       "      <td>0.048</td>\n",
       "      <td>-0.059</td>\n",
       "      <td>0.079</td>\n",
       "      <td>0.258</td>\n",
       "      <td>0.160</td>\n",
       "      <td>-0.130</td>\n",
       "      <td>0.094</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.098</td>\n",
       "      <td>0.193</td>\n",
       "      <td>0.023</td>\n",
       "      <td>-0.145</td>\n",
       "      <td>0.133</td>\n",
       "      <td>-0.051</td>\n",
       "      <td>-0.142</td>\n",
       "      <td>-0.134</td>\n",
       "      <td>-0.265</td>\n",
       "      <td>-0.149</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>-0.062</td>\n",
       "      <td>0.073</td>\n",
       "      <td>-0.101</td>\n",
       "      <td>0.148</td>\n",
       "      <td>0.062</td>\n",
       "      <td>-0.120</td>\n",
       "      <td>0.338</td>\n",
       "      <td>-0.077</td>\n",
       "      <td>0.092</td>\n",
       "      <td>-0.061</td>\n",
       "      <td>-0.166</td>\n",
       "      <td>-0.065</td>\n",
       "      <td>0.044</td>\n",
       "      <td>-0.061</td>\n",
       "      <td>-0.168</td>\n",
       "      <td>0.089</td>\n",
       "      <td>0.040</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>0.180</td>\n",
       "      <td>-0.129</td>\n",
       "      <td>-0.185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.008</td>\n",
       "      <td>-0.119</td>\n",
       "      <td>-0.050</td>\n",
       "      <td>-0.066</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.105</td>\n",
       "      <td>0.137</td>\n",
       "      <td>-0.089</td>\n",
       "      <td>0.192</td>\n",
       "      <td>0.267</td>\n",
       "      <td>0.111</td>\n",
       "      <td>-0.243</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.117</td>\n",
       "      <td>0.072</td>\n",
       "      <td>0.252</td>\n",
       "      <td>0.019</td>\n",
       "      <td>-0.160</td>\n",
       "      <td>0.142</td>\n",
       "      <td>-0.048</td>\n",
       "      <td>-0.060</td>\n",
       "      <td>-0.056</td>\n",
       "      <td>-0.233</td>\n",
       "      <td>-0.162</td>\n",
       "      <td>-0.013</td>\n",
       "      <td>-0.059</td>\n",
       "      <td>0.009</td>\n",
       "      <td>-0.127</td>\n",
       "      <td>0.175</td>\n",
       "      <td>0.017</td>\n",
       "      <td>-0.116</td>\n",
       "      <td>0.319</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>0.092</td>\n",
       "      <td>0.022</td>\n",
       "      <td>-0.150</td>\n",
       "      <td>-0.110</td>\n",
       "      <td>0.050</td>\n",
       "      <td>-0.022</td>\n",
       "      <td>-0.168</td>\n",
       "      <td>0.095</td>\n",
       "      <td>-0.016</td>\n",
       "      <td>-0.127</td>\n",
       "      <td>0.260</td>\n",
       "      <td>-0.153</td>\n",
       "      <td>-0.090</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    w2v_0  w2v_1  w2v_2  w2v_3  w2v_4  w2v_5  w2v_6  w2v_7  w2v_8  w2v_9  w2v_10  w2v_11  w2v_12  w2v_13  w2v_14  w2v_15  w2v_16  w2v_17  w2v_18  w2v_19  w2v_20  w2v_21  w2v_22  w2v_23  w2v_24  w2v_25  w2v_26  w2v_27  w2v_28  w2v_29  w2v_30  w2v_31  w2v_32  w2v_33  w2v_34  w2v_35  w2v_36  w2v_37  w2v_38  w2v_39  w2v_40  w2v_41  w2v_42  w2v_43  w2v_44  w2v_45  w2v_46  w2v_47  w2v_48\n",
       "0  -0.157 -0.057 -0.118  0.153 -0.099  0.079  0.054 -0.016  0.073  0.048  -0.059   0.079   0.258   0.160  -0.130   0.094   0.057   0.098   0.193   0.023  -0.145   0.133  -0.051  -0.142  -0.134  -0.265  -0.149  -0.021  -0.062   0.073  -0.101   0.148   0.062  -0.120   0.338  -0.077   0.092  -0.061  -0.166  -0.065   0.044  -0.061  -0.168   0.089   0.040  -0.125   0.180  -0.129  -0.185\n",
       "1   0.138  0.105 -0.107  0.014 -0.019 -0.072  0.028  0.090  0.070 -0.002   0.094  -0.028   0.242   0.284  -0.144   0.050   0.182   0.107   0.215   0.014  -0.082   0.077   0.010  -0.169  -0.093  -0.360  -0.074   0.094  -0.156   0.110  -0.138   0.124   0.101  -0.094   0.252  -0.120   0.127  -0.092  -0.186  -0.014   0.004  -0.075  -0.111   0.093   0.026  -0.164   0.242  -0.158  -0.277\n",
       "2  -0.119  0.082 -0.057 -0.097  0.053  0.003  0.041  0.010  0.070  0.055  -0.049   0.065   0.303   0.214  -0.080   0.060   0.172   0.106   0.183   0.076  -0.121   0.134  -0.116  -0.087  -0.041  -0.191  -0.234  -0.117  -0.217   0.107  -0.099   0.188   0.136  -0.108   0.339   0.065   0.046  -0.203   0.001  -0.046  -0.002   0.038  -0.086   0.124   0.029  -0.209   0.186  -0.080  -0.167\n",
       "3   0.016 -0.032 -0.110  0.010  0.093  0.097  0.171  0.059  0.090 -0.220  -0.107   0.129   0.317   0.211  -0.014   0.125   0.046   0.144   0.353   0.032  -0.110   0.121  -0.161  -0.087  -0.102  -0.341  -0.077  -0.056   0.028   0.011  -0.074   0.202  -0.002  -0.108   0.243   0.047   0.080  -0.047  -0.088  -0.038   0.082  -0.074  -0.140   0.092   0.192  -0.097   0.182  -0.108  -0.080\n",
       "4   0.008 -0.119 -0.050 -0.066 -0.011  0.044  0.042  0.034  0.105  0.137  -0.089   0.192   0.267   0.111  -0.243   0.039   0.117   0.072   0.252   0.019  -0.160   0.142  -0.048  -0.060  -0.056  -0.233  -0.162  -0.013  -0.059   0.009  -0.127   0.175   0.017  -0.116   0.319  -0.023   0.092   0.022  -0.150  -0.110   0.050  -0.022  -0.168   0.095  -0.016  -0.127   0.260  -0.153  -0.090\n",
       "5   0.045 -0.003 -0.142 -0.197  0.044  0.003  0.055  0.106  0.024  0.053  -0.107   0.133   0.301   0.081  -0.201   0.029   0.162   0.042   0.232   0.012  -0.172   0.210  -0.024  -0.109   0.080  -0.216  -0.054  -0.044  -0.162   0.061  -0.120   0.212   0.061  -0.224   0.274   0.155   0.008  -0.171  -0.042  -0.028   0.060  -0.107  -0.037   0.138   0.117  -0.202   0.167  -0.258  -0.132\n",
       "6  -0.119  0.082 -0.057 -0.097  0.053  0.003  0.041  0.010  0.070  0.055  -0.049   0.065   0.303   0.214  -0.080   0.060   0.172   0.106   0.183   0.076  -0.121   0.134  -0.116  -0.087  -0.041  -0.191  -0.234  -0.117  -0.217   0.107  -0.099   0.188   0.136  -0.108   0.339   0.065   0.046  -0.203   0.001  -0.046  -0.002   0.038  -0.086   0.124   0.029  -0.209   0.186  -0.080  -0.167\n",
       "7  -0.022 -0.032 -0.102 -0.005 -0.053  0.113 -0.075  0.083  0.216 -0.077  -0.066   0.012   0.343   0.073  -0.220   0.042   0.130   0.103   0.258   0.024  -0.157   0.215   0.026  -0.072  -0.044  -0.275  -0.057  -0.043  -0.135   0.053  -0.170   0.101   0.034  -0.126   0.376  -0.022   0.069  -0.051  -0.187  -0.031   0.003  -0.018  -0.112   0.096  -0.023  -0.110   0.093  -0.197  -0.076\n",
       "8  -0.274 -0.463 -0.166 -0.291  0.267  0.133  0.061 -0.195  0.216 -0.279  -0.148   0.232   0.483   0.245   0.022   0.128  -0.073   0.354   0.393   0.308  -0.358   0.077   0.063   0.041  -0.233  -0.392  -0.039  -0.084  -0.013   0.196  -0.261   0.172   0.185   0.208   0.216  -0.250  -0.025  -0.011   0.076  -0.028   0.139  -0.079  -0.163  -0.031   0.325   0.167  -0.015  -0.092   0.033\n",
       "9  -0.119  0.082 -0.057 -0.097  0.053  0.003  0.041  0.010  0.070  0.055  -0.049   0.065   0.303   0.214  -0.080   0.060   0.172   0.106   0.183   0.076  -0.121   0.134  -0.116  -0.087  -0.041  -0.191  -0.234  -0.117  -0.217   0.107  -0.099   0.188   0.136  -0.108   0.339   0.065   0.046  -0.203   0.001  -0.046  -0.002   0.038  -0.086   0.124   0.029  -0.209   0.186  -0.080  -0.167\n",
       "10  0.016 -0.032 -0.110  0.010  0.093  0.097  0.171  0.059  0.090 -0.220  -0.107   0.129   0.317   0.211  -0.014   0.125   0.046   0.144   0.353   0.032  -0.110   0.121  -0.161  -0.087  -0.102  -0.341  -0.077  -0.056   0.028   0.011  -0.074   0.202  -0.002  -0.108   0.243   0.047   0.080  -0.047  -0.088  -0.038   0.082  -0.074  -0.140   0.092   0.192  -0.097   0.182  -0.108  -0.080\n",
       "11 -0.125 -0.243 -0.159  0.063  0.052  0.070  0.102  0.100  0.055 -0.140  -0.107   0.112   0.149   0.033  -0.082  -0.057   0.194   0.054   0.196  -0.016  -0.051   0.081  -0.074  -0.102  -0.130  -0.329  -0.056  -0.026  -0.115  -0.003  -0.105   0.250   0.069  -0.147   0.336   0.009   0.121  -0.062  -0.230  -0.066   0.049  -0.014  -0.133   0.154  -0.073  -0.171   0.287   0.007  -0.158\n",
       "12  0.138  0.105 -0.107  0.014 -0.019 -0.072  0.028  0.090  0.070 -0.002   0.094  -0.028   0.242   0.284  -0.144   0.050   0.182   0.107   0.215   0.014  -0.082   0.077   0.010  -0.169  -0.093  -0.360  -0.074   0.094  -0.156   0.110  -0.138   0.124   0.101  -0.094   0.252  -0.120   0.127  -0.092  -0.186  -0.014   0.004  -0.075  -0.111   0.093   0.026  -0.164   0.242  -0.158  -0.277\n",
       "13  0.138  0.105 -0.107  0.014 -0.019 -0.072  0.028  0.090  0.070 -0.002   0.094  -0.028   0.242   0.284  -0.144   0.050   0.182   0.107   0.215   0.014  -0.082   0.077   0.010  -0.169  -0.093  -0.360  -0.074   0.094  -0.156   0.110  -0.138   0.124   0.101  -0.094   0.252  -0.120   0.127  -0.092  -0.186  -0.014   0.004  -0.075  -0.111   0.093   0.026  -0.164   0.242  -0.158  -0.277\n",
       "14 -0.081 -0.126 -0.097 -0.084 -0.074  0.058  0.016 -0.014  0.041 -0.079  -0.055   0.038   0.239   0.139  -0.201   0.111   0.149   0.022   0.204   0.031  -0.226   0.166  -0.043  -0.089  -0.011  -0.279  -0.147  -0.018  -0.145   0.030  -0.134   0.093   0.096  -0.143   0.342  -0.010   0.115   0.003  -0.079  -0.066   0.020   0.007  -0.130   0.090  -0.042  -0.134   0.195  -0.180  -0.040\n",
       "15 -0.032 -0.155 -0.175  0.044 -0.107 -0.050  0.044  0.001  0.095 -0.176   0.088   0.063   0.206  -0.026  -0.102   0.116   0.064   0.053   0.190   0.033  -0.084   0.108  -0.108  -0.081  -0.077  -0.204  -0.130  -0.054  -0.093   0.150  -0.063   0.177   0.063  -0.128   0.442  -0.093   0.032  -0.105  -0.080  -0.099   0.049   0.010  -0.200   0.131   0.175  -0.253   0.267  -0.210  -0.148\n",
       "16  0.016 -0.032 -0.110  0.010  0.093  0.097  0.171  0.059  0.090 -0.220  -0.107   0.129   0.317   0.211  -0.014   0.125   0.046   0.144   0.353   0.032  -0.110   0.121  -0.161  -0.087  -0.102  -0.341  -0.077  -0.056   0.028   0.011  -0.074   0.202  -0.002  -0.108   0.243   0.047   0.080  -0.047  -0.088  -0.038   0.082  -0.074  -0.140   0.092   0.192  -0.097   0.182  -0.108  -0.080\n",
       "17 -0.119  0.082 -0.057 -0.097  0.053  0.003  0.041  0.010  0.070  0.055  -0.049   0.065   0.303   0.214  -0.080   0.060   0.172   0.106   0.183   0.076  -0.121   0.134  -0.116  -0.087  -0.041  -0.191  -0.234  -0.117  -0.217   0.107  -0.099   0.188   0.136  -0.108   0.339   0.065   0.046  -0.203   0.001  -0.046  -0.002   0.038  -0.086   0.124   0.029  -0.209   0.186  -0.080  -0.167\n",
       "18  0.096  0.002  0.042  0.011 -0.050 -0.057  0.071  0.111  0.076 -0.108  -0.013   0.041   0.148   0.272  -0.142   0.166   0.226   0.120   0.333   0.098  -0.182   0.151  -0.111  -0.059  -0.062  -0.242  -0.141  -0.036  -0.204   0.058  -0.119   0.196   0.110  -0.062   0.374  -0.187   0.038  -0.025  -0.056   0.042   0.022   0.002  -0.082   0.045   0.070  -0.097   0.191  -0.078  -0.070\n",
       "19  0.008 -0.119 -0.050 -0.066 -0.011  0.044  0.042  0.034  0.105  0.137  -0.089   0.192   0.267   0.111  -0.243   0.039   0.117   0.072   0.252   0.019  -0.160   0.142  -0.048  -0.060  -0.056  -0.233  -0.162  -0.013  -0.059   0.009  -0.127   0.175   0.017  -0.116   0.319  -0.023   0.092   0.022  -0.150  -0.110   0.050  -0.022  -0.168   0.095  -0.016  -0.127   0.260  -0.153  -0.090\n",
       "20  0.001  0.036 -0.025 -0.022 -0.047 -0.057 -0.010  0.159  0.092 -0.193   0.008   0.084   0.261   0.211  -0.212   0.015   0.197   0.059   0.250   0.030  -0.137   0.111  -0.045  -0.132   0.001  -0.273  -0.057   0.069  -0.162   0.110  -0.122   0.160   0.126  -0.183   0.228  -0.105   0.157  -0.037  -0.052  -0.023   0.046  -0.048  -0.043   0.037   0.043  -0.245   0.287  -0.074  -0.148\n",
       "21 -0.125 -0.243 -0.159  0.063  0.052  0.070  0.102  0.100  0.055 -0.140  -0.107   0.112   0.149   0.033  -0.082  -0.057   0.194   0.054   0.196  -0.016  -0.051   0.081  -0.074  -0.102  -0.130  -0.329  -0.056  -0.026  -0.115  -0.003  -0.105   0.250   0.069  -0.147   0.336   0.009   0.121  -0.062  -0.230  -0.066   0.049  -0.014  -0.133   0.154  -0.073  -0.171   0.287   0.007  -0.158\n",
       "22  0.207 -0.077  0.050 -0.103  0.072  0.256  0.049 -0.070  0.042 -0.147  -0.120   0.148   0.343   0.253   0.022   0.128   0.073   0.216   0.452   0.077  -0.033   0.117  -0.261  -0.141  -0.087  -0.282  -0.111  -0.109   0.030   0.141   0.064   0.168   0.105  -0.095   0.212  -0.085   0.052  -0.119   0.024  -0.024   0.096  -0.105  -0.219   0.162   0.376  -0.171   0.183   0.029   0.038\n",
       "23  0.001  0.036 -0.025 -0.022 -0.047 -0.057 -0.010  0.159  0.092 -0.193   0.008   0.084   0.261   0.211  -0.212   0.015   0.197   0.059   0.250   0.030  -0.137   0.111  -0.045  -0.132   0.001  -0.273  -0.057   0.069  -0.162   0.110  -0.122   0.160   0.126  -0.183   0.228  -0.105   0.157  -0.037  -0.052  -0.023   0.046  -0.048  -0.043   0.037   0.043  -0.245   0.287  -0.074  -0.148\n",
       "24  0.138  0.105 -0.107  0.014 -0.019 -0.072  0.028  0.090  0.070 -0.002   0.094  -0.028   0.242   0.284  -0.144   0.050   0.182   0.107   0.215   0.014  -0.082   0.077   0.010  -0.169  -0.093  -0.360  -0.074   0.094  -0.156   0.110  -0.138   0.124   0.101  -0.094   0.252  -0.120   0.127  -0.092  -0.186  -0.014   0.004  -0.075  -0.111   0.093   0.026  -0.164   0.242  -0.158  -0.277\n",
       "25 -0.125 -0.243 -0.159  0.063  0.052  0.070  0.102  0.100  0.055 -0.140  -0.107   0.112   0.149   0.033  -0.082  -0.057   0.194   0.054   0.196  -0.016  -0.051   0.081  -0.074  -0.102  -0.130  -0.329  -0.056  -0.026  -0.115  -0.003  -0.105   0.250   0.069  -0.147   0.336   0.009   0.121  -0.062  -0.230  -0.066   0.049  -0.014  -0.133   0.154  -0.073  -0.171   0.287   0.007  -0.158\n",
       "26 -0.048 -0.148 -0.175  0.075 -0.050  0.068  0.060  0.031  0.085 -0.063  -0.091   0.085   0.220   0.139  -0.147   0.012   0.203   0.099   0.182   0.060  -0.148   0.243  -0.033  -0.120  -0.070  -0.232  -0.136  -0.128  -0.167   0.044  -0.079   0.214   0.070  -0.109   0.323  -0.004  -0.026  -0.105  -0.129   0.013   0.069  -0.155  -0.091   0.132   0.042  -0.087   0.148  -0.125  -0.077\n",
       "27 -0.157 -0.057 -0.118  0.153 -0.099  0.079  0.054 -0.016  0.073  0.048  -0.059   0.079   0.258   0.160  -0.130   0.094   0.057   0.098   0.193   0.023  -0.145   0.133  -0.051  -0.142  -0.134  -0.265  -0.149  -0.021  -0.062   0.073  -0.101   0.148   0.062  -0.120   0.338  -0.077   0.092  -0.061  -0.166  -0.065   0.044  -0.061  -0.168   0.089   0.040  -0.125   0.180  -0.129  -0.185\n",
       "28  0.008 -0.119 -0.050 -0.066 -0.011  0.044  0.042  0.034  0.105  0.137  -0.089   0.192   0.267   0.111  -0.243   0.039   0.117   0.072   0.252   0.019  -0.160   0.142  -0.048  -0.060  -0.056  -0.233  -0.162  -0.013  -0.059   0.009  -0.127   0.175   0.017  -0.116   0.319  -0.023   0.092   0.022  -0.150  -0.110   0.050  -0.022  -0.168   0.095  -0.016  -0.127   0.260  -0.153  -0.090\n",
       "29 -0.081 -0.126 -0.097 -0.084 -0.074  0.058  0.016 -0.014  0.041 -0.079  -0.055   0.038   0.239   0.139  -0.201   0.111   0.149   0.022   0.204   0.031  -0.226   0.166  -0.043  -0.089  -0.011  -0.279  -0.147  -0.018  -0.145   0.030  -0.134   0.093   0.096  -0.143   0.342  -0.010   0.115   0.003  -0.079  -0.066   0.020   0.007  -0.130   0.090  -0.042  -0.134   0.195  -0.180  -0.040\n",
       "30 -0.119  0.082 -0.057 -0.097  0.053  0.003  0.041  0.010  0.070  0.055  -0.049   0.065   0.303   0.214  -0.080   0.060   0.172   0.106   0.183   0.076  -0.121   0.134  -0.116  -0.087  -0.041  -0.191  -0.234  -0.117  -0.217   0.107  -0.099   0.188   0.136  -0.108   0.339   0.065   0.046  -0.203   0.001  -0.046  -0.002   0.038  -0.086   0.124   0.029  -0.209   0.186  -0.080  -0.167\n",
       "31  0.045 -0.003 -0.142 -0.197  0.044  0.003  0.055  0.106  0.024  0.053  -0.107   0.133   0.301   0.081  -0.201   0.029   0.162   0.042   0.232   0.012  -0.172   0.210  -0.024  -0.109   0.080  -0.216  -0.054  -0.044  -0.162   0.061  -0.120   0.212   0.061  -0.224   0.274   0.155   0.008  -0.171  -0.042  -0.028   0.060  -0.107  -0.037   0.138   0.117  -0.202   0.167  -0.258  -0.132\n",
       "32 -0.125 -0.243 -0.159  0.063  0.052  0.070  0.102  0.100  0.055 -0.140  -0.107   0.112   0.149   0.033  -0.082  -0.057   0.194   0.054   0.196  -0.016  -0.051   0.081  -0.074  -0.102  -0.130  -0.329  -0.056  -0.026  -0.115  -0.003  -0.105   0.250   0.069  -0.147   0.336   0.009   0.121  -0.062  -0.230  -0.066   0.049  -0.014  -0.133   0.154  -0.073  -0.171   0.287   0.007  -0.158\n",
       "33  0.158 -0.027 -0.093  0.034 -0.141  0.083  0.008  0.053  0.136  0.091   0.049   0.176   0.263   0.302  -0.163   0.067   0.163  -0.022   0.217   0.088  -0.215   0.226  -0.085  -0.120   0.050  -0.287  -0.215  -0.116  -0.078   0.091  -0.162   0.170   0.004  -0.051   0.284  -0.004   0.073  -0.137  -0.053  -0.033   0.024  -0.070  -0.027   0.189   0.008  -0.119   0.164  -0.103   0.020\n",
       "34  0.158 -0.027 -0.093  0.034 -0.141  0.083  0.008  0.053  0.136  0.091   0.049   0.176   0.263   0.302  -0.163   0.067   0.163  -0.022   0.217   0.088  -0.215   0.226  -0.085  -0.120   0.050  -0.287  -0.215  -0.116  -0.078   0.091  -0.162   0.170   0.004  -0.051   0.284  -0.004   0.073  -0.137  -0.053  -0.033   0.024  -0.070  -0.027   0.189   0.008  -0.119   0.164  -0.103   0.020\n",
       "35 -0.005 -0.012  0.030 -0.028 -0.140 -0.018  0.165  0.060  0.138 -0.108  -0.121   0.240   0.296   0.206  -0.100  -0.038   0.246   0.154   0.183  -0.015   0.019   0.170  -0.025  -0.105  -0.025  -0.186  -0.039  -0.112  -0.116  -0.039  -0.166   0.163   0.173  -0.101   0.401   0.088   0.003  -0.113  -0.149  -0.106  -0.039   0.033  -0.258   0.190  -0.003  -0.098   0.100  -0.113  -0.032\n",
       "36 -0.022 -0.032 -0.102 -0.005 -0.053  0.113 -0.075  0.083  0.216 -0.077  -0.066   0.012   0.343   0.073  -0.220   0.042   0.130   0.103   0.258   0.024  -0.157   0.215   0.026  -0.072  -0.044  -0.275  -0.057  -0.043  -0.135   0.053  -0.170   0.101   0.034  -0.126   0.376  -0.022   0.069  -0.051  -0.187  -0.031   0.003  -0.018  -0.112   0.096  -0.023  -0.110   0.093  -0.197  -0.076\n",
       "37 -0.119  0.082 -0.057 -0.097  0.053  0.003  0.041  0.010  0.070  0.055  -0.049   0.065   0.303   0.214  -0.080   0.060   0.172   0.106   0.183   0.076  -0.121   0.134  -0.116  -0.087  -0.041  -0.191  -0.234  -0.117  -0.217   0.107  -0.099   0.188   0.136  -0.108   0.339   0.065   0.046  -0.203   0.001  -0.046  -0.002   0.038  -0.086   0.124   0.029  -0.209   0.186  -0.080  -0.167\n",
       "38 -0.125 -0.243 -0.159  0.063  0.052  0.070  0.102  0.100  0.055 -0.140  -0.107   0.112   0.149   0.033  -0.082  -0.057   0.194   0.054   0.196  -0.016  -0.051   0.081  -0.074  -0.102  -0.130  -0.329  -0.056  -0.026  -0.115  -0.003  -0.105   0.250   0.069  -0.147   0.336   0.009   0.121  -0.062  -0.230  -0.066   0.049  -0.014  -0.133   0.154  -0.073  -0.171   0.287   0.007  -0.158\n",
       "39 -0.043  0.048 -0.129  0.076 -0.074  0.055  0.030  0.071 -0.040 -0.049  -0.200   0.085   0.100   0.166  -0.247   0.130   0.094   0.058   0.298   0.005  -0.253   0.227  -0.035  -0.147  -0.082  -0.317  -0.025  -0.046  -0.030   0.103  -0.133   0.252  -0.001  -0.142   0.293  -0.012   0.086  -0.136  -0.093   0.075   0.106  -0.052   0.013   0.099   0.094  -0.234   0.172  -0.092  -0.030\n",
       "40  0.008 -0.119 -0.050 -0.066 -0.011  0.044  0.042  0.034  0.105  0.137  -0.089   0.192   0.267   0.111  -0.243   0.039   0.117   0.072   0.252   0.019  -0.160   0.142  -0.048  -0.060  -0.056  -0.233  -0.162  -0.013  -0.059   0.009  -0.127   0.175   0.017  -0.116   0.319  -0.023   0.092   0.022  -0.150  -0.110   0.050  -0.022  -0.168   0.095  -0.016  -0.127   0.260  -0.153  -0.090\n",
       "41 -0.125 -0.243 -0.159  0.063  0.052  0.070  0.102  0.100  0.055 -0.140  -0.107   0.112   0.149   0.033  -0.082  -0.057   0.194   0.054   0.196  -0.016  -0.051   0.081  -0.074  -0.102  -0.130  -0.329  -0.056  -0.026  -0.115  -0.003  -0.105   0.250   0.069  -0.147   0.336   0.009   0.121  -0.062  -0.230  -0.066   0.049  -0.014  -0.133   0.154  -0.073  -0.171   0.287   0.007  -0.158\n",
       "42 -0.022 -0.032 -0.102 -0.005 -0.053  0.113 -0.075  0.083  0.216 -0.077  -0.066   0.012   0.343   0.073  -0.220   0.042   0.130   0.103   0.258   0.024  -0.157   0.215   0.026  -0.072  -0.044  -0.275  -0.057  -0.043  -0.135   0.053  -0.170   0.101   0.034  -0.126   0.376  -0.022   0.069  -0.051  -0.187  -0.031   0.003  -0.018  -0.112   0.096  -0.023  -0.110   0.093  -0.197  -0.076\n",
       "43  0.001  0.036 -0.025 -0.022 -0.047 -0.057 -0.010  0.159  0.092 -0.193   0.008   0.084   0.261   0.211  -0.212   0.015   0.197   0.059   0.250   0.030  -0.137   0.111  -0.045  -0.132   0.001  -0.273  -0.057   0.069  -0.162   0.110  -0.122   0.160   0.126  -0.183   0.228  -0.105   0.157  -0.037  -0.052  -0.023   0.046  -0.048  -0.043   0.037   0.043  -0.245   0.287  -0.074  -0.148\n",
       "44 -0.081 -0.126 -0.097 -0.084 -0.074  0.058  0.016 -0.014  0.041 -0.079  -0.055   0.038   0.239   0.139  -0.201   0.111   0.149   0.022   0.204   0.031  -0.226   0.166  -0.043  -0.089  -0.011  -0.279  -0.147  -0.018  -0.145   0.030  -0.134   0.093   0.096  -0.143   0.342  -0.010   0.115   0.003  -0.079  -0.066   0.020   0.007  -0.130   0.090  -0.042  -0.134   0.195  -0.180  -0.040\n",
       "45  0.045 -0.003 -0.142 -0.197  0.044  0.003  0.055  0.106  0.024  0.053  -0.107   0.133   0.301   0.081  -0.201   0.029   0.162   0.042   0.232   0.012  -0.172   0.210  -0.024  -0.109   0.080  -0.216  -0.054  -0.044  -0.162   0.061  -0.120   0.212   0.061  -0.224   0.274   0.155   0.008  -0.171  -0.042  -0.028   0.060  -0.107  -0.037   0.138   0.117  -0.202   0.167  -0.258  -0.132\n",
       "46  0.008 -0.119 -0.050 -0.066 -0.011  0.044  0.042  0.034  0.105  0.137  -0.089   0.192   0.267   0.111  -0.243   0.039   0.117   0.072   0.252   0.019  -0.160   0.142  -0.048  -0.060  -0.056  -0.233  -0.162  -0.013  -0.059   0.009  -0.127   0.175   0.017  -0.116   0.319  -0.023   0.092   0.022  -0.150  -0.110   0.050  -0.022  -0.168   0.095  -0.016  -0.127   0.260  -0.153  -0.090\n",
       "47 -0.081 -0.126 -0.097 -0.084 -0.074  0.058  0.016 -0.014  0.041 -0.079  -0.055   0.038   0.239   0.139  -0.201   0.111   0.149   0.022   0.204   0.031  -0.226   0.166  -0.043  -0.089  -0.011  -0.279  -0.147  -0.018  -0.145   0.030  -0.134   0.093   0.096  -0.143   0.342  -0.010   0.115   0.003  -0.079  -0.066   0.020   0.007  -0.130   0.090  -0.042  -0.134   0.195  -0.180  -0.040\n",
       "48 -0.157 -0.057 -0.118  0.153 -0.099  0.079  0.054 -0.016  0.073  0.048  -0.059   0.079   0.258   0.160  -0.130   0.094   0.057   0.098   0.193   0.023  -0.145   0.133  -0.051  -0.142  -0.134  -0.265  -0.149  -0.021  -0.062   0.073  -0.101   0.148   0.062  -0.120   0.338  -0.077   0.092  -0.061  -0.166  -0.065   0.044  -0.061  -0.168   0.089   0.040  -0.125   0.180  -0.129  -0.185\n",
       "49  0.008 -0.119 -0.050 -0.066 -0.011  0.044  0.042  0.034  0.105  0.137  -0.089   0.192   0.267   0.111  -0.243   0.039   0.117   0.072   0.252   0.019  -0.160   0.142  -0.048  -0.060  -0.056  -0.233  -0.162  -0.013  -0.059   0.009  -0.127   0.175   0.017  -0.116   0.319  -0.023   0.092   0.022  -0.150  -0.110   0.050  -0.022  -0.168   0.095  -0.016  -0.127   0.260  -0.153  -0.090"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "2019-09-24 21:29:23: In: run_kmeans \n",
       "\n",
       "\n",
       "Begin "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running with number of clusters = 10\n",
      "in run_kmeans, parameters = {'n_clusters': 10}\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'<' not supported between instances of 'int' and 'dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1224-72cab1959bf4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain_nlp_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-1223-9122add57b10>\u001b[0m in \u001b[0;36mmain_nlp_new\u001b[0;34m(entry_point)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m     \u001b[0mdf_w2v\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_book_blocks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mnum_features\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m     \u001b[0mcluster_w2v\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_kmeans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_w2v\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"n_clusters\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Now we are getting the cluster from w2v...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1192-5b6e65c6cce5>\u001b[0m in \u001b[0;36mrun_kmeans\u001b[0;34m(data, parameters)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m#     pred = KMeans(n_clusters=K, random_state=42).fit_predict(data)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;31m#     Z = pd.DataFrame()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m#     Z = merge_predict_and_cluster(data, target, pred) # let's merge the data dataframe, prediction, and the cluster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/cluster/k_means_.py\u001b[0m in \u001b[0;36mfit_predict\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    993\u001b[0m             \u001b[0mIndex\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mcluster\u001b[0m \u001b[0meach\u001b[0m \u001b[0msample\u001b[0m \u001b[0mbelongs\u001b[0m \u001b[0mto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    994\u001b[0m         \"\"\"\n\u001b[0;32m--> 995\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    996\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    997\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/cluster/k_means_.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    967\u001b[0m                 \u001b[0mtol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy_x\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    968\u001b[0m                 \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malgorithm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malgorithm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 969\u001b[0;31m                 return_n_iter=True)\n\u001b[0m\u001b[1;32m    970\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    971\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/cluster/k_means_.py\u001b[0m in \u001b[0;36mk_means\u001b[0;34m(X, n_clusters, sample_weight, init, precompute_distances, n_init, max_iter, verbose, tol, random_state, copy_x, n_jobs, algorithm, return_n_iter)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     order=order, copy=copy_x)\n\u001b[1;32m    310\u001b[0m     \u001b[0;31m# verify that the number of samples given is larger than k\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 311\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mn_clusters\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m         raise ValueError(\"n_samples=%d should be >= n_clusters=%d\" % (\n\u001b[1;32m    313\u001b[0m             _num_samples(X), n_clusters))\n",
      "\u001b[0;31mTypeError\u001b[0m: '<' not supported between instances of 'int' and 'dict'"
     ]
    }
   ],
   "source": [
    "main_nlp_new(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_book_blocks.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Summary of result runs\n",
    "\n",
    "|Model|NLP Feature Generator|Training Score|:Test Score|Metrics Accuracy|Accuracy of Model|Status|\n",
    "|:----|:--------------------|:-------------|:----------|:---------------|:----------------|:----|\n",
    "|Random Forests|Word Counts|98.37|86.59|86.59|82.05| |\n",
    "|Multinomial Naive Bayes|Word Counts|97.55|92.28|92.28|88.58| |\n",
    "|Bernoulli Naive Bayes|Word Counts|82.34|83.33|83.33|76.09| |\n",
    "|Random Forests|tfidf|99.76|98.76|98.76|99.11|**Winner** |\n",
    "|Multinomial Naive Bayes|tfidf|99.11|98.76|98.76|99.11|**Winner** |\n",
    "|Bernoulli Naive Bayes|tfidf|99.05|98.76|98.76|99.11|**Winner**|"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
