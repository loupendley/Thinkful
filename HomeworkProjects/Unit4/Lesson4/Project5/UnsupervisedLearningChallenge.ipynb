{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.thinkful.com'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsupervised Learning Challenge: Build you own NLP model  \n",
    "#### For this challenge, you will need to choose a corpus of data from nltk or another source that includes categories you can predict and create an analysis pipeline that includes the following steps:\n",
    "\n",
    "1.  Data cleaning / processing / language parsing  \n",
    "2.  Create features using two different NLP methods: For example, BoW vs tf-idf.  \n",
    "3.  Use the features to fit supervised learning models for each feature set to predict the category outcomes.  \n",
    "4.  Assess your models using cross-validation and determine whether one model performed better.  \n",
    "5.  Pick one of the models and try to increase accuracy by at least 5 percentage points. \n",
    "\n",
    "Write up your report in a Jupyter notebook. Be sure to explicitly justify the choices you make throughout, and submit it below.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions that need answering:\n",
    "\n",
    " 1. What question are you trying to solve (or prove wrong) ?   \n",
    " __No questions to answer for this challenge.__\n",
    " 1. What kind of data do you have? -> describe the source.. \n",
    " __Sentiment analaysis, using bag of words, tfidf, Random Forest, Bernoulli Naive Bayes, and Multinomial Naive Bayes.__\n",
    " 1. Do some EDA, plots\n",
    " 1. What's missing from the data and how do you deal with it?  \n",
    " __No missing data here.__\n",
    " 1. How can you add, change, or remove features to get more out of your data?  \n",
    " __No added features here; it's sentiment analysis.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Report Template\n",
    "\n",
    "#### \n",
    "\n",
    "\n",
    "#### Key Learning. \n",
    "You cannot always improve on a model for even 5 percentage points.  Especially when using bag of words which has no configurable parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "max_iterations         = 10            # set it to > 0 for determining the features inportance\n",
    "random_state           = 57\n",
    "rows_in_training_set   = 10000\n",
    "rows_in_test_set       = 200000\n",
    "test_size              = 0.10\n",
    "train_size             = 0.90\n",
    "rfc_test_size          = 50000\n",
    "rfc_train_size         = 5000\n",
    "sample_size            = 10\n",
    "run_CountVectorizer    = False\n",
    "run_TfidfVectorizer    = True\n",
    "BegTimeStampNewlines   = 3\n",
    "EndTimeStampNewlines   = 3\n",
    "EndTimeStamp           = '\\n'*EndTimeStampNewlines+'End'\n",
    "BegTimeStamp           = 'End'+'\\n'*BegTimeStampNewlines\n",
    "SustainerSTDDEVLimit   = 0.020\n",
    "\n",
    "num_clusters = 3\n",
    "target_column = 'yyyy'\n",
    "xcolumnname = 'xxxx'\n",
    "CrossValidations = 5 # We are using 5 cross validations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Controls for running sentiment_analyzer\n",
    "flag_to_run_rf = False\n",
    "flag_to_plot_them = False\n",
    "flag_to_run_correlation_matrix = False\n",
    "flag_to_run_features_importance = False\n",
    "flag_to_run_gradient_boosting  = False\n",
    "flag_to_run_linear_regression  = False\n",
    "flag_to_run_logistic_regression = False\n",
    "flag_to_run_lasso_regression = False\n",
    "flag_to_run_ridge_regression = False\n",
    "flag_to_run_svc = False\n",
    "flag_to_run_vectorizer_nb = False\n",
    "flag_to_run_sentiment_analyzer = False\n",
    "flag_to_run_affinity_propagation = False\n",
    "flag_to_run_kmeans = False\n",
    "flag_to_run_mean_shift = False\n",
    "flag_to_run_spectral_clustering = False\n",
    "flag_to_run_elbow_plot = False\n",
    "\n",
    "debug = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to /Users/lou/nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import chardet\n",
    "import datetime\n",
    "from sklearn import datasets, ensemble, metrics, linear_model\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "import time, sys\n",
    "import seaborn as sns\n",
    "from sklearn.svm import SVC\n",
    "import statsmodels.api as sm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import cross_val_predict, cross_val_score, GridSearchCV,cross_val_score, train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import MinMaxScaler, normalize\n",
    "from IPython.display import Markdown, display\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import metrics\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import pairwise_distances, mean_squared_error\n",
    "from sklearn.cluster import AffinityPropagation, KMeans, MeanShift, estimate_bandwidth, SpectralClustering\n",
    "from scipy.spatial.distance import cdist\n",
    "import spacy\n",
    "import re\n",
    "from nltk.corpus import gutenberg, stopwords\n",
    "import nltk\n",
    "from collections import Counter\n",
    "import gensim\n",
    "from gensim.models import word2vec\n",
    "import random\n",
    "\n",
    "nltk.download('gutenberg') # Load the gutenberg nltk works\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regression = False\n"
     ]
    }
   ],
   "source": [
    "# add this to a dictionary\n",
    "# Constants\n",
    "max_iterations         = 10            # set it to > 0 for determining the features inportance\n",
    "random_state           = 57\n",
    "test_size              = 0.10\n",
    "train_size             = 0.90\n",
    "\n",
    "begin_string = '\\n'*3+'Begin'\n",
    "end_string = 'End'+'\\n'*3\n",
    "\n",
    "# Regression/Classification control\n",
    "Regression = False \n",
    "\n",
    "print(\"Regression = {}\".format(Regression))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display preferences.\n",
    "%matplotlib inline\n",
    "pd.options.display.float_format = '{:.3f}'.format\n",
    "pd.set_option('display.max_rows', 1000)\n",
    "pd.set_option('display.max_row', 1000)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_time_to_complete():\n",
    "    objects = ('BernoulliNB', 'MultinomialNB', 'Logistic Regression')\n",
    "    y_pos = np.arange(len(objects))\n",
    "    performance = [18,17,32]\n",
    "\n",
    "    plt.bar(y_pos, performance, align='center', alpha=0.5)\n",
    "    plt.xticks(y_pos, objects)\n",
    "    plt.ylabel('Time in Minutes')\n",
    "    plt.title('Yelp Sentiment Analysis Time to Complete')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def file_stuff():\n",
    "    # Use this for stand-alone file\n",
    "    \n",
    "    path = \"../../../../\"\n",
    "    filename = \"Datafiles/bostonmarathon/results/2013/results.csv\"\n",
    "    print(\"fullfilename = {}\".format(path+filename))\n",
    "    df = pd.read_csv(path+filename)\n",
    "    print(\"There are {} rows in this file.\".format(df.shape[0]))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def text_cleaner(text):\n",
    "    '''\n",
    "    # Utility function for standard text cleaning.\n",
    "    '''\n",
    "    # Visual inspection identifies a form of punctuation spaCy does not\n",
    "    # recognize: the double dash '--'.  Better get rid of it now!\n",
    "    \n",
    "    text = re.sub(r'--',' ',text)\n",
    "    text = re.sub(\"[\\[].*?[\\]]\", \"\", text)\n",
    "    text = ' '.join(text.split())\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_clean_parse_group_gutenberg(gutenberg_file, author, percent_of_file): \n",
    "    \"\"\"\n",
    "    # currently, this function handles:\n",
    "    #    Persuasion, Austen\n",
    "    #    Alice In Wonderland Austen\n",
    "    #    Paradise Lost Milton\n",
    "    #    Moby Dick Melville\n",
    "    \"\"\"\n",
    "\n",
    "    # Load and clean the data.\n",
    "    if gutenberg_file == \"persuasion\":\n",
    "        file_to_load = gutenberg.raw('austen-persuasion.txt')\n",
    "        book = re.sub(r'Chapter \\d+', '', persuasion)\n",
    "    elif gutenberg_file == \"alice\":\n",
    "        file_to_load = gutenberg.raw('carroll-alice.txt')\n",
    "        book = re.sub(r'CHAPTER .*', '', file_to_load)\n",
    "    elif gutenberg_file == \"paradise\":\n",
    "        file_to_load = gutenberg.raw('milton-paradise.txt')\n",
    "        book = re.sub(r'BOOK .*', '', file_to_load)\n",
    "    elif gutenberg_file == \"moby\":\n",
    "        file_to_load = gutenberg.raw('melville-moby_dick.txt')\n",
    "        book = re.sub(r'BOOK .*', '', file_to_load)\n",
    "\n",
    "    book = text_cleaner(book[:int(len(book)/percent_of_file)])\n",
    "    \n",
    "    nlp = spacy.load('en')\n",
    "    book_doc = nlp(book)\n",
    "    \n",
    "    print(\"book_doc datatype is {}\".format(type(book_doc)))\n",
    "    \n",
    "#     book_sents = pd.DataFrame() # I just added this\n",
    "#     book_sents = [[sent, author] for sent in book_doc.sents]\n",
    "    return pd.DataFrame([[sent, author] for sent in book_doc.sents]), book_doc\n",
    "    \n",
    "#     return book_sents, book_doc # previously, I had return pd.DataFrame(book_sents), book_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_gutenberg(book):\n",
    "    '''\n",
    "    # Parse the cleaned novels\n",
    "    '''\n",
    "    \n",
    "    nlp = spacy.load('en')\n",
    "#     book_doc = nlp(book)\n",
    "\n",
    "    return nlp(book)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def group_gutenberg(book_doc, author):\n",
    "    '''\n",
    "    # Group into sentences.\n",
    "    '''\n",
    "    print(\"book_doc datatype is {}\".format(type(book_doc)))\n",
    "    \n",
    "    book_sents = [[sent, author] for sent in book_doc.sents]\n",
    "\n",
    "    # Combine the sentences from the two novels into one data frame.\n",
    "#     sentences = pd.DataFrame(book_sents)\n",
    "    if debug:\n",
    "        sentences.head()\n",
    "        \n",
    "    return pd.DataFrame(book_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_cleanup(df):\n",
    "\n",
    "    # data Cleanup --> last used for the Boston Marathon challenge\n",
    "    \n",
    "#     Left over from the Boston Marathon challenge...\n",
    "  \n",
    "    df['gender_int'] = np.where(df['gender'] == 'M', 1, 0).astype(float)\n",
    "    df['bib_int'] = df['bib'].replace(to_replace=r'[W|F]', value='-', regex=True).astype(int)\n",
    "    kcolumns = ['5k', '10k', '20k', '25k', '30k', '35k', '40k', 'half']\n",
    "    for kcol in kcolumns:\n",
    "        df[kcol] = np.where(df[kcol] == '-', 0, df[kcol])\n",
    "        df[kcol] = df[kcol].astype(float)\n",
    "    df['5kpace']   = df['5k']/5.0\n",
    "    df['10kpace']  = df['10k']/10.0\n",
    "    df['20kpace']  = df['20k']/20.0\n",
    "    df['halfpace'] = df['half']/21.095\n",
    "    df['25kpace']  = df['25k']/25.0\n",
    "    df['30kpace']  = df['30k']/30.0\n",
    "    df['35kpace']  = df['35k']/35.0\n",
    "    df['40kpace']  = df['40k']/40.0\n",
    "    df['officialpace'] = df['official']/42.19\n",
    "    # df['raceavg'] = ,axis=0).mean()\n",
    "    df['racestd'] = df[['5kpace','10kpace','20kpace','halfpace','25kpace','30kpace','35kpace','40kpace','officialpace']].std(axis=1)\n",
    "    df['raceavg'] = df[['5kpace','10kpace','20kpace','halfpace','25kpace','30kpace','35kpace','40kpace','officialpace']].mean(axis=1)\n",
    "#     X = df[['age', 'gender_int','genderdiv', 'country', 'official','racestd','raceavg']]\n",
    "#     X = pd.get_dummies(X)\n",
    "\n",
    "    df.drop('ctz', axis=1, inplace=True)\n",
    "    df.drop('state',axis=1, inplace=True)\n",
    "    # these are the 2% sustainers.  They can be running at any pace, but they are consistent!\n",
    "    df['sustainer'] = np.where(df['racestd'] <= SustainerSTDDEVLimit, 1, 0).astype(float) # they sustained their pace very well for the race\n",
    "    scaler = MinMaxScaler()\n",
    "    \n",
    "    scaler.fit(df[['age']])\n",
    "    df['age_scaled'] = scaler.transform(df[['age']]).astype(float)\n",
    "    \n",
    "    scaler.fit(df[['overall']])\n",
    "    df['overall_scaled'] = scaler.transform(df[['overall']]).astype(float)\n",
    "    \n",
    "    scaler.fit(df[['pace']])\n",
    "    df['pace_scaled'] = scaler.transform(df[['pace']])\n",
    "    \n",
    "    scaler.fit(df[['official']])\n",
    "    df['official_scaled'] = scaler.transform(df[['official']]).astype(float)\n",
    "    \n",
    "    display('columns are now', df.columns)\n",
    "#     df = fcn_MinMaxScaler(df, 'age', 'age_scaled')\n",
    "#     df = fcn_MinMaxScaler(df, 'official', 'official_scaled')\n",
    "    X = df[['age_scaled', 'sustainer', 'gender_int', 'racestd']]\n",
    "#     X = pd.get_dummies(X)\n",
    "   \n",
    "    \n",
    "    display(\"df columns cpt 92310: \", df.columns)\n",
    "    \n",
    "    global target_column, xcolumnname, ycolumnname\n",
    "    \n",
    "#     target_column = 'overall_scaled'\n",
    "#     xcolumnname = 'age_scaled'\n",
    "    ycolumnname = target_column\n",
    "    \n",
    "    y = df[target_column]\n",
    "    printFormatted(\"target, y column is {}\".format(target_column))\n",
    "\n",
    "    if debug == True:\n",
    "        print_timestamp(\"X and y variables created\")\n",
    "        \n",
    "    printFormatted('we have cleaned up the dataframe.')\n",
    "    display_column_names('df values', df)\n",
    "    display_column_names('X values', X)\n",
    "    return df, X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printFormatted(string):\n",
    "    newline = '\\n'\n",
    "    display(Markdown(string))\n",
    "    write_to_logfile(string+newline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fcn_MinMaxScaler(dataframe, orig_column, new_column):\n",
    "    display(\"cp 1: In fcn_MinMaxScaler.  shape is:\", dataframe.shape)\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(dataframe[['{}'.format(orig_column)]])\n",
    "    dataframe[['{}'.format(new_column)]] = scaler.transform(dataframe['{}'.format(orig_column)])\n",
    "    display(\"cp 2: In fcn_MinMaxScaler.  shape is:\", dataframe.shape)\n",
    "    \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_facet():\n",
    "    g = sns.FacetGrid(data=df, col='stars')\n",
    "    g.map(plt.hist, 'message_length', bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_logfile(message, mdformat=''):\n",
    "    bufsize = 0\n",
    "    with open('TestResults.md', 'a+') as the_file:\n",
    "        the_file.write('{} {}'.format(mdformat, message))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_model_accuracy():\n",
    "    objects = ('BernoulliNB', 'MultinomialNB', 'Logistic Regression')\n",
    "    y_pos = np.arange(len(objects))\n",
    "    performance = [75.81,85.98,91.08]\n",
    "\n",
    "    plt.bar(y_pos, performance, align='center', alpha=0.5)\n",
    "    plt.xticks(y_pos, objects)\n",
    "    plt.ylabel('Accuracy Percent')\n",
    "    plt.title('Yelp Sentiment Analysis Accuracy')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_timestamp(displaytext):    \n",
    "    import sys\n",
    "    import datetime\n",
    "    datetime_now = str(datetime.datetime.now())\n",
    "    printFormatted(\"{:19.19}: In: {} {} \".format(datetime_now, sys._getframe(1).f_code.co_name, displaytext))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_current_datetime():\n",
    "    datetime_now = str(datetime.datetime.now())\n",
    "    return datetime_now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_demographics(dataframe, num_rows):\n",
    "\n",
    "    display(\"dataframe.isnull().sum()\", dataframe.isnull().sum())\n",
    "\n",
    "    display(\"dataframe.columns\\n\", dataframe.columns)\n",
    "    display(\"dataframe.head({})\\n\".format(num_rows), dataframe.head(num_rows))\n",
    "\n",
    "    display(\"dataframe.sample({})\\n\".format(num_rows), dataframe.sample(num_rows))\n",
    "    display(\"dataframe.dtypes\\n\", dataframe.dtypes)\n",
    "    display(\"dataframe.describe()\\n\", dataframe.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_them():\n",
    "    for column in X_train.columns:\n",
    "#         plt.hist(X_train[column]*100, bins=40)\n",
    "        plt.scatter(y_train, X_train[column]*100)\n",
    "        plt.xlabel(column)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rfc_and_feature_importances(rf):    # Here we are using Random Forest classifier method to determine the top 30 features.\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, train_size=train_size)\n",
    "    \n",
    "    ## Fit the model on your training data.\n",
    "    rf.fit(X_train, y_train) \n",
    "    \n",
    "    ## And score it on your testing data.\n",
    "    rf.score(X_test, y_test)\n",
    "\n",
    "    feature_importance = rf.feature_importances_\n",
    "\n",
    "    # Make importances relative to max importance.\n",
    "    feature_importance = 100.0 * (feature_importance / feature_importance.max())\n",
    "    sorted_idx = np.argsort(feature_importance)\n",
    "    cols=X.columns[sorted_idx].tolist() \n",
    "    cols=cols[::-1]\n",
    "    pos = np.arange(sorted_idx.shape[0]) + .5\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.barh(pos, feature_importance[sorted_idx], align='center')\n",
    "    plt.yticks(pos, X.columns[sorted_idx])\n",
    "    plt.xlabel('Relative Importance')\n",
    "    plt.title('Variable Importance')\n",
    "    plt.show()\n",
    "#     print(\"We are returning these columns {}\".format(cols))\n",
    "    return cols[:30] # return it sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def run_features_importance(rf,n):\n",
    "# Here we will return the feature importances\n",
    "    all_feature_important_columns = []\n",
    " \n",
    "    for i in range(1,n):\n",
    "        print_timestamp('running rfc iteration {} features importance for {} times'.format(i,n))\n",
    "        columns2 = rfc_and_feature_importances(rf)\n",
    "#         columns2.extend('{}'.format(i))\n",
    "        all_feature_important_columns = all_feature_important_columns + columns2\n",
    "    #     print(\"all_feature_import_columns={}\".format(all_feature_important_columns))\n",
    "\n",
    "    print(\"\\nBOD:\\nall_feature_important_columns = {}\\nEOD\".format(sorted(all_feature_important_columns)))\n",
    "    for feature in set(all_feature_important_columns):\n",
    "        print_timestamp(\"the NOC of feature {} in all_feature_important_columns is {}\".format(feature, all_feature_important_columns.count(feature)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_correlation_matrix():\n",
    "    \n",
    "    print_timestamp('Begin'+'\\n'*3)\n",
    "    \n",
    "    # Setup the correlation matrix.\n",
    "    corrmat = X.corr()\n",
    "    print(corrmat)\n",
    "\n",
    "    # Set up the subplots\n",
    "    f, ax = plt.subplots(figsize=(12, 9))\n",
    "\n",
    "    # Let's draw the heatmap using seaborn.\n",
    "    sns.heatmap(corrmat, vmax=.6, square=True)\n",
    "    plt.show()\n",
    "    \n",
    "    print_timestamp('\\n'*3+'End')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_characteristics():\n",
    "    \n",
    "    printFormatted(\"#### Columns used in the dataset\")\n",
    "    display(df.columns)\n",
    "\n",
    "    print(\"\\n\\n\")\n",
    "    printFormatted(\"#### Describe of the df dataset\")\n",
    "    display(df.describe())\n",
    "\n",
    "    print(\"\\n\\n\")\n",
    "    printFormatted(\"#### Sample of 10 from the dataset\")\n",
    "    display(df.sample(sample_size))\n",
    "\n",
    "    print(\"\\n\\n\")\n",
    "    printFormatted(\"#### Number of nulls in X\")\n",
    "    display(X.isnull().sum())\n",
    "    print(\"\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_test_set(X, y):\n",
    "#     global X_train, X_test, y_train, y_test\n",
    "    # Let's fit it with the RFC training set\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, train_size=train_size, random_state=0)\n",
    "    print(\"train_size = {}, X_train is {}, and y_train is {}\".format(train_size, len(X_train), len(y_train)))\n",
    "    print(\"test_size  = {}, X_test  is {}, and y_test is {}\".format(test_size, len(X_test), len(y_test)))\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_rf(X_train=None, X_test=None, y_train=None, y_test=None, params=None, cross_validate=None):\n",
    "    \n",
    "    rfc = ensemble.RandomForestClassifier(n_estimators=1000)\n",
    "    if params != None:\n",
    "        print(\"In run_rf, params = {}\".format(params)) \n",
    "        rfc.set_params(**params)\n",
    "        \n",
    "    ## Fit the model on your training data.\n",
    "    rfc_fit = rfc.fit(X_train, y_train)  \n",
    "    \n",
    "    #   Let's score it with the test data set    this is new 13-Aug-2019\n",
    "    print_training_and_test_scores(rfc_fit, X_train, X_test, y_train, y_test) # new on 13-Aug-2019\n",
    "    \n",
    "#   Let's produce the metrics scores\n",
    "    print_metrics_score(rfc_fit, X_train, X_test, y_train, y_test) # new on 13-Aug-2019\n",
    "    \n",
    "#   Let's run cross validation \n",
    "    if cross_validate == True:\n",
    "        print_cross_validation_scores(rfc_fit, X_train, X_test, y_train, y_test)\n",
    "        \n",
    "#   Let's run the confusion matrix\n",
    "    if confusion_matrix == True:\n",
    "        confusion_matrix_function(rfc_fit, X_train, X_test, y_train, y_test)\n",
    "    \n",
    "#     ## Let's score it with the training data set\n",
    "#     train_score = rfc.score(X_train, y_train)\n",
    "#     printFormatted(\"### Training score = {:.2%}\".format(train_score))\n",
    "\n",
    "#     ## Let's score it with the test data set\n",
    "#     test_score = rfc.score(X_test, y_test)\n",
    "#     printFormatted(\"### Test score = {:.2%}\".format(test_score))\n",
    "    \n",
    "#     metrics_train_score = metrics.accuracy_score(y_train, y_pred_class2)\n",
    "#     metrics_test_score =  metrics.accuracy_score(y_test, y_pred_class)\n",
    "\n",
    "#     printFormatted('###  Metrics train accuracy score = {:.2%} with {}'.format(metrics_train_score, 'Random Forest Classifier'))\n",
    "#     printFormatted('###  Metrics test accuracy score = {:.2%} with {}'.format(metrics_test_score, 'Random Forest Classifier'))\n",
    "    \n",
    "#     if cross_validate == True:\n",
    "#         accuracy = cross_val_score(rfc, X_train, y_train, scoring='accuracy', cv = 5)\n",
    "#         printFormatted(\"### Cross validation scores:  {}\".format(accuracy))\n",
    "#         printFormatted(\"### Accuracy of Model with Cross Validation average is: {:.2%}\".format(accuracy.mean()))\n",
    "        \n",
    "#     #   Let's produce the metrics scores\n",
    "#     print_metrics_score(mnb_fit, X_train, X_test, y_train, y_test) # new on 13-Aug-2019\n",
    "      \n",
    "    print_timestamp('End run_rfr part 1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_BernoulliNB(X_train=None, X_test=None, y_train=None, y_test=None, params=None, cross_validate=None):\n",
    "    \n",
    "    # Our data is binary / boolean, so we're importing the Bernoulli classifier.\n",
    "\n",
    "    # Instantiate our model and store it in a new variable.\n",
    "    bnb = BernoulliNB()\n",
    "\n",
    "    # Fit our model to the data.\n",
    "    bnb_fit = bnb.fit(X_train, y_train)\n",
    "\n",
    "    # Classify, storing the result in a new variable.\n",
    "#     y_pred = bnb.predict(data)\n",
    "#     y_pred = bnb.predict(X_train)\n",
    "\n",
    "    # Display our results.\n",
    "#     print(\"Number of mislabeled points out of a total {} points : {}\".format(\n",
    "#        X_train.shape[0],\n",
    "#         (y_test != y_pred).sum() \n",
    "#     ))\n",
    "    \n",
    "#   Let's score it with the test data set    this is new 13-Aug-2019\n",
    "    print_training_and_test_scores(bnb_fit, X_train, X_test, y_train, y_test) # new on 13-Aug-2019\n",
    "    \n",
    "#   Let's produce the metrics scores\n",
    "    print_metrics_score(bnb_fit, X_train, X_test, y_train, y_test) # new on 13-Aug-2019\n",
    "    \n",
    "#   Let's run cross validation \n",
    "    if cross_validate == True:\n",
    "        print_cross_validation_scores(bnb_fit, X_train, X_test, y_train, y_test)\n",
    "        \n",
    "#   Let's run the confusion matrix\n",
    "    if confusion_matrix == True:\n",
    "        confusion_matrix_function(bnb_fit, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_analyzer(path, parameters, classifier, tfidf_parms, X_train=None, X_test=None, y_train=None, y_test=None, cross_validate=None):\n",
    "    # path A = the old path\n",
    "    # path B = the new path, no CountVectorizer at all\n",
    "    \n",
    "# run block of code and catch warnings\n",
    "  \n",
    "    if debug == True:\n",
    "        print_timestamp(BegTimeStamp+\" running with path={}\".format(path))\n",
    "    \n",
    "    global vectorized\n",
    "    vectorized = True\n",
    "    \n",
    "    pipeline_array = []\n",
    "   \n",
    "    if path == \"A\":\n",
    "        if classifier == 'bnb':\n",
    "            pipeline_array.append(Pipeline([\n",
    "                ('tfidf', TfidfVectorizer(**tfidf_parms)),\n",
    "                ('clf',   BernoulliNB(**parameters))\n",
    "            ]))\n",
    "        elif classifier == 'svc':\n",
    "            pipeline_array.append(Pipeline([\n",
    "                ('tfidf', TfidfVectorizer(**tfidf_parms)),\n",
    "                ('clf',   SVC(kernel = 'linear', **parameters))\n",
    "            ])) \n",
    "        elif classifier == 'mlb':\n",
    "            pipeline_array.append(Pipeline([\n",
    "                ('tfidf', TfidfVectorizer(**tfidf_parms)),\n",
    "                ('clf',   MultinomialNB(**parameters))\n",
    "            ]))\n",
    "        elif classifier == 'logit':\n",
    "            pipeline_array.append(Pipeline([\n",
    "                ('tfidf', TfidfVectorizer(**tfidf_parms)),\n",
    "                ('clf',   LogisticRegression(**parameters))\n",
    "            ]))\n",
    "        elif classifier == 'rfc':\n",
    "            pipeline_array.append(Pipeline([\n",
    "                ('tfidf', TfidfVectorizer(**tfidf_parms)),\n",
    "                ('clf',   ensemble.RandomForestClassifier(**parameters))\n",
    "            ]))  \n",
    "            \n",
    "    elif path == \"B\":\n",
    "        if classifier == 'bnb':\n",
    "            pipeline_array.append(Pipeline([\n",
    "                ('tfidf', TfidfVectorizer(**tfidf_parms)),\n",
    "                ('clf',   BernoulliNB(**parameters))\n",
    "            ]))\n",
    "        elif classifier == 'svc':\n",
    "            pipeline_array.append(Pipeline([\n",
    "                ('tfidf', TfidfVectorizer(**tfidf_parms)),\n",
    "                ('clf',   SVC(kernel = 'linear', **parameters))\n",
    "            ])) \n",
    "        elif classifier == 'mlb':\n",
    "            pipeline_array.append(Pipeline([\n",
    "                ('tfidf', TfidfVectorizer(**tfidf_parms)),\n",
    "                ('clf',   MultinomialNB(**parameters))\n",
    "            ]))\n",
    "        elif classifier == 'logit':\n",
    "            pipeline_array.append(Pipeline([\n",
    "                ('tfidf', TfidfVectorizer()),\n",
    "                ('clf',   LogisticRegression(**parameters))\n",
    "            ]))\n",
    "        elif classifier == 'rfc':\n",
    "            pipeline_array.append(Pipeline([\n",
    "                ('tfidf', TfidfVectorizer(**tfidf_parms)),\n",
    "                ('clf',   ensemble.RandomForestClassifier(**parameters))\n",
    "            ]))\n",
    "\n",
    "    pipe = pipeline_array[0]\n",
    "    \n",
    "    try:\n",
    "        vect_name_list = str(pipe.named_steps['vect']).split('(')\n",
    "        vect_name = \"vect = {}, \".format(vect_name_list[0])\n",
    "    except:\n",
    "        vect_name = ''\n",
    "\n",
    "    classifier_name_list=str(pipe.named_steps['clf']).split('(')\n",
    "    classifier_name=classifier_name_list[0]\n",
    "    tfidf_name_list = str(pipe.named_steps['tfidf']).split('(')\n",
    "    if len(tfidf_name_list) > 0:\n",
    "        tfidf_name = tfidf_name_list[0]\n",
    "    else:\n",
    "        tfidf_name = ''\n",
    "\n",
    "    printFormatted(\"###  Now running with: {} tfidf={} and clf={} {}\\nparameters={} \\n\\n tfidf_parms={}\".format( vect_name,\n",
    "                                                                                                tfidf_name,\n",
    "                                                                                                classifier_name,\n",
    "                                                                                                return_current_datetime(),\n",
    "                                                                                                parameters,\n",
    "                                                                                                tfidf_parms\n",
    "                                                                                                ))\n",
    "    pipefit = pipe.fit(X_train, y_train)\n",
    "\n",
    "#   Let's score it with the test data set    this is new 13-Aug-2019\n",
    "    print_training_and_test_scores(pipefit, X_train, X_test, y_train, y_test) # new on 13-Aug-2019\n",
    "    \n",
    "#   Let's produce the metrics scores\n",
    "    print_metrics_score(pipefit, X_train, X_test, y_train, y_test) # new on 13-Aug-2019\n",
    "    \n",
    "#   Let's run cross validation \n",
    "    if cross_validate == True:\n",
    "        print_cross_validation_scores(pipefit, X_train, X_test, y_train, y_test)\n",
    "        \n",
    "#   Let's run the confusion matrix\n",
    "    if confusion_matrix == True:\n",
    "        confusion_matrix_function(pipefit, X_train, X_test, y_train, y_test)\n",
    "            \n",
    "    if debug == True:\n",
    "        printFormatted(\"Steps information: {}\".format(pipe.steps))\n",
    "        print_timestamp(\"Finished running pipeline with:\\n{}: \".format(classifier_name))\n",
    "        \n",
    "    print_timestamp(EndTimeStamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_training_and_test_scores(model, X_train, X_test, y_train, y_test):\n",
    "    \n",
    "    ## Let's score it with the test data set    this is new 13-Aug-2019\n",
    "    training_score = model.score(X_train, y_train) \n",
    "    printFormatted(\"### Training score = {:.2%}\".format(training_score))\n",
    "    \n",
    "    ## Let's score it with the test data set  this is new 13-Aug-2019\n",
    "    test_score = model.score(X_test, y_test)\n",
    "    printFormatted(\"### Test score = {:.2%}\".format(test_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metrics_score(fit_model, X_train, X_test, y_train, y_test):\n",
    "    y_pred_class  = fit_model.predict(X_test)\n",
    "    metrics_test_score =  metrics.accuracy_score(y_test, y_pred_class)\n",
    "    printFormatted('###  Metrics test accuracy score = {:.2%}'.format(metrics_test_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_cross_validation_scores(fit_model, X_train, X_test, y_train, y_test):\n",
    "    \n",
    "        accuracy = cross_val_score(fit_model, X_train, y_train, scoring='accuracy', cv = 5)\n",
    "        printFormatted(\"### Cross validation scores:  {}\".format(accuracy))\n",
    "        printFormatted(\"### Accuracy of Model with Cross Validation average is: {:.2%}\".format(accuracy.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_gradient_boosting():\n",
    "\n",
    "    print_timestamp('Begin')\n",
    "    \n",
    "    clf = ensemble.GradientBoostingClassifier(**params)\n",
    "\n",
    "    #Let's run cross validate score with the training data set\n",
    "    cross_val_score(clf, X_train, y_train, cv=5)\n",
    "\n",
    "    loss_function = 'deviance' # could be exponential\n",
    "    depth_value = 8\n",
    "    params = {'n_estimators': 500,\n",
    "              'max_depth': 8,\n",
    "              'loss_function': loss_function,\n",
    "              'max_leaf_nodes': depth_value, # 8 worked best...\n",
    "              'min_samples_leaf': depth_value * 3\n",
    "              ,'random_state' : random_state\n",
    "             }\n",
    "\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    predict_train = clf.predict(X_train)\n",
    "    predict_test = clf.predict(X_test)\n",
    "    \n",
    "    print_timestamp('End')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_svc(X_train=None, X_test=None, y_train=None, y_test=None, cross_validate=None, params=None):\n",
    "\n",
    "    print_timestamp('\\n'*3+'Begin run_svc')\n",
    "    \n",
    "    # Let's do a linear Support Vector Classifier\n",
    "    print_timestamp('Running SVC(kernel=linear')\n",
    "    svm = SVC(kernel = 'linear')\n",
    "    \n",
    "    if params == True:\n",
    "        print(\"In run_rf, params = {}\".format(params)) \n",
    "        svm.set_params(**params)\n",
    "    \n",
    "    # Let's fit the training model\n",
    "    print_timestamp('Running svm.fit')\n",
    "    svm.fit(X_train, y_train)\n",
    "    \n",
    "    # Let's score the training set\n",
    "    print_timestamp('Running svm.score for the training set')\n",
    "    svm_training_score = svm.score(X_train, y_train)\n",
    "    printFormatted(\"###  SVM Training score={:.2%}\".format(svm_training_score))\n",
    "\n",
    "    # Let's score the test set\n",
    "    print_timestamp('Running svm.fit for the test set')\n",
    "    svm_test_score = svm.score(X_test, y_test)\n",
    "    printFormatted(\"###  SVM Test score={:.2%}\".format(svm_test_score))\n",
    "\n",
    "    if cross_validate == True:\n",
    "        accuracy = cross_val_score(svm, X_train, y_train, scoring='accuracy', cv = 5)\n",
    "        printFormatted(\"### Cross validation scores:  {}\".format(accuracy))\n",
    "        printFormatted(\"### Accuracy of Model with Cross Validation average is: {:.2%}\".format(accuracy.mean()))\n",
    "\n",
    "    print_timestamp('\\n'*3+'End run_svc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_logistic_regression():\n",
    "    print_timestamp('\\n'*3+'Begin')\n",
    "\n",
    "    lr = LogisticRegression(C=1e20, solver='lbfgs', max_iter=1000)\n",
    "\n",
    "    print_timestamp('Running lr.fit for the training set')\n",
    "    lr.fit(X_train, y_train)\n",
    "    \n",
    "    print_timestamp('Running lr.fit for the training set')\n",
    "    print('\\nR-squared simple model training set yields:')\n",
    "    print(lr.score(X_train, y_train))\n",
    "    print(\"here comes the test set\")\n",
    "    lrscore = lr.score(X_test, y_test)\n",
    "    printFormatted(\"###  Logistic Regression score={:.2%}\".format(lrscore))\n",
    "    \n",
    "    print_timestamp('\\n'*3+'End')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_linear_regression():\n",
    "\n",
    "    print_timestamp('\\n'*3+'Begin')\n",
    "\n",
    "    regr = linear_model.LinearRegression()\n",
    "\n",
    "    print_timestamp('Running regr.fit for the training set')\n",
    "    regr.fit(X_train, y_train)\n",
    "    \n",
    "    print(\"\\nCoeffecients: \\n\", regr.coef_)\n",
    "    print(\"\\nIntercept: \\n\", regr.intercept_)\n",
    "    print(\"\\nR-squared for training data set:\")\n",
    "    print(regr.score(X_train, y_train))\n",
    "    \n",
    "    print(\"\\nR-squared for test data set:\")\n",
    "    print(regr.score(X_test, y_test))\n",
    "    \n",
    "    print_timestamp('End run_linear_regression.\\n\\n')\n",
    "    \n",
    "    print_timestamp('\\n'*3+'End')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def run_ridge_regression():\n",
    "    # Fitting a ridge regression model. Alpha is the regularization\n",
    "    # parameter (usually called lambda). As alpha gets larger, parameter\n",
    "    # shrinkage grows more pronounced. Note that by convention, the\n",
    "    # intercept is not regularized. Since we standardized the data\n",
    "    # earlier, the intercept should be equal to zero and can be dropped.\n",
    "    print_timestamp('\\n'*3+'Begin')\n",
    "    \n",
    "    ridgeregr = linear_model.Ridge(alpha=10, fit_intercept=False) \n",
    "    ridgeregr.fit(X_train, y_train)\n",
    "    print(ridgeregr.score(X_train, y_train))\n",
    "\n",
    "    print_timestamp('\\n'*3+'End')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_affinity_propagation(data, target):\n",
    "      \n",
    "    print_timestamp('\\n'*3+'starting AffinityPropagation')\n",
    "\n",
    "    print_timestamp('\\n'*3+'Begin')\n",
    "    \n",
    "    ap = AffinityPropagation()\n",
    "#     ap = AffinityPropagation(damping=0.5,\n",
    "#                          max_iter=200,\n",
    "#                          convergence_iter=15,\n",
    "#                          copy=True,\n",
    "#                          preference=None,\n",
    "#                          affinity='euclidean',\n",
    "#                          verbose=False) \n",
    "\n",
    "    model = ap.fit(data)\n",
    "    pred = ap.predict(data)\n",
    "\n",
    "    Z = merge_predict_and_cluster(data, target, pred) # let's merge the data dataframe, prediction, and the cluster\n",
    "    \n",
    "    # Pull the number of clusters and cluster assignments for each data point.\n",
    "    cluster_centers_indices = ap.cluster_centers_indices_\n",
    "    n_clusters_ = len(cluster_centers_indices)\n",
    "    labels = ap.labels_\n",
    "    \n",
    "    print('Estimated number of clusters: {}'.format(n_clusters_))\n",
    "\n",
    "    labels = model.labels_\n",
    "    \n",
    "    print(\"from run_affinity_propagation {}\".format(metrics.silhouette_score(data, labels, metric='euclidean')))\n",
    "    \n",
    "    print_timestamp('\\n'*3+'finished with AffinityPropagation')\n",
    "    \n",
    "    return Z, n_clusters_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def run_kmeans(data, target, K):\n",
    "\n",
    "    print_timestamp('\\n'*3+'Begin')\n",
    "    print(\"running with number of clusters = {}\".format(K))\n",
    "    km = KMeans(n_clusters=K, random_state=42)\n",
    "\n",
    "#     pred = KMeans(n_clusters=K, random_state=42).fit_predict(data)\n",
    "    pred = km.fit_predict(data)\n",
    "#     Z = pd.DataFrame()\n",
    "    Z = merge_predict_and_cluster(data, target, pred) # let's merge the data dataframe, prediction, and the cluster\n",
    "#     Z = pd.merge(data, pd.DataFrame(pred), left_index=True, right_index=True)\n",
    "#     display_column_names('first Z values', Z)\n",
    "#     Z.rename(columns={Z.columns[-1]: 'cluster'}, inplace=True)\n",
    "#     display_column_names('second Z values', Z)\n",
    "#     Z = pd.merge(Z, target, left_index=True, right_index=True)\n",
    "#     display_column_names('third Z values', Z)\n",
    "#     print(\"z columns are {}\".format(Z.columns))\n",
    "\n",
    "    if debug == True:  \n",
    "        print(\"the shape of Kmeans_pred is {}, and the shape of X is {}, and the shape of Z is {}\".format(pred.shape,\n",
    "                                                                                                      data.shape,\n",
    "                                                                                                      Z.shape))\n",
    "        display(Z.head(100))\n",
    "        display_column_names('Z below values', Z)\n",
    "\n",
    "        count = Z.groupby(['cluster']).count() \n",
    "        display(\"Z: Count by clusters are this:\\n\", count) \n",
    "  \n",
    "    return Z\n",
    "        \n",
    "    print_timestamp('\\n'*3+'End')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_predict_and_cluster(dataframe, target, predict):\n",
    "    Z = pd.merge(dataframe, target, left_index=True, right_index=True)\n",
    "    Z = pd.merge(Z, pd.DataFrame(predict), left_index=True, right_index=True)\n",
    "    Z.rename(columns={Z.columns[-1]: 'cluster'}, inplace=True)\n",
    "    \n",
    "    return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix_function(model_fit, X_train, X_test, y_train, y_test):\n",
    "    \n",
    "    y_pred_class  = model_fit.predict(X_test)\n",
    "    \n",
    "    conf_matrix = confusion_matrix_function(y_test, y_pred_class)\n",
    "    printFormatted(\"### Confusion Matrix:  {}\".format(conf_matrixscores))\n",
    "    \n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_spectral_clustering(data, target, K):\n",
    "    display_dataframe_shape('entering run_spectral_clustering, data has shape of:', data)\n",
    "    display_dataframe_shape('entering run_spectral_clustering, target has shape of:', target)\n",
    "    print_timestamp('\\n'*3+'Begin')\n",
    "    \n",
    "#     for clusternum in range(2, K):\n",
    "    print_timestamp(\"Running spectral_clustering with {} clusters.\".format(K))\n",
    "    n_clusters=K\n",
    "\n",
    "    # Declare and fit the model.\n",
    "    sc = SpectralClustering(n_clusters=K)\n",
    "    sc.fit(data)\n",
    "\n",
    "    #Predicted clusters.\n",
    "    predict=sc.fit_predict(data)\n",
    "\n",
    "    Z = merge_predict_and_cluster(data, target, predict) # let's merge the data dataframe, prediction, and the cluster\n",
    "\n",
    "    if debug == True:\n",
    "        display_dataframe_shape('in run_spectral_clustering, Z has shape of:', Z)\n",
    "        display_dataframe_shape('in run_spectral_clustering, target has shape of:', target)\n",
    "        display(\"the datatypes of Z are\", Z.dtypes)\n",
    "\n",
    "#     plt.scatter(Z['cluster'], Z[target_column], c=Z['cluster'])\n",
    "#     plt.show()\n",
    "\n",
    "    labels = sc.labels_\n",
    "    print(\"from spectral clustering {}\".format(metrics.silhouette_score(data, labels, metric='euclidean')))\n",
    "\n",
    "#     print('Comparing the assigned categories to the ones in the data:')\n",
    "#     print(pd.crosstab(target,predict))\n",
    "    \n",
    "    print_timestamp('\\n'*3+'End')\n",
    "    \n",
    "    return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_the_elbow(X):\n",
    "    printFormatted(\"## We are plotting the elbow method!\")\n",
    "    # calculate distortion for a range of number of cluster\n",
    "    distortions = []\n",
    "    for i in range(1, 11):\n",
    "        km = KMeans(\n",
    "            n_clusters=i, init='random',\n",
    "            n_init=10, max_iter=300,\n",
    "            tol=1e-04, random_state=0\n",
    "        )\n",
    "        km.fit(X)\n",
    "        distortions.append(km.inertia_)\n",
    "\n",
    "    # plot\n",
    "    plt.plot(range(1, 11), distortions, marker='o')\n",
    "    plt.xlabel('Number of clusters')\n",
    "    plt.ylabel('Distortion')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_it_clusters(dataframe, xvalue, yvalue, title):\n",
    "    \n",
    "    if debug == True:\n",
    "        display_dataframe_shape('entry received in plot_it_clusters', dataframe)\n",
    "        display(dataframe.dtypes)\n",
    "\n",
    "    data_demographics(dataframe, 10)\n",
    "        \n",
    "    plt.rcParams['figure.figsize'] = [xvalue, yvalue]\n",
    "    plt.xlabel(xcolumnname)\n",
    "    plt.ylabel(ycolumnname)\n",
    "    \n",
    "    df0 = dataframe[dataframe.cluster == 0]\n",
    "    df1 = dataframe[dataframe.cluster == 1]\n",
    "    df2 = dataframe[dataframe.cluster == 2]\n",
    "    df3 = dataframe[dataframe.cluster == 3]\n",
    "    df4 = dataframe[dataframe.cluster == 4]\n",
    "    df5 = dataframe[dataframe.cluster == 5]\n",
    "    \n",
    "    plt.scatter(df0[xcolumnname], df0[ycolumnname], color='green')\n",
    "    plt.scatter(df1[xcolumnname], df1[ycolumnname], color='red')\n",
    "    plt.scatter(df2[xcolumnname], df2[ycolumnname], color='blue')\n",
    "    plt.scatter(df3[xcolumnname], df3[ycolumnname], color='black')\n",
    "    plt.scatter(df4[xcolumnname], df4[ycolumnname], color='magenta')\n",
    "    plt.scatter(df5[xcolumnname], df5[ycolumnname], color='orange')\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "        \n",
    "#     plt.scatter(km.cluster_centers_[:,0],km.cluster_centers_[:,1],color='purple',marker='*',label='centroid')\n",
    "    \n",
    "#     if type == 'KMeans':\n",
    "#         plt.xlabel('Age')\n",
    "#         plt.ylabel('Income ($)')\n",
    "#         plt.legend()\n",
    "#         plt.scatter(km.cluster_centers[:,0], \n",
    "#                     km.cluster_centers[:,1],\n",
    "#                     marker = '*',\n",
    "#                     label = 'centroid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_mean_shift(data, target):\n",
    "    \n",
    "    print_timestamp('\\n'*3+'Begin')  \n",
    "\n",
    "    X_train = data\n",
    "    \n",
    "    # Here we set the bandwidth. This function automatically derives a bandwidth\n",
    "    # number based on an inspection of the distances among points in the data.\n",
    "    bandwidth = estimate_bandwidth(X_train, quantile=0.2, n_samples=500)\n",
    "\n",
    "    # Declare and fit the model.\n",
    "    ms = MeanShift(bandwidth=bandwidth, bin_seeding=True)\n",
    "    if debug == True:\n",
    "        display_dataframe_shape('this is the shape of data coming into run_mean_shift', data)\n",
    "    ms.fit(data)\n",
    "\n",
    "    if debug == True:\n",
    "        display_dataframe_shape('this is the shape of target coming into run_mean_shift', target)\n",
    "    pred = ms.predict(data)\n",
    "    if debug == True:\n",
    "        display_dataframe_shape('this is the shape of pred after predict in run_mean_shift', data)\n",
    "        \n",
    "    Z = merge_predict_and_cluster(data, target, pred) # let's merge the data dataframe, prediction, and the cluster\n",
    "\n",
    "    # Extract cluster assignments for each data point.\n",
    "    labels = ms.labels_\n",
    "\n",
    "    print(\"from mean shift {}\".format(metrics.silhouette_score(data, labels, metric='euclidean')))\n",
    "    \n",
    "    # Coordinates of the cluster centers.\n",
    "    cluster_centers = ms.cluster_centers_\n",
    "\n",
    "    # Count our clusters.\n",
    "    n_clusters_ = len(np.unique(labels))\n",
    "\n",
    "    print(\"Number of estimated clusters: {}\".format(n_clusters_))\n",
    "    \n",
    "    print_timestamp('\\n'*3+'End')\n",
    "    \n",
    "    return Z, n_clusters_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_tfidf_vectorizer(df, max_df=0.5, min_df=2, stop_words='english', lowercase=True, use_idf=True, norm=u'l2', smooth_idf=True):\n",
    "    print_timestamp(\"in run_tfidf_vectorizer: df is a {} datatype.\".format(type(df)))\n",
    "    vectorizer = TfidfVectorizer(\n",
    "                             max_df=max_df, # drop words that occur in more than half the paragraphs\n",
    "                             min_df=min_df, # only use words that appear at least twice\n",
    "                             stop_words=stop_words, \n",
    "                             lowercase=lowercase, #convert everything to lower case (since Alice in Wonderland has the HABIT of CAPITALIZING WORDS for EMPHASIS)\n",
    "                             use_idf=use_idf,#we definitely want to use inverse document frequencies in our weighting\n",
    "                             norm=norm, #Applies a correction factor so that longer paragraphs and shorter paragraphs get treated equally\n",
    "                             smooth_idf=smooth_idf #Adds 1 to all document frequencies, as if an extra document existed that used every word once.  Prevents divide-by-zero errors\n",
    "                            )\n",
    "\n",
    "    #Applying the vectorizer\n",
    "    tfidf_df = vectorizer.fit(df)\n",
    "    \n",
    "    return tfidf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_cleaner(text):\n",
    "    # Visual inspection identifies a form of punctuation spaCy does not\n",
    "    # recognize: the double dash '--'.  Better get rid of it now!\n",
    "    text = re.sub(r'--',' ',text)\n",
    "    text = re.sub(\"[\\[].*?[\\]]\", \"\", text)\n",
    "    text = ' '.join(text.split())\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_of_words(text):\n",
    "    \n",
    "    # Filter out punctuation and stop words.\n",
    "    allwords = [token.lemma_\n",
    "                for token in text\n",
    "                if not token.is_punct\n",
    "                and not token.is_stop]\n",
    "    \n",
    "    # Return the most common words.\n",
    "    return [item[0] for item in Counter(allwords).most_common(2000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bow_features_dev(sentences, common_words):\n",
    "    display(sentences.head(10))\n",
    "    num_sentences_to_print = 10\n",
    "    print(\"inside bow_features: sentences is a {} datatype, of {} length,\\nand common_words is a {} datatype of {} length.\"\n",
    "          .format(type(sentences), sentences.shape[0], type(common_words), len(common_words)))\n",
    "    print(\"here come {} sentences: {}\".format(num_sentences_to_print, sentences[0:num_sentences_to_print]))\n",
    "    # Scaffold the data frame and initialize counts to zero.\n",
    "    df = pd.DataFrame(columns=common_words)\n",
    "    print(\"in bow_features: sentences.iloc[0] = {}\".format(sentences.iloc[0]))\n",
    "    df['text_sentence'] = sentences.iloc[0] # this could be the problem\n",
    "    df['text_source'] = sentences.iloc[1]   # this too could be the problem...\n",
    "    df.loc[:, common_words] = 0\n",
    "    \n",
    "    # Process each row, counting the occurrence of words in each sentence.\n",
    "    num_values = len(list(enumerate(df['text_sentence'])))\n",
    "    print(\"There are {} enumerated items for iterations.\".format(num_values))\n",
    "    for i, sentence in enumerate(df['text_sentence']):\n",
    "        \n",
    "        # Convert the sentence to lemmas, then filter out punctuation,\n",
    "        # stop words, and uncommon words.\n",
    "        words = [token.lemma_\n",
    "                 for token in sentence\n",
    "                 if (\n",
    "                     not token.is_punct\n",
    "                     and not token.is_stop\n",
    "                     and token.lemma_ in common_words\n",
    "                 )]\n",
    "        \n",
    "        # Populate the row with word counts.\n",
    "        for word in words:\n",
    "            df.loc[i, word] += 1\n",
    "        \n",
    "        # This counter is just to make sure the kernel didn't hang.\n",
    "        if i % 50 == 0:\n",
    "            print(\"Processing row {}\".format(i))\n",
    "            \n",
    "    return pd.DataFrame(df)\n",
    "\n",
    "#     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bow_features(sentences, common_words):\n",
    "  \n",
    "    # Scaffold the data frame and initialize counts to zero.\n",
    "    df = pd.DataFrame(columns=common_words)\n",
    "    df['text_sentence'] = sentences[0] # this could be the problem\n",
    "    df['text_source'] = sentences[1]   # this too could be the problem...\n",
    "    df.loc[:, common_words] = 0\n",
    "    \n",
    "    # Process each row, counting the occurrence of words in each sentence.\n",
    "    for i, sentence in enumerate(df['text_sentence']):\n",
    "        \n",
    "        # Convert the sentence to lemmas, then filter out punctuation,\n",
    "        # stop words, and uncommon words.\n",
    "        words = [token.lemma_\n",
    "                 for token in sentence\n",
    "                 if (\n",
    "                     not token.is_punct\n",
    "                     and not token.is_stop\n",
    "                     and token.lemma_ in common_words\n",
    "                 )]\n",
    "        \n",
    "        # Populate the row with word counts.\n",
    "        for word in words:\n",
    "            df.loc[i, word] += 1\n",
    "        \n",
    "        # This counter is just to make sure the kernel didn't hang.\n",
    "        if i % 50 == 0:\n",
    "            print(\"Processing row {}\".format(i))\n",
    "            \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_mnb(X_train=None, X_test=None, y_train=None, y_test=None, cross_validate=None, params=None):\n",
    "\n",
    "    mnb = MultinomialNB()\n",
    "    \n",
    "    if params == True:\n",
    "        print(\"In run_rf, params = {}\".format(params)) \n",
    "        mnb.set_params(**params)\n",
    "        \n",
    "    mnb_fit = mnb.fit(X_train, y_train)\n",
    "        \n",
    "#     training_score = mnb.score(X_train, y_train) \n",
    "#     printFormatted(\"### Training score = {:.2%}\".format(training_score))\n",
    "    \n",
    "#      ## Let's score it with the test data set\n",
    "#     test_score = mnb.score(X_test, y_test)\n",
    "    \n",
    "    #   Let's score it with the test data set    this is new 13-Aug-2019\n",
    "    print_training_and_test_scores(mnb_fit, X_train, X_test, y_train, y_test) # new on 13-Aug-2019\n",
    "    \n",
    "#   Let's produce the metrics scores\n",
    "    print_metrics_score(mnb_fit, X_train, X_test, y_train, y_test) # new on 13-Aug-2019\n",
    "    \n",
    "    if cross_validate == True:\n",
    "        print_cross_validation_scores(mnb_fit, X_train, X_test, y_train, y_test)\n",
    "        \n",
    "    if confusion_matrix == True:\n",
    "        confusion_matrix_function(mnb_fit, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2vec_function(sentences, workers=4, min_count=10, window=6, sg=0, sample=1e-3, size=300, hs=1):\n",
    "# import gensim\n",
    "# from gensim.models import word2vec\n",
    "# original values from the curriculum:\n",
    "# workers = 4, min_count=10, window=6, sg=0, sample=1e-3, size=300, hs=1\n",
    "\n",
    "    model = word2vec.Word2Vec(\n",
    "        sentences,\n",
    "        workers=workers,     # Number of threads to run in parallel (if your computer does parallel processing).\n",
    "        min_count=min_count,  # Minimum word count threshold.\n",
    "        window=window,      # Number of words around target word to consider.\n",
    "        sg=sg,          # Use CBOW because our corpus is small.\n",
    "        sample=sample ,  # Penalize frequent words.\n",
    "        size=size,      # Word vector length.\n",
    "        hs=hs           # Use hierarchical softmax.\n",
    "    )\n",
    "\n",
    "    print('done!')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorizer_nb(type_of_vectorizer):\n",
    "\n",
    "    print_timestamp(BegTimeStamp)\n",
    "    \n",
    "    # 1. import and instantiate CountVectorizer (with the default parameters)\n",
    "\n",
    "    # 2. instantiate CountVectorizer (vectorizer)\n",
    "\n",
    "#     X = df.message\n",
    "#     y = df.sentiment_label\n",
    "\n",
    "    # split X and y into training and testing sets\n",
    "    # by default, it splits 75% training and 25% test\n",
    "    # random_state=1 for reproducibility\n",
    "    \n",
    "#     X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n",
    "\n",
    "    # 3. fit & transform\n",
    "    if type_of_vectorizer == 'Count':\n",
    "        print(\"We are running with CountVectorizer\")\n",
    "        vectorizer = CountVectorizer()\n",
    "        vectorizer.fit(X_train)\n",
    "        vectorizer_method = 'CountVectorizer'\n",
    "    elif type_of_vectorizer == 'Tfidf':\n",
    "        print(\"We are running with TfidfVectorizer\")\n",
    "        vectorizer = TfidfVectorizer()\n",
    "        vectorizer.fit_transform(X_train)\n",
    "        vectorizer_method = 'TfidfVectorizer'\n",
    "    \n",
    "    # 4. transform training data\n",
    "    X_train_dtm = vectorizer.transform(X_train)\n",
    "\n",
    "    # equivalently: combine fit and transform into a single step\n",
    "    # this is faster and what most people would do\n",
    "    X_train_dtm = vectorizer.fit_transform(X_train)\n",
    "\n",
    "    # 4. transform testing data (using fitted vocabulary) into a document-term matrix\n",
    "    X_test_dtm = vectorizer.transform(X_test)\n",
    "\n",
    "    # 1. import\n",
    "\n",
    "    # 2. instantiate a Multinomial Naive Bayes model\n",
    "    nb = MultinomialNB()\n",
    "\n",
    "    # 3. train the model \n",
    "    # using X_train_dtm (timing it with an IPython \"magic command\")\n",
    "\n",
    "    nb.fit(X_train_dtm, y_train)\n",
    "    \n",
    "    \n",
    "    # 4. make class predictions for X_test_dtm\n",
    "    y_pred_class = nb.predict(X_test_dtm)\n",
    "\n",
    "    # calculate accuracy of class predictions\n",
    "\n",
    "    met_test_score = metrics.accuracy_score(y_test, y_pred_class)\n",
    "    printFormatted('###  With {} vectorizer, the metrics accuracy score = {:.2%}'.format(vectorizer_method,\n",
    "                                                                                         met_test_score))\n",
    "    \n",
    "    print_timestamp(EndTimeStamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_column_names(label, df):\n",
    "    display(\"Label: {}: Column names are:\".format(label), df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_datatype(var):\n",
    "    # this function just returns the data type of the variable var\n",
    "    return \"{}\".format(type(var))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_dataframe_shape(label, df):\n",
    "    display(\"Label: {}: Dataframe shape is:\".format(label), df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_it(X_train, X_test, y_train, y_test, y):\n",
    "    \n",
    "#     file_stuff()\n",
    "    \n",
    "#     data_cleanup()\n",
    "    \n",
    "    print_timestamp('\\n'*3+'Begin')\n",
    "    \n",
    "    if Regression == True:\n",
    "        print_timestamp(\"We are running with a Regression model\")\n",
    "    elif Regression == False:\n",
    "        print_timestamp(\"We are running with a Classifier model\")\n",
    "    else:\n",
    "        print_timestamp(\"We have failed to set the Regression variable\")\n",
    "        sys.exit(main())\n",
    "        \n",
    "\n",
    "    if flag_to_plot_them == True:\n",
    "        plot_them()\n",
    "\n",
    "    if flag_to_run_features_importance == True:\n",
    "        \n",
    "        number_of_features_to_consider = 50\n",
    "        params = {'n_estimators': 100}\n",
    "\n",
    "        if Regression == True:\n",
    "            print_timestamp('We are running RandomForestRegressor')\n",
    "            rf = ensemble.RandomForestRegressor(**params)\n",
    "            \n",
    "        else:\n",
    "            print_timestamp('We are running RandomForestClassifier')\n",
    "            rf = ensemble.RandomForestClassifier(**params)\n",
    "\n",
    "        run_features_importance(rf, number_of_features_to_consider)\n",
    "\n",
    "    if flag_to_run_correlation_matrix == True:\n",
    "        run_correlation_matrix()\n",
    "        \n",
    "    if flag_to_run_sentiment_analyzer == True:\n",
    "        path = \"B\"\n",
    "\n",
    "\n",
    "        for path in ['A']:\n",
    "            for vectorizer_iterator in ['logit', 'mlb', 'bnb']:\n",
    "                if vectorizer_iterator == 'rfc':\n",
    "                    sentiment_analyzer(path=path, parameters=params, classifier=vectorizer_iterator, tfidf_parms=tfidf_parms)\n",
    "\n",
    "                elif vectorizer_iterator == 'bnb':\n",
    "                    parameters = {}\n",
    "                    sentiment_analyzer(path=path, parameters=parameters, classifier=vectorizer_iterator, tfidf_parms=tfidf_parms)\n",
    "\n",
    "                elif vectorizer_iterator == 'mlb':\n",
    "                    parameters = {}\n",
    "                    sentiment_analyzer(path=path, parameters=parameters, classifier=vectorizer_iterator, tfidf_parms=tfidf_parms)\n",
    "\n",
    "                elif vectorizer_iterator == 'logit': # newton-cg took too long. sag and saga about the same as lbfgs.\n",
    "                    tfidf_parms = {'max_features' :  10000 } # determined this through iterative testing\n",
    "                    parameters = {'C' :1e20, 'solver': 'lbfgs', 'max_iter': 1000} # max_iter=100 reports warning, try 1000\n",
    "                    sentiment_analyzer(path=path, parameters=parameters, classifier=vectorizer_iterator, tfidf_parms=tfidf_parms)\n",
    "\n",
    "                elif vectorizer_iterator == 'svc':\n",
    "                    parameters = {}\n",
    "                    sentiment_analyzer(path=path, parameters=parameters, classifier=vectorizer_iterator, tfidf_parms=tfidf_parms)\n",
    "                    if confusion_matrix != None:\n",
    "                        confusion_matrix_function(y_test, y_pred_class)\n",
    "\n",
    "    if flag_to_run_rf == True:\n",
    "        #     params = {}\n",
    "        params = {'n_estimators': 100} \n",
    "\n",
    "        if Regression == True:\n",
    "            rf = ensemble.RandomForestRegressor(**params)\n",
    "            print_timestamp('We are running RandomForestRegressor')\n",
    "        else:\n",
    "            rf = ensemble.RandomForestClassifier(**params)\n",
    "            print_timestamp('We are running RandomForestClassifier')\n",
    "\n",
    "        run_rf(rf)\n",
    "\n",
    "    if flag_to_run_gradient_boosting  == True:\n",
    "        run_gradient_boosting()\n",
    "\n",
    "    if flag_to_run_linear_regression  == True:\n",
    "        run_linear_regression()\n",
    "\n",
    "    if flag_to_run_logistic_regression == True:\n",
    "        run_logistic_regression()\n",
    "\n",
    "    if flag_to_run_svc == True:\n",
    "        run_svc() \n",
    "\n",
    "    if flag_to_run_ridge_regression == True:\n",
    "        run_ridge_regression()\n",
    "        \n",
    "    if flag_to_run_vectorizer_nb == True:\n",
    "        for vectorizer_iterator in ['Count', 'Tfidf']:\n",
    "            vectorizer_nb(vectorizer_iterator)\n",
    "        \n",
    "    if flag_to_run_kmeans == True:\n",
    "        method = KMeans(\n",
    "             n_clusters=num_clusters\n",
    "#                 ,random_state=42\n",
    "#                 ,init='random'\n",
    "#                 ,n_init=10\n",
    "#                 ,max_iter=300\n",
    "#                 ,tol=1e-04 \n",
    "        )\n",
    "        df1 = run_kmeans(X_train, y_train, num_clusters)\n",
    "        plot_it_clusters(df1, xvalue=16, yvalue=16, title=\"KMeans with number of clusters = {}\".format(num_clusters))\n",
    "        display(\"next plot please\")\n",
    "\n",
    "    if flag_to_run_affinity_propagation == True:\n",
    "        display_column_names('columns of X_train going into affinity_propagation: ', X_train)\n",
    "        df2, ap_num_clusters = run_affinity_propagation(X_train, y_train)\n",
    "        plot_it_clusters(df2, xvalue=16, yvalue=16, title=\"Affinity Propagation with number of clusters = {}\".format(ap_num_clusters))\n",
    "        \n",
    "    if flag_to_run_mean_shift == True:\n",
    "        df3, mean_shift_num_clusters = run_mean_shift(X_train, y_train)\n",
    "        plot_it_clusters(df3, xvalue=16, yvalue=16, title=\"Mean Shift with number of clusters = {}\".format(mean_shift_num_clusters))\n",
    "    \n",
    "    if flag_to_run_spectral_clustering == True:\n",
    "        df4 = run_spectral_clustering(X_train, y_train, K=num_clusters)\n",
    "        plot_it_clusters(df4, xvalue=16, yvalue=16,title=\"Spectral clustering with number of clusters = {}\".format(num_clusters) )\n",
    "\n",
    "    print_timestamp('End'+'\\n'*3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(entry_point):\n",
    "        \n",
    "    if entry_point == 0:\n",
    "        print_timestamp(\"Starting main()\")\n",
    "        df = file_stuff()\n",
    "        data_demographics(df, 5)\n",
    "        display_column_names('post data_demographics of df', df)\n",
    "        df, X, y = dataset_cleanup(df)\n",
    "        display_column_names('post dataset_cleanup on X', X)\n",
    "        data_demographics(df, 5)\n",
    "        display_column_names('post data_demographics on X #2', X)\n",
    "#         make_X_and_Y()\n",
    "        X_train, X_test, y_train, y_test = training_test_set(X, y)\n",
    "        display_column_names('after training_test_set: columns of X_train going into affinity_propagation: ', X_train)\n",
    "#         data_characteristics()\n",
    "#         plot_time_to_complete()\n",
    "#         plot_model_accuracy()\n",
    "#         plot_facet()\n",
    "\n",
    "    if flag_to_run_elbow_plot == True:    do_the_elbow(X)\n",
    "    run_it(X_train, X_test, y_train, y_test, y)\n",
    "        \n",
    "    print_timestamp(\"Ending main()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_word_counts_from_text_files(gutenberg_books, regex=None, book_divisor=1 ):\n",
    "    print_timestamp(\"Starting main_nlp()\")\n",
    "      \n",
    "    sentences = pd.DataFrame()\n",
    "    \n",
    "    \n",
    "    book_list = [('milton-paradise.txt',r'BOOK .*') , ('melville-moby_dick.txt',r'BOOK .*',)]\n",
    "#     gutenberg_books is a list of files on the computer\n",
    "    for book in book_list:\n",
    "#     paradise_book = 'milton-paradise.txt'\n",
    "#     moby_book = 'melville-moby_dick.txt'\n",
    "\n",
    "        book - gutenberg.raw(book)\n",
    "        book = re.sub(book[1],'', book)\n",
    "#     paradise = gutenberg.raw(paradise_book)\n",
    "#     paradise = re.sub(r'BOOK .*', '', paradise)\n",
    "\n",
    "#     moby = gutenberg.raw(moby_book)\n",
    "#     moby = re.sub(r'BOOK .*', '', moby)\n",
    "        book = text_cleaner(book[:int(len(book)/book_divisor)])\n",
    "\n",
    "\n",
    "    paradise = text_cleaner(paradise[:int(len(paradise)/paradise_divisor)])\n",
    "    moby     = text_cleaner(moby[:int(len(moby)/moby_divisor)])\n",
    "\n",
    "    if debug == True:\n",
    "        print(\"length of paradise is {}, and length of moby is {}.\".format(len(paradise), len(moby)))\n",
    "\n",
    "    paradise_doc = nlp(paradise)\n",
    "    moby_doc = nlp(moby)\n",
    "\n",
    "    paradise_sents = [[sent, \"Milton\"] for sent in paradise_doc.sents]\n",
    "    moby_sents = [[sent, \"Melville\"] for sent in moby_doc.sents]\n",
    "\n",
    "    print(\"paradise_sents is a {} datatype, and moby_sents is a {} datatype.\".format(display_datatype(paradise_sents), display_datatype(moby_sents)))\n",
    "    # a better way to do this is with pd.append... from https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.append.html\n",
    "#     >>> df = pd.DataFrame([[1, 2], [3, 4]], columns=list('AB'))\n",
    "#     >>> df\n",
    "#        A  B\n",
    "#     0  1  2\n",
    "#     1  3  4\n",
    "#     >>> df2 = pd.DataFrame([[5, 6], [7, 8]], columns=list('AB'))\n",
    "#     >>> df.append(df2, ignore_index=True)\n",
    "#            A  B\n",
    "#         0  1  2\n",
    "#         1  3  4\n",
    "#         2  5  6\n",
    "#         3  7  8\n",
    "    sentences = pd.DataFrame(paradise_sents + moby_sents) \n",
    "    sentences = pd.DataFrame(paradise_sents)\n",
    "    sentences.append(moby_sents, ignore_index=True)\n",
    "    \n",
    "    if debug == True:\n",
    "        display(\"Here is a sample from sentences:\\n\", sentences.sample(20))  \n",
    "        print(\"sentences is a {} datatype.\".format(display_datatype(sentences)))\n",
    "        sentences.head(30)\n",
    "\n",
    "    paradisewords = bag_of_words(paradise_doc)\n",
    "    mobywords = bag_of_words(moby_doc)\n",
    "    common_words = list(set(paradisewords + mobywords))\n",
    "    word_counts = bow_features(sentences, common_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_nlp(entry_point):\n",
    "    \n",
    "    # this function was specifically designed for this NLP Challenge\n",
    "    \n",
    "    if entry_point == 0:\n",
    "        confusion_matrix = None\n",
    "        \n",
    "        paradise_divisor = 20 # was 20\n",
    "        moby_divisor     = 53 # was 53\n",
    "        \n",
    "        print_timestamp(\"Starting main_nlp()\")\n",
    "        \n",
    "        paradise_book = 'milton-paradise.txt'\n",
    "        moby_book = 'melville-moby_dick.txt'\n",
    "        \n",
    "        paradise = gutenberg.raw(paradise_book)\n",
    "        paradise = re.sub(r'BOOK .*', '', paradise)\n",
    "\n",
    "        moby = gutenberg.raw(moby_book)\n",
    "        moby = re.sub(r'BOOK .*', '', moby)\n",
    "        \n",
    "        paradise = text_cleaner(paradise[:int(len(paradise)/paradise_divisor)])\n",
    "        moby     = text_cleaner(moby[:int(len(moby)/moby_divisor)])\n",
    "        \n",
    "        if debug == True:\n",
    "            print(\"length of paradise is {}, and length of moby is {}.\".format(len(paradise), len(moby)))\n",
    "        \n",
    "        paradise_doc = nlp(paradise)\n",
    "        moby_doc = nlp(moby)\n",
    "        \n",
    "        paradise_sents = [[sent, \"Milton\"] for sent in paradise_doc.sents]\n",
    "        moby_sents = [[sent, \"Melville\"] for sent in moby_doc.sents]\n",
    "\n",
    "        print(\"paradise_sents is a {} datatype, and moby_sents is a {} datatype.\".format(display_datatype(paradise_sents), display_datatype(moby_sents)))\n",
    "        sentences = pd.DataFrame(paradise_sents + moby_sents) \n",
    "        if debug == True:\n",
    "            display(\"Here is a sample from sentences:\\n\", sentences.sample(20))  \n",
    "            print(\"sentences is a {} datatype.\".format(display_datatype(sentences)))\n",
    "            sentences.head(30)\n",
    "        \n",
    "        paradisewords = bag_of_words(paradise_doc)\n",
    "        mobywords = bag_of_words(moby_doc)\n",
    "        common_words = list(set(paradisewords + mobywords))\n",
    "        word_counts = bow_features(sentences, common_words)\n",
    "        \n",
    "        if debug == True:  display(\"words_counts sample:\\n\", word_counts.sample(20))\n",
    "        word_counts['author'] = np.where(word_counts['text_source'] == 'Milton', pd.to_numeric(1), pd.to_numeric(0))\n",
    "\n",
    "        X = np.array(word_counts.drop(['text_sentence', 'text_source', 'author'], 1))\n",
    "        Y = word_counts['author']\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.4, random_state=0)\n",
    "        \n",
    "        for runit in range(0,2):\n",
    "            if runit == 0:\n",
    "                \n",
    "                n_estimators = 100\n",
    "                \n",
    "                params = {'n_estimators': n_estimators}\n",
    "                printFormatted('## Running Random Forests with Word Counts')\n",
    "                run_rf(X_train=X_train, X_test=X_test, y_train=y_train, y_test=y_test, params=params, cross_validate=True)\n",
    "\n",
    "                printFormatted('## Running MultiNomial Naive Bayes with Word Counts')\n",
    "                params = {}\n",
    "                run_mnb(X_train=X_train, X_test=X_test, y_train=y_train, y_test=y_test, params=params, cross_validate=True)\n",
    "\n",
    "                printFormatted('## Running Bernoulli Naive Bayes with Word Counts')\n",
    "                run_BernoulliNB(X_train=X_train, X_test=X_test, y_train=y_train, y_test=y_test, params=params, cross_validate=True) \n",
    "       \n",
    "            else:\n",
    "            \n",
    "                author = []\n",
    "                all_paras = []\n",
    "                for paragraph in gutenberg.paras(paradise_book):\n",
    "                    para = paragraph[0]\n",
    "                    para = [re.sub(r'--', '', word) for word in para]\n",
    "                    all_paras.append(' '.join(para))\n",
    "                    author.append('Milton')\n",
    "\n",
    "                for paragraph in gutenberg.paras(moby_book):\n",
    "                    para = paragraph[0]\n",
    "                    para = [re.sub(r'--', '', word) for word in para]\n",
    "                    all_paras.append(' '.join(para))\n",
    "                    author.append('Melville')\n",
    "\n",
    "                paragraphs = pd.DataFrame()        \n",
    "                paragraphs['paragraphs'] = all_paras\n",
    "                paragraphs['author'] = author\n",
    "                paragraphs['author'] = np.where(paragraphs['author'] == 'Milton', pd.to_numeric(1), pd.to_numeric(0))\n",
    "                \n",
    "                if debug == True:\n",
    "                    display(\"paragraphs sampe:\\n\", paragraphs.sample(20))\n",
    "                    display(\"describe paragraphs\\n\", paragraphs.describe())\n",
    "\n",
    "                X = paragraphs['paragraphs']\n",
    "                Y = paragraphs['author']\n",
    "                printFormatted('## Running with Paragraph Counts and tfidf and others')\n",
    "\n",
    "                X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.4, random_state=0)\n",
    "                params = {}\n",
    "                tfidf_parms = {}\n",
    "                for vectorizer_iterator in ['rfc', 'bnb', 'mlb', 'svc']:\n",
    "\n",
    "                    path = 'A'\n",
    "                    if vectorizer_iterator == 'rfc':\n",
    "                        sentiment_analyzer(path=path, parameters=params, classifier=vectorizer_iterator, tfidf_parms=tfidf_parms, \n",
    "                                           X_train=X_train, X_test=X_test, y_train=y_train, y_test=y_test, cross_validate=True)\n",
    "\n",
    "                    elif vectorizer_iterator == 'bnb':\n",
    "                        parameters = {}\n",
    "                        sentiment_analyzer(path=path, parameters=params, classifier=vectorizer_iterator, tfidf_parms=tfidf_parms, \n",
    "                                           X_train=X_train, X_test=X_test, y_train=y_train, y_test=y_test, cross_validate=True)\n",
    "\n",
    "                    elif vectorizer_iterator == 'mlb':\n",
    "                        parameters = {}\n",
    "                        sentiment_analyzer(path=path, parameters=params, classifier=vectorizer_iterator, tfidf_parms=tfidf_parms, \n",
    "                                           X_train=X_train, X_test=X_test, y_train=y_train, y_test=y_test, cross_validate=True)\n",
    "                        \n",
    "                    elif vectorizer_iterator == 'logit': # newton-cg took too long. sag and saga about the same as lbfgs.\n",
    "                        tfidf_parms = {'max_features' : 10000 } # determined this through iterative testing\n",
    "                        \n",
    "                        parameters = {'C' :1e20, 'solver': 'lbfgs', 'max_iter': 1000} # max_iter=100 reports warning, try 1000\n",
    "                        sentiment_analyzer(path=path, parameters=parameters, classifier=vectorizer_iterator, tfidf_parms=tfidf_parms, \n",
    "                                           X_train=X_train, X_test=X_test, y_train=y_train, y_test=y_test, cross_validate=True)\n",
    "\n",
    "                    elif vectorizer_iterator == 'svc':\n",
    "                        parameters = {}\n",
    "                        sentiment_analyzer(path=path, parameters=parameters, classifier=vectorizer_iterator, tfidf_parms=tfidf_parms, \n",
    "                                           X_train=X_train, X_test=X_test, y_train=y_train, y_test=y_test, cross_validate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "2019-08-14 23:06:41: In: main_nlp Starting main_nlp() "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "paradise_sents is a <class 'list'> datatype, and moby_sents is a <class 'list'> datatype.\n",
      "Processing row 0\n",
      "Processing row 50\n",
      "Processing row 100\n",
      "Processing row 150\n",
      "Processing row 200\n",
      "Processing row 250\n",
      "Processing row 300\n",
      "Processing row 350\n",
      "Processing row 400\n",
      "Processing row 450\n",
      "Processing row 500\n",
      "Processing row 550\n",
      "Processing row 600\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Running Random Forests with Word Counts"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In run_rf, params = {'n_estimators': 100}\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Training score = 98.37%"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Test score = 86.59%"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "###  Metrics test accuracy score = 86.59%"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Cross validation scores:  [0.83783784 0.83783784 0.7972973  0.83783784 0.79166667]"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Accuracy of Model with Cross Validation average is: 82.05%"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "2019-08-14 23:09:57: In: run_rf End run_rfr part 1 "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## Running MultiNomial Naive Bayes with Word Counts"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Training score = 97.55%"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Test score = 92.28%"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "###  Metrics test accuracy score = 92.28%"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Cross validation scores:  [0.90540541 0.85135135 0.89189189 0.90540541 0.875     ]"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Accuracy of Model with Cross Validation average is: 88.58%"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## Running Bernoulli Naive Bayes with Word Counts"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Training score = 82.34%"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Test score = 83.33%"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "###  Metrics test accuracy score = 83.33%"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Cross validation scores:  [0.7972973  0.74324324 0.74324324 0.75675676 0.76388889]"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Accuracy of Model with Cross Validation average is: 76.09%"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## Running with Paragraph Counts and tfidf and others"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "###  Now running with:  tfidf=TfidfVectorizer and clf=RandomForestClassifier 2019-08-14 23:09:59.268467\n",
       "parameters={} \n",
       "\n",
       " tfidf_parms={}"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lou/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Training score = 99.76%"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Test score = 98.76%"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "###  Metrics test accuracy score = 98.76%"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Cross validation scores:  [0.99115044 0.99115044 0.99115044 0.99112426 0.99112426]"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Accuracy of Model with Cross Validation average is: 99.11%"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "2019-08-14 23:09:59: In: sentiment_analyzer \n",
       "\n",
       "\n",
       "End "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "###  Now running with:  tfidf=TfidfVectorizer and clf=BernoulliNB 2019-08-14 23:09:59.774286\n",
       "parameters={} \n",
       "\n",
       " tfidf_parms={}"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Training score = 99.05%"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Test score = 98.76%"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "###  Metrics test accuracy score = 98.76%"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Cross validation scores:  [0.99115044 0.99115044 0.99115044 0.99112426 0.99112426]"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Accuracy of Model with Cross Validation average is: 99.11%"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "2019-08-14 23:10:00: In: sentiment_analyzer \n",
       "\n",
       "\n",
       "End "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "###  Now running with:  tfidf=TfidfVectorizer and clf=MultinomialNB 2019-08-14 23:10:00.125135\n",
       "parameters={} \n",
       "\n",
       " tfidf_parms={}"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Training score = 99.11%"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Test score = 98.76%"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "###  Metrics test accuracy score = 98.76%"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Cross validation scores:  [0.99115044 0.99115044 0.99115044 0.99112426 0.99112426]"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Accuracy of Model with Cross Validation average is: 99.11%"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "2019-08-14 23:10:00: In: sentiment_analyzer \n",
       "\n",
       "\n",
       "End "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "###  Now running with:  tfidf=TfidfVectorizer and clf=SVC 2019-08-14 23:10:00.467223\n",
       "parameters={} \n",
       "\n",
       " tfidf_parms={}"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Training score = 99.11%"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Test score = 98.76%"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "###  Metrics test accuracy score = 98.76%"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Cross validation scores:  [0.99115044 0.99115044 0.99115044 0.99112426 0.99112426]"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Accuracy of Model with Cross Validation average is: 99.11%"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "2019-08-14 23:10:01: In: sentiment_analyzer \n",
       "\n",
       "\n",
       "End "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it ran ok, dude!\n"
     ]
    }
   ],
   "source": [
    "main_nlp(0)\n",
    "print(\"it ran ok, dude!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge Conclusion\n",
    "#### As you can see, the best metrics accuracy cross validations scores came from the models using tfidf for feature generation, and they all tied at accuracy of 98.76%\n",
    "\n",
    "#### I tried to increase the metrics accuracy of the word counts Random Forest model, but was unable to change it.  I was unable to change the parameter settings for word counts, as this model generator has fewer options than tfidf method of generating features.\n",
    "\n",
    "#### All of the models running with tfidf did exceptionally well, and the models using word counts were less favorable than the other test group."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Summary of result runs\n",
    "\n",
    "|Model|NLP Feature Generator|Training Score|:Test Score|Metrics Accuracy|Accuracy of Model|Status|\n",
    "|:----|:--------------------|:-------------|:----------|:---------------|:----------------|:----|\n",
    "|Random Forests|Word Counts|98.37|86.59|86.59|82.05| |\n",
    "|Multinomial Naive Bayes|Word Counts|97.55|92.28|92.28|88.58| |\n",
    "|Bernoulli Naive Bayes|Word Counts|82.34|83.33|83.33|76.09| |\n",
    "|Random Forests|tfidf|99.76|98.76|98.76|99.11|**Winner** |\n",
    "|Multinomial Naive Bayes|tfidf|99.11|98.76|98.76|99.11|**Winner** |\n",
    "|Bernoulli Naive Bayes|tfidf|99.05|98.76|98.76|99.11|**Winner**|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
