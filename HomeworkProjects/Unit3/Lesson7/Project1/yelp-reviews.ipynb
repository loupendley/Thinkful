{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://https://www.kaggle.com/yelp-dataset/yelp-dataset'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervised Learning Capstone  \n",
    "#### You're ready to put into practice everything you've learned so far.  \n",
    "\n",
    "####  First: Go out and find a dataset of interest. It could be from one of our recommended resources, some other aggregation, or scraped yourself. Just make sure it has lots of variables in it, including an outcome of interest to you.  \n",
    "\n",
    "####  Second: Explore the data. Get to know the data. Spend a lot of time going over its quirks and peccadilloes. You should understand how it was gathered, what's in it, and what the variables look like.  \n",
    "\n",
    "####  Third: Model your outcome of interest. You should try several different approaches and really work to tune a variety of models before using the model evaluation techniques to choose what you consider to be the best performer. Make sure to think about explanatory versus predictive power and experiment with both.  \n",
    "\n",
    "####  So, here is the deliverable: Prepare a slide deck and 15 minute presentation that guides viewers through your model. Be sure to cover a few specific things:  \n",
    "\n",
    "1. A specified research question your model addresses  \n",
    "1. How you chose your model specification and what alternatives you compared it to  \n",
    "1. The practical uses of your model for an audience of interest  \n",
    "1. Any weak points or shortcomings of your model  \n",
    "\n",
    "\n",
    "This presentation is not a drill. You'll be presenting this slide deck live to a group as the culmination of your work in the last two supervised learning units. As a secondary matter, your slides and / or the Jupyter notebook you use or adapt them into should be worthy of inclusion as examples of your work product when applying to jobs.\n",
    "\n",
    "Good luck!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Specific items to cover:\n",
    "\n",
    "1.  #### Research question that this model addresses:\n",
    "    #### Can we predict the number of stars for a Yelp rating from the review provided by the customer?  \n",
    "    \n",
    "1.  #### How did I choose my model specifications and against what alternatives did I compare it?\n",
    "#### This was my performance criteria for this capstone:\n",
    "    1. #### Model needed to complete the fitting, and prediction, and scoring within 20 minutes, with minimal parameter tuning of either the vectorizer, or the model, using a classifier model.\n",
    "    \n",
    "    \n",
    "####  I chose a classifier model suitable for sentiment analysis, and chose from these 5 models:  \n",
    "1. #### Random Forests\n",
    "2. #### Support Vector Machines\n",
    "3. #### Naive Bayes\n",
    "4. #### MultiNomial Naive Bayes\n",
    "5. #### Logistic Regression\n",
    "    \n",
    "####    In addition, I chose 2 Vectorizers for the model:\n",
    "1. #### CountVectorizer\n",
    "1. #### TfidfVectorizer\n",
    "    \n",
    "    \n",
    "#### My findings on the 5 models attempted above:\n",
    "1. Random Forests - RFC\n",
    "With the dataset from Kaggle, RFC was not sufficiently performant using either CountVectorizer, or TfidVectorizer, in relation to the timings of the other models.  Several of my test runs did complete in several hours, but that was not within my criteria.\n",
    "       \n",
    "2. Support Vector Machines - SVC\n",
    "SVC was another candidate that was not sufficiently performant, and did not finish in the 20 minute time requirement.  Perhaps this model requires more powerful hardware to complete in less than 20 minutes.  On several test runs, I allowed it to run overnight, and minimal progress was made.  It could be that there are parameter that could optimize this, but this is my first time to use SVC for sentiment analysis. \n",
    "       \n",
    "1. Naive Bayes - BernoulliNB\n",
    "This model using some different testing with parameters, and combinations of CountVectorizer, and TfidfVectorizer was performant.  This became my third choice, coming in at a prediction rate of 75.81%, with an elapsed time of 11 minutes.  \n",
    "       \n",
    "4. MultiNomial Naive Bayes - MultinomialNB\n",
    "This model was initially my pick to be the most performant, but unfortunately, it came in as my second, at a prediction rate of 85.98%, and completed in 12 minutes.  \n",
    "       \n",
    "5. Logistic Regression - LR  \n",
    "This model was not the fastest, but had the best prediction percent at 91.08%, and completing in 17 minutes.  This is my pick for best accuracy, given the performance within my requirement.\n",
    "       \n",
    "Discoveries in my testing:\n",
    "    \n",
    "1.  Iterating through the different models, and varying parameters to find the best fit can take a long, long time.  Perhaps optimizations can help on this.  \n",
    "2.  Given the numerous parameters on some of the models, finding the right combination of n parameters could use some extra computing resources to shorten the iteration times.\n",
    "    \n",
    "    \n",
    "       \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practical Uses of the Model\n",
    "\n",
    "#### This model is very valuable in many ways.\n",
    "\n",
    "#### Once trained, and optimized for performance, which is beyond the scope of this capstone, these are some of the things that could be of great business value:\n",
    "\n",
    "1.  #### Sentiment analysis in real time, while providing a review, and potentially linking up an unhappy customer with a discounted \"next visit\" special.\n",
    "1.  #### Daily sentiment temperature for business owners that use Yelp, to see if the reviews are going up, down, or level for a given business period - daily, weekly, monthly, and reaching out to those customers in some way.\n",
    "1.  #### Applying this same model to e-mails from customers/vendors/employees to measure sentiment, and to be proactive.\n",
    "1.  #### With modifications, determine if fake reviews are being submitted, and to stop them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Dataset\n",
    "\n",
    "#### The Yelp dataset that originiated from Kaggle includes 5.2 million Yelp user reviews for 174,000 business, spanning 11 metropolitan cities around the world.\n",
    "\n",
    "#### Dataset components:\n",
    "1. review_id  \n",
    "An internal id from Yelp\n",
    "1. user_id  \n",
    "An internal user id from Yelp.\n",
    "1. business_id  \n",
    "An internal business id from Yelp.\n",
    "1. stars  \n",
    "The number of stars that a review was given by a reviewer.  1 being the worst, and 5 being the best.\n",
    "1. useful\n",
    "This is another rating that a person reading the review can provide as a rating of usefulness.\n",
    "1. funny  \n",
    "This is another level of review that a person reading the review can provide as a rating of funny.\n",
    "1. cool  \n",
    "This is another level of review that a person reading the review can provide as a rating of cool.\n",
    "1. text  \n",
    "This is a free form text review field, that can be up to 5,000 characters.\n",
    "1. date  \n",
    "Date of review."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions that need answering:\n",
    "\n",
    " 1. What question are you trying to solve or prove in this model ?   \n",
    " __We are correlating the sentiment analysis from the review with the stars rating that a customer provided.  We want to predict the stars rating from the customer review.  .__\n",
    " 1. What kind of data do you have? -> describe the source.. \n",
    " __The dataset includes 5.2 million Yelp user reviews for 174,000 businesses, spanning 11 metropolitan cities.__\n",
    " 1. What's missing from the data and how do you deal with it?  \n",
    " __There really is nothing missing from the data.    __\n",
    " 1. Where are the outliers and why should we pay attention to them?  \n",
    " __There were no outliers to mention in the dataset.  Since this is sentiment data, we don't have outliers.__\n",
    " 1. How can you add, change, or remove features to get more out of your data?  \n",
    " __I added message_length, derived from the length of the message.  In addition Feature importances, PCA, dropping low variance items, correlated feature pairs, features too highly correlated, feature engineering, transforming, timestamp and make a month, day, year column from it.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regression = False\n"
     ]
    }
   ],
   "source": [
    "# Constants\n",
    "random_state           = 57\n",
    "train_size             = 0.90\n",
    "test_size              = 0.10\n",
    "max_data               = 0.149568495  #6685900 #        file_line_count # for the entire file 1m = 0.149568495 full: 6685900\n",
    "rfc_test_size          = 50000\n",
    "rfc_train_size         = 5000\n",
    "run_CountVectorizer    = True\n",
    "run_TfidfVectorizer    = True\n",
    "BegTimeStampNewlines   = 3\n",
    "EndTimeStampNewlines   = 3\n",
    "EndTimeStamp           = '\\n'*EndTimeStampNewlines+'End'\n",
    "BegTimeStamp           = 'Begin'+'\\n'*BegTimeStampNewlines\n",
    "\n",
    "# Regression/Classification control\n",
    "Regression = False \n",
    "\n",
    "print(\"Regression = {}\".format(Regression))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Controls\n",
    "flag_to_run_rf = False\n",
    "flag_to_plot_them = False\n",
    "flag_to_run_correlation_matrix = False\n",
    "flag_to_run_features_importance = False\n",
    "flag_to_run_gradient_boosting  = False\n",
    "flag_to_run_linear_regression  = False\n",
    "flag_to_run_logistic_regression = False\n",
    "flag_to_run_lasso_regression = False\n",
    "flag_to_run_ridge_regression = False\n",
    "flag_to_run_svc = False\n",
    "flag_to_run_vectorizer_nb = False\n",
    "flag_to_run_sentiment_analyzer = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import chardet\n",
    "import datetime\n",
    "from sklearn import ensemble\n",
    "from sklearn import datasets\n",
    "from sklearn import metrics\n",
    "from sklearn import linear_model\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "import time, sys\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.svm import SVC\n",
    "import statsmodels.api as sm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import cross_val_predict, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from IPython.display import Markdown, display\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "# from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd set preferences.\n",
    "%matplotlib inline\n",
    "pd.options.display.float_format = '{:.3f}'.format\n",
    "# pd.set_option('display.max_rows', 1000)\n",
    "pd.set_option('display.max_row', 1000)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('max_colwidth',200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_length(full_file_name):\n",
    "    count = 0\n",
    "    for line in open(full_file_name).readlines(  ): \n",
    "        count += 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_logfile(message, mdformat=''):\n",
    "    bufsize = 0\n",
    "    with open('TestResults.md', 'a+') as the_file:\n",
    "        the_file.write('{} {}'.format(mdformat, message))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_timestamp(displaytext):    \n",
    "    import sys\n",
    "    import datetime\n",
    "    datetime_now = str(datetime.datetime.now())\n",
    "    printFormatted(\"{:19.19}: In: {} {} \".format(datetime_now, sys._getframe(1).f_code.co_name, displaytext))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def file_stuff():\n",
    "\n",
    "    global df\n",
    "    \n",
    "    file = 'yelp_academic_dataset_review.json'# /Users/lou/GITHubProjects/Thinkful/Datafiles/yelp/yelp_academic_dataset_review.json\n",
    "    path = path=\"../../../../Datafiles/yelp/\"\n",
    "    column_names = []\n",
    "    import json\n",
    "\n",
    "    print_timestamp(\"Log start get filecount\")\n",
    "    file_line_count = file_length(path+file) # \n",
    "\n",
    "    file_len = file_length(path+file)\n",
    "    print_timestamp(\"There are {:,} lines in data file {}.\".format(file_len, file))\n",
    "\n",
    "    line = None\n",
    "    data = []\n",
    "    cnt = 0\n",
    "    if isinstance(max_data, float):\n",
    "        max_data2 = max_data * file_len # it's a factor of the full file size\n",
    "    elif isinstance(max_data, int):\n",
    "        max_data2 = max_data # it's the number of records in the file\n",
    "\n",
    "    with open(path+file, 'r',encoding='UTF-8') as f:\n",
    "      line = f.readline()\n",
    "      while line and cnt < max_data2:\n",
    "        data.append(json.loads(line))\n",
    "        line = f.readline()\n",
    "        cnt += 1\n",
    "    print_timestamp(\"Log end read datafile {}\".format(file))\n",
    "\n",
    "    column_names = ['review_id', 'user_id', 'business_id', 'stars', 'useful','funny','cool','text', 'date']\n",
    "    df = pd.DataFrame(data, columns = column_names)\n",
    "    print_timestamp(\"there are {:,} entries in  {}\".format(len(df), 'data frame df'))\n",
    "    print_timestamp(\"columns={}\".format(df.columns))\n",
    "\n",
    "    # data Cleanup\n",
    "\n",
    "    df['sentiment_label'] = df.stars.map({4.0:1, 5.0: 1, 3.0:0, 2.0: 0, 1.0:0}) # Let's conver the stars values into success(1.0) or fail(0)\n",
    "    df = df.rename({'text': 'message'}, axis=1)\n",
    "    df['message_length'] = df['message'].str.len()\n",
    "    print_timestamp(\"columns are {}\".format(df.columns))\n",
    "\n",
    "    print_timestamp('we have cleaned up the dataframe.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printFormatted(string):\n",
    "    newline = '\\n'\n",
    "    display(Markdown(string))\n",
    "    write_to_logfile(string+newline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_X_and_Y():\n",
    "        \n",
    "    global X, y\n",
    "    \n",
    "    # Definine outcome and predictors.\n",
    "    print_timestamp(\"the current time of start is {}\".format(str(datetime.datetime.now())))\n",
    "\n",
    "#     # Make the categorical variables below into enumerated categorical variables\n",
    "#     for dummy_column in []: # Remove these for now...\n",
    "#         df = pd.concat([df, pd.get_dummies(df[dummy_column])], axis=1)\n",
    "#     df.columns = df.columns.str.replace(' ', '')\n",
    "\n",
    "    # Definine outcome and predictors.\n",
    "\n",
    "    y = df['sentiment_label']\n",
    "    X = df['message']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_them():\n",
    "    for column in X_train.columns:\n",
    "#         plt.hist(X_train[column]*100, bins=40)\n",
    "        plt.scatter(y_train, X_train[column]*100)\n",
    "        plt.xlabel(column)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_test_set():\n",
    "    \n",
    "    global X_train, X_test, y_train, y_test\n",
    "    printFormatted(\"test_size={}, and train_size={}\".format(test_size,train_size))\n",
    "    # Let's fit it with the RFC training set\n",
    "    #  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, train_size=train_size, random_state=0)\n",
    "    printFormatted(\"train_size = {}, X_train is {}, and y_train is {}\".format(train_size, len(X_train), len(y_train)))\n",
    "    printFormatted(\"test_size  = {}, X_test  is {}, and y_test is {}\".format(test_size, len(X_test), len(y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_them_others(df,columnlist):\n",
    "    \n",
    "    print_timestamp(BegTimeStamp)\n",
    "    \n",
    "    print_timestamp(\"We will plot these independent variables {}\".format(columnlist))\n",
    "    for column in columnlist:\n",
    "        plt.hist(df[column], bins=90)\n",
    "        plt.xlabel(column)\n",
    "        plt.ylabel('count')\n",
    "        plt.show()\n",
    "#     dfcolumn.plot.hist()\n",
    "\n",
    "    print_timestamp(EndTimeStamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_correlation_matrix():\n",
    "    \n",
    "    print_timestamp(BegTimeStamp)\n",
    "    \n",
    "    # Setup the correlation matrix.\n",
    "    corrmat = X.corr()\n",
    "    print(corrmat)\n",
    "\n",
    "    # Set up the subplots\n",
    "    f, ax = plt.subplots(figsize=(12, 9))\n",
    "\n",
    "    # Let's draw the heatmap using seaborn.\n",
    "    sns.heatmap(corrmat, vmax=.6, square=True)\n",
    "    plt.show()\n",
    "    \n",
    "    print_timestamp(EndTimeStamp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try predicting with gradient boosting classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_analyzer(path, parameters, classifier, tfidf_parms):\n",
    "    # path A = the old path\n",
    "    # path B = the new path, no CountVectorizer at all\n",
    "    \n",
    "    \n",
    "# run block of code and catch warnings\n",
    "  \n",
    "    print_timestamp(BegTimeStamp+\" running with path={}\".format(path))\n",
    "   \n",
    "#     tfidf_parms = {\n",
    "#         'strip_accents': 'unicode',\n",
    "#         'stop_words'   : 'english',\n",
    "#         'ngram_range'  : (2,2),\n",
    "#         'max_df'       : 3,\n",
    "#         'min_df'       : 2\n",
    "#     }\n",
    "    \n",
    "    global vectorized\n",
    "    vectorized = True\n",
    "    \n",
    "    pipeline_array = []\n",
    "   \n",
    "    if path == \"A\":\n",
    "        if classifier == 'bnb':\n",
    "            pipeline_array.append(Pipeline([\n",
    "#                 ('vect',  CountVectorizer()),\n",
    "#                 ('tfidf', TfidfTransformer()),\n",
    "                ('tfidf', TfidfVectorizer(**tfidf_parms)),\n",
    "                ('clf',   BernoulliNB(**parameters))\n",
    "            ]))\n",
    "        elif classifier == 'svc':\n",
    "            pipeline_array.append(Pipeline([\n",
    "    #             ('vect',  CountVectorizer()),\n",
    "    #             ('tfidf', TfidfTransformer()),\n",
    "                ('tfidf', TfidfVectorizer(**tfidf_parms)),\n",
    "                ('clf',   SVC(kernel = 'linear', **parameters))\n",
    "            ])) \n",
    "        elif classifier == 'mlb':\n",
    "            pipeline_array.append(Pipeline([\n",
    "#                 ('vect',  CountVectorizer()),\n",
    "#                 ('tfidf', TfidfTransformer()),\n",
    "                ('tfidf', TfidfVectorizer(**tfidf_parms)),\n",
    "                ('clf',   MultinomialNB(**parameters))\n",
    "            ]))\n",
    "        elif classifier == 'logit':\n",
    "            pipeline_array.append(Pipeline([\n",
    "                ('tfidf', TfidfVectorizer(**tfidf_parms)),\n",
    "#                 ('vect',  CountVectorizer()),\n",
    "#                 ('tfidf', TfidfTransformer()),\n",
    "                ('clf',   LogisticRegression(**parameters))\n",
    "            ]))\n",
    "        elif classifier == 'rfc':\n",
    "            pipeline_array.append(Pipeline([\n",
    "                ('vect',  CountVectorizer()),\n",
    "                ('tfidf', TfidfTransformer()),\n",
    "#                 ('tfidf', TfidfVectorizer(**tfidf_parms)),\n",
    "                ('clf',   ensemble.RandomForestClassifier(**parameters))\n",
    "            ]))  \n",
    "            \n",
    "    elif path == \"B\":\n",
    "        if classifier == 'bnb':\n",
    "            pipeline_array.append(Pipeline([\n",
    "                ('tfidf', TfidfVectorizer(**tfidf_parms)),\n",
    "                ('clf',   BernoulliNB(**parameters))\n",
    "            ]))\n",
    "        elif classifier == 'svc':\n",
    "            pipeline_array.append(Pipeline([\n",
    "                ('tfidf', TfidfVectorizer(**tfidf_parms)),\n",
    "                ('clf',   SVC(kernel = 'linear', **parameters))\n",
    "            ])) \n",
    "        elif classifier == 'mlb':\n",
    "            pipeline_array.append(Pipeline([\n",
    "                ('tfidf', TfidfVectorizer(**tfidf_parms)),\n",
    "                ('clf',   MultinomialNB(**parameters))\n",
    "            ]))\n",
    "        elif classifier == 'logit':\n",
    "            pipeline_array.append(Pipeline([\n",
    "                ('tfidf', TfidfVectorizer()),\n",
    "                ('clf',   LogisticRegression(**parameters))\n",
    "            ]))\n",
    "        elif classifier == 'rfc':\n",
    "            pipeline_array.append(Pipeline([\n",
    "                ('tfidf', TfidfVectorizer(**tfidf_parms)),\n",
    "                ('clf',   ensemble.RandomForestClassifier(**parameters))\n",
    "            ]))\n",
    "\n",
    "    for pipe in pipeline_array:\n",
    "        len1 = len(pipeline_array)\n",
    "        \n",
    "        try:\n",
    "            vect_name_list = str(pipe.named_steps['vect']).split('(')\n",
    "            vect_name = \"vect = {}, \".format(vect_name_list[0])\n",
    "        except:\n",
    "            vect_name = ''\n",
    "            \n",
    "        classifier_name_list=str(pipe.named_steps['clf']).split('(')\n",
    "        classifier_name=classifier_name_list[0]\n",
    "        tfidf_name_list = str(pipe.named_steps['tfidf']).split('(')\n",
    "        if len(tfidf_name_list) > 0:\n",
    "            tfidf_name = tfidf_name_list[0]\n",
    "        else:\n",
    "            tfidf_name = ''\n",
    "\n",
    "        printFormatted(\"####  Now running pipeline with: {} tfidf={} and clf={}\\nparameters={}:\".format(vect_name,\n",
    "                                                                                                        tfidf_name,\n",
    "                                                                                                        classifier_name,\n",
    "                                                                                                        parameters))\n",
    "\n",
    "        pipe.fit(X_train, y_train)\n",
    "\n",
    "        y_pred_class = pipe.predict(X_test)\n",
    "\n",
    "        metrics_test_score = metrics.accuracy_score(y_test, y_pred_class)\n",
    "        printFormatted('###  Metrics accuracy score = {:.2%} with {}'.format(metrics_test_score, classifier_name))\n",
    "\n",
    "        printFormatted(\"Steps information: {}\".format(pipe.steps))\n",
    "        print_timestamp(\"Finished running pipeline with:\\n{}: \".format(classifier_name))\n",
    "            \n",
    "    print_timestamp(EndTimeStamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_it():\n",
    "    \n",
    "    print_timestamp(BegTimeStamp)\n",
    "   \n",
    "    if Regression == True:\n",
    "        print_timestamp(\"We are running with a Regression model\")\n",
    "    elif Regression == False:\n",
    "        print_timestamp(\"We are running with a Classifier model\")\n",
    "    else:\n",
    "        print_timestamp(\"We have failed to set the Regression variable\")\n",
    "        sys.exit(main())\n",
    "        \n",
    "    if flag_to_plot_them == True:\n",
    "        print(\"we went to flag_to_plot_them\")\n",
    "#         for colnames in ['stars','useful','funny','cool','message_length']:\n",
    "#             plot_them_others(df[colnames])\n",
    "        columns_to_plot = ['funny','cool','useful','message_length','sentiment_label']\n",
    "        plot_them_others(df, columns_to_plot)\n",
    "        \n",
    "#         plot_them()\n",
    "\n",
    "    if flag_to_run_features_importance == True:\n",
    "        \n",
    "        number_of_features_to_consider = 50\n",
    "        params = {'n_estimators': 100}\n",
    "\n",
    "        if Regression == True:\n",
    "            print_timestamp('We are running RandomForestRegressor')\n",
    "            rf = ensemble.RandomForestRegressor(**params)\n",
    "            \n",
    "        else:\n",
    "            print_timestamp('We are running RandomForestClassifier')\n",
    "            rf = ensemble.RandomForestClassifier(**params)\n",
    "\n",
    "        run_features_importance(rf, number_of_features_to_consider)\n",
    "\n",
    "    if flag_to_run_correlation_matrix == True:\n",
    "        run_correlation_matrix()\n",
    "\n",
    "    if flag_to_run_rf == True:\n",
    "        #     params = {}\n",
    "        params = {'n_estimators': 100} \n",
    "\n",
    "        if Regression == True:\n",
    "            rf = ensemble.RandomForestRegressor(**params)\n",
    "            print_timestamp('We are running RandomForestRegressor')\n",
    "        else:\n",
    "            rf = ensemble.RandomForestClassifier(**params)\n",
    "            print_timestamp('We are running RandomForestClassifier')\n",
    "\n",
    "        run_rf(rf)\n",
    "\n",
    "    if flag_to_run_gradient_boosting  == True:\n",
    "        run_gradient_boosting()\n",
    "\n",
    "    if flag_to_run_linear_regression  == True:\n",
    "        run_linear_regression()\n",
    "\n",
    "    if flag_to_run_logistic_regression == True:\n",
    "        run_logistic_regression()\n",
    "\n",
    "    if flag_to_run_svc == True:\n",
    "        run_svc() \n",
    "\n",
    "    if flag_to_run_ridge_regression == True:\n",
    "        run_ridge_regression()\n",
    "        \n",
    "    if flag_to_run_sentiment_analyzer == True:\n",
    "        path = \"B\"\n",
    "        tfidf_parms = {\n",
    "#       original parameters\n",
    "#         'strip_accents': 'unicode', unused\n",
    "#             'stop_words'   : 'english',\n",
    "#             'ngram_range'  : (2,2),\n",
    "#             'max_df'       : .5,\n",
    "#             'min_df'       : 15,\n",
    "#             'max_features' : 50000\n",
    "            \n",
    "            'stop_words'   : 'english',\n",
    "            'ngram_range'  : (1,3),\n",
    "            'max_df'       : .8,\n",
    "            'min_df'       : .1,\n",
    "            'max_features' : 50000\n",
    "        }\n",
    "                \n",
    "        printFormatted(\"tfidf_parms={}\".format(tfidf_parms))\n",
    "        for path in ['A']:\n",
    "#             for vectorizer_iterator in ['bnb', 'mlb', 'logit','rfc', 'svc']:  # removed 'rfc' -> too slow with large datasets\n",
    "            for vectorizer_iterator in ['logit', 'mlb', 'bnb']:\n",
    "                if vectorizer_iterator == 'rfc':\n",
    "                    sentiment_analyzer(path, params, vectorizer_iterator, tfidf_parms)\n",
    "                elif vectorizer_iterator == 'bnb':\n",
    "                    parameters = {}\n",
    "                    sentiment_analyzer(path, parameters, vectorizer_iterator, tfidf_parms)\n",
    "                elif vectorizer_iterator == 'mlb':\n",
    "                    parameters = {}\n",
    "                    sentiment_analyzer(path, parameters, vectorizer_iterator, tfidf_parms)\n",
    "                elif vectorizer_iterator == 'logit': # newton-cg took too long. sag and saga about the same as lbfgs.\n",
    "                    parameters = {'C' :1e20, 'solver': 'lbfgs', 'max_iter': 100} # not that good\n",
    "                    sentiment_analyzer(path, parameters, vectorizer_iterator, tfidf_parms)\n",
    "                elif vectorizer_iterator == 'svc':\n",
    "                    parameters = {}\n",
    "                    sentiment_analyzer(path, parameters, vectorizer_iterator, tfidf_parms)\n",
    "\n",
    "    print_timestamp(EndTimeStamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(entry_point):\n",
    "    if entry_point == 0:\n",
    "        print_timestamp(\"Starting main()\")\n",
    "        file_stuff()\n",
    "\n",
    "        make_X_and_Y()\n",
    "\n",
    "#         X_describe()\n",
    "#         X_dtypes()\n",
    "#         df_sample(10)\n",
    "#         X_isnull()\n",
    "\n",
    "        training_test_set()\n",
    "\n",
    "    run_it()\n",
    "    \n",
    "    print_timestamp(\"Ending main()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "2019-06-24 22:16:29: In: main Starting main() "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "2019-06-24 22:16:29: In: file_stuff Log start get filecount "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "2019-06-24 22:17:01: In: file_stuff There are 6,685,900 lines in data file yelp_academic_dataset_review.json. "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "2019-06-24 22:17:09: In: file_stuff Log end read datafile yelp_academic_dataset_review.json "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "2019-06-24 22:17:11: In: file_stuff there are 1,000,001 entries in  data frame df "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "2019-06-24 22:17:11: In: file_stuff columns=Index(['review_id', 'user_id', 'business_id', 'stars', 'useful', 'funny', 'cool', 'text', 'date'], dtype='object') "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "2019-06-24 22:17:12: In: file_stuff columns are Index(['review_id', 'user_id', 'business_id', 'stars', 'useful', 'funny', 'cool', 'message', 'date', 'sentiment_label', 'message_length'], dtype='object') "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "2019-06-24 22:17:12: In: file_stuff we have cleaned up the dataframe. "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "2019-06-24 22:17:12: In: make_X_and_Y the current time of start is 2019-06-24 22:17:12.605287 "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "test_size=0.1, and train_size=0.9"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "train_size = 0.9, X_train is 900000, and y_train is 900000"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "test_size  = 0.1, X_test  is 100001, and y_test is 100001"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "2019-06-24 22:17:12: In: run_it Begin\n",
       "\n",
       "\n",
       " "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "2019-06-24 22:17:12: In: run_it We are running with a Classifier model "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "tfidf_parms={'stop_words': 'english', 'ngram_range': (1, 3), 'max_df': 0.8, 'min_df': 0.1, 'max_features': 50000}"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "2019-06-24 22:17:12: In: sentiment_analyzer Begin\n",
       "\n",
       "\n",
       " running with path=A "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "####  Now running pipeline with:  tfidf=TfidfVectorizer and clf=LogisticRegression\n",
       "parameters={'C': 1e+20, 'solver': 'lbfgs', 'max_iter': 100}:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "###  Metrics accuracy score = 76.24% with LogisticRegression"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Steps information: [('tfidf', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=0.8, max_features=50000, min_df=0.1,\n",
       "        ngram_range=(1, 3), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "        stop_words='english', strip_accents=None, sublinear_tf=False,\n",
       "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "        vocabulary=None)), ('clf', LogisticRegression(C=1e+20, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='lbfgs',\n",
       "          tol=0.0001, verbose=0, warm_start=False))]"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "2019-06-24 22:26:05: In: sentiment_analyzer Finished running pipeline with:\n",
       "LogisticRegression:  "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "2019-06-24 22:26:05: In: sentiment_analyzer \n",
       "\n",
       "\n",
       "End "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "2019-06-24 22:26:16: In: sentiment_analyzer Begin\n",
       "\n",
       "\n",
       " running with path=A "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "####  Now running pipeline with:  tfidf=TfidfVectorizer and clf=MultinomialNB\n",
       "parameters={}:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "main(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_timestamp(BegTimeStamp)\n",
    "tfidf_parms = {\n",
    "#         'strip_accents': 'unicode',\n",
    "        'stop_words'   : 'english',\n",
    "        'ngram_range'  : (2,2),\n",
    "        'max_df'       : .5,\n",
    "        'min_df'       : 20,\n",
    "        'max_features' : 50000\n",
    "    }\n",
    "\n",
    "Tf = TfidfVectorizer(**tfidf_parms)\n",
    "tfifx = Tf.fit_transform(X_train[:100000])\n",
    "\n",
    "# Tf.vocabulary_\n",
    "# len(Tf.vocabulary_)\n",
    "# tfifx.shape\n",
    "print_timestamp(EndTimeStamp+\"Here is the shape of Tf \" + str(tfifx.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tf.vocabulary_\n",
    "tfifx.shape\n",
    "# Tf.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def X_describe():\n",
    "    X.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def X_dtypes():\n",
    "    X.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_sample(n):\n",
    "    df.sample(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def X_isnull():\n",
    "    X.isnull().sum()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
