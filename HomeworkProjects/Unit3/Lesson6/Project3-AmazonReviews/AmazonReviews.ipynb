{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use one of the following datasets to perform sentiment analysis on the given Amazon reviews. \n",
    "\n",
    "### Pick one of the \"small\" datasets that is a reasonable size for your computer. \n",
    "\n",
    "### The goal is to create a model to algorithmically predict if a review is positive or negative just based on its text. Try to see how these reviews compare across categories. Does a review classification model for one category work for another?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions that need answering:\n",
    "\n",
    " 1. What question are you trying to solve (or prove wrong) ?   \n",
    " __We are trying to predict the sentiment of the review.__\n",
    " 1. What kind of data do you have? -> describe the source.. \n",
    " __Review data from Amazon music,  This includes the rating, from 1-5, and the review comments.__\n",
    " 1. Do some EDA, plots  \n",
    " __Since this is sentiment analysis, we really do not have any EDA.__. \n",
    " 1. What's missing from the data and how do you deal with it?  \n",
    " __There are no missing elements from this dataset, and minimal cleanup.  __\n",
    " 1. Where are the outliers and why should we pay attention to them?  \n",
    " __We do not have any outliers here.__  \n",
    " 1. Why did you pick this model for the project?\n",
    " I picked logistic regression, a classification model, with a binary outcome, favorable or non-favorable, as that is exactly what we are trying to do here; sentiment analysis.__\n",
    " 1. How can you add, change, or remove features to get more out of your data? \n",
    " __Not a lot of things I can think of to do in a sentiment analysis model.__\n",
    "Items that could be done inclue: Feature importances, PCA, dropping low variance items, correlated feature pairs, features too highly correlated, feature engineering, transforming, timestamp and make a month, day, year column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1035,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'http://jmcauley.ucsd.edu/data/amazon/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1036,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1036-722c008479c7>, line 7)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1036-722c008479c7>\"\u001b[0;36m, line \u001b[0;32m7\u001b[0m\n\u001b[0;31m    train_size             = 0.80c\u001b[0m\n\u001b[0m                                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Constants\n",
    "max_iterations         = 10            # set it to > 0 for determining the features inportance\n",
    "random_state           = 57\n",
    "rows_in_training_set   = 10000\n",
    "rows_in_test_set       = 200000\n",
    "test_size              = 0.20\n",
    "train_size             = 0.80c\n",
    "rfc_test_size          = 50000\n",
    "rfc_train_size         = 5000\n",
    "run_CountVectorizer    = True\n",
    "run_TfidfVectorizer    = False\n",
    "BegTimeStampNewlines   = 3\n",
    "EndTimeStampNewlines   = 3\n",
    "EndTimeStamp           = '\\n'*EndTimeStampNewlines+'End'\n",
    "BegTimeStamp           = 'End'+'\\n'*BegTimeStampNewlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1037,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Controls\n",
    "flag_to_run_rfc = False\n",
    "flag_to_run_rfr = False\n",
    "flag_to_plot_them = False\n",
    "flag_to_run_correlation_matrix = False\n",
    "flag_to_run_features_importance = False\n",
    "flag_to_run_gradient_boosting  = False\n",
    "flag_to_run_linear_regression  = False\n",
    "flag_to_run_logistic_regression = False\n",
    "flag_to_run_sentiment_analyis = False\n",
    "flag_to_run_svc = False\n",
    "flag_to_run_vectorizer_nb = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1038,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import datetime\n",
    "from sklearn import ensemble\n",
    "from sklearn import datasets\n",
    "from sklearn import metrics\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time, sys\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.svm import SVC\n",
    "import statsmodels.api as sm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import linear_model\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1039,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display preferences.\n",
    "%matplotlib inline\n",
    "pd.options.display.float_format = '{:.3f}'.format\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1040,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 64,706 entries in the data file reviews_Digital_Music_5.json\n"
     ]
    }
   ],
   "source": [
    "file = 'reviews_Digital_Music_5.json'\n",
    "path = path=\"../../../../Datafiles/\"\n",
    "df = pd.read_json(path+file, lines=True)\n",
    "columns_to_skip = ['reviewerName']\n",
    "print(\"there are {:,} entries in the data file {}\".format(len(df), file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1041,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       overall                                         reviewText\n",
      "23491        4  My first acquaintance with Aimee Mann is this ...\n",
      "44060        5  This 2-disc best of collection from the Eagles...\n",
      "34995        4  Is this a great album? By all means! Is it Mil...\n",
      "60324        4  Born This Way is an odd record that blends ele...\n",
      "34659        4  This is stainds best album to date and the who...\n",
      "42940        5  In a time where most popular hip hop has gone ...\n",
      "52732        5  just keep putting them out and believe u me im...\n",
      "18480        4  Volcano is not as consistently strong as some ...\n",
      "45390        5  This album was my first introduction to Sufjan...\n",
      "39316        5  I know I'm a little late on reviewing this alb...\n"
     ]
    }
   ],
   "source": [
    "# df.columns\n",
    "print(df[['overall','reviewText']].sample(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1042,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we have cleaned up the dataframe.\n"
     ]
    }
   ],
   "source": [
    "# data Cleanup\n",
    "columns_to_cleanup = []\n",
    "for column in columns_to_cleanup:\n",
    "    print(\"we are now cleaning up column {}\".format(column))\n",
    "    df[column].fillna(-1, inplace=True)\n",
    "# columns_to_use = []\n",
    "# print(columns_to_use)\n",
    "df['message'] = df['reviewText']\n",
    "df['sentiment'] = df['overall']\n",
    "\n",
    "# convert to a numerical variable.  \n",
    "# set only the 4's and 5's to 1, which is positive, and all others to 0.\n",
    "df['sentiment_label'] = df.sentiment.map({4:1, 5: 1, 3:0, 2: 0, 1:0}) \n",
    "\n",
    "print('we have cleaned up the dataframe.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1043,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asin</th>\n",
       "      <th>helpful</th>\n",
       "      <th>overall</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>message</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>sentiment_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16095</th>\n",
       "      <td>B000002L4I</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>4</td>\n",
       "      <td>Carlene Carter first appeared in the end of th...</td>\n",
       "      <td>12 11, 2012</td>\n",
       "      <td>A3URRZ02P8KLWM</td>\n",
       "      <td>Joe Kuether</td>\n",
       "      <td>Unique character</td>\n",
       "      <td>1355184000</td>\n",
       "      <td>Carlene Carter first appeared in the end of th...</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44970</th>\n",
       "      <td>B00016XMOE</td>\n",
       "      <td>[1, 3]</td>\n",
       "      <td>4</td>\n",
       "      <td>I could want to listen the following three son...</td>\n",
       "      <td>09 7, 2004</td>\n",
       "      <td>AFV9SYG84ET9I</td>\n",
       "      <td>Nihit Saxena \"nihits\"</td>\n",
       "      <td>Only three songs</td>\n",
       "      <td>1094515200</td>\n",
       "      <td>I could want to listen the following three son...</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27652</th>\n",
       "      <td>B00000FDHE</td>\n",
       "      <td>[1, 9]</td>\n",
       "      <td>2</td>\n",
       "      <td>DJ Quik is a great producer, but this album do...</td>\n",
       "      <td>02 8, 2005</td>\n",
       "      <td>A1PULJNO1T3YVX</td>\n",
       "      <td>The Best Rap Critic</td>\n",
       "      <td>2.5 Stars</td>\n",
       "      <td>1107820800</td>\n",
       "      <td>DJ Quik is a great producer, but this album do...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52794</th>\n",
       "      <td>B000GG4XI8</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>5</td>\n",
       "      <td>This is a great CD! Now I was born in 1966, bu...</td>\n",
       "      <td>09 13, 2012</td>\n",
       "      <td>A9GYC5DZSSSLT</td>\n",
       "      <td>Amazon Customer</td>\n",
       "      <td>The time machine!</td>\n",
       "      <td>1347494400</td>\n",
       "      <td>This is a great CD! Now I was born in 1966, bu...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42728</th>\n",
       "      <td>B0000A5A0K</td>\n",
       "      <td>[1, 3]</td>\n",
       "      <td>3</td>\n",
       "      <td>Hmmm.  BT seems to have drifted right into car...</td>\n",
       "      <td>11 24, 2004</td>\n",
       "      <td>A363QRXP83K8ZE</td>\n",
       "      <td>N. P. Stathoulopoulos \"nick9155\"</td>\n",
       "      <td>Vroooooom</td>\n",
       "      <td>1101254400</td>\n",
       "      <td>Hmmm.  BT seems to have drifted right into car...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1342</th>\n",
       "      <td>B000000NN1</td>\n",
       "      <td>[14, 15]</td>\n",
       "      <td>5</td>\n",
       "      <td>It is a well-know fact that Yes-frontman, Jon ...</td>\n",
       "      <td>12 21, 2002</td>\n",
       "      <td>A1IANEBSMVGHS9</td>\n",
       "      <td>Manny Hernandez \"@askmanny\"</td>\n",
       "      <td>The music he could have recorded with Yes...</td>\n",
       "      <td>1040428800</td>\n",
       "      <td>It is a well-know fact that Yes-frontman, Jon ...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11722</th>\n",
       "      <td>B000002GF0</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>5</td>\n",
       "      <td>A great disc from the Brothers Johnson.  My fa...</td>\n",
       "      <td>12 20, 2006</td>\n",
       "      <td>A36NUDST4Y5JBA</td>\n",
       "      <td>D. S. HARDEN \"a movie fan\"</td>\n",
       "      <td>Light up the Night</td>\n",
       "      <td>1166572800</td>\n",
       "      <td>A great disc from the Brothers Johnson.  My fa...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41146</th>\n",
       "      <td>B000089RVR</td>\n",
       "      <td>[3, 3]</td>\n",
       "      <td>4</td>\n",
       "      <td>Dido's voice has this ethereal feminity to it ...</td>\n",
       "      <td>05 6, 2007</td>\n",
       "      <td>A18MBO1U4DPY20</td>\n",
       "      <td>Harkanwar Anand</td>\n",
       "      <td>Make up your heart to break it again</td>\n",
       "      <td>1178409600</td>\n",
       "      <td>Dido's voice has this ethereal feminity to it ...</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23289</th>\n",
       "      <td>B000003N94</td>\n",
       "      <td>[9, 21]</td>\n",
       "      <td>2</td>\n",
       "      <td>John Coltrane may be the most influential and ...</td>\n",
       "      <td>11 25, 2000</td>\n",
       "      <td>A6FIAB28IS79</td>\n",
       "      <td>Samuel Chell</td>\n",
       "      <td>Head for the exits.</td>\n",
       "      <td>975110400</td>\n",
       "      <td>John Coltrane may be the most influential and ...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55434</th>\n",
       "      <td>B000XXWK9I</td>\n",
       "      <td>[2, 3]</td>\n",
       "      <td>3</td>\n",
       "      <td>Rick Ross was never great but he had the poten...</td>\n",
       "      <td>03 25, 2008</td>\n",
       "      <td>A8SCX6VUTE05H</td>\n",
       "      <td>Nuisance \"the rebel\"</td>\n",
       "      <td>Better than his first album but not by much</td>\n",
       "      <td>1206403200</td>\n",
       "      <td>Rick Ross was never great but he had the poten...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             asin   helpful  overall                                         reviewText   reviewTime      reviewerID                      reviewerName                                       summary  unixReviewTime                                            message  sentiment  sentiment_label\n",
       "16095  B000002L4I    [0, 0]        4  Carlene Carter first appeared in the end of th...  12 11, 2012  A3URRZ02P8KLWM                       Joe Kuether                              Unique character      1355184000  Carlene Carter first appeared in the end of th...          4                1\n",
       "44970  B00016XMOE    [1, 3]        4  I could want to listen the following three son...   09 7, 2004   AFV9SYG84ET9I             Nihit Saxena \"nihits\"                              Only three songs      1094515200  I could want to listen the following three son...          4                1\n",
       "27652  B00000FDHE    [1, 9]        2  DJ Quik is a great producer, but this album do...   02 8, 2005  A1PULJNO1T3YVX               The Best Rap Critic                                     2.5 Stars      1107820800  DJ Quik is a great producer, but this album do...          2                0\n",
       "52794  B000GG4XI8    [0, 0]        5  This is a great CD! Now I was born in 1966, bu...  09 13, 2012   A9GYC5DZSSSLT                   Amazon Customer                             The time machine!      1347494400  This is a great CD! Now I was born in 1966, bu...          5                1\n",
       "42728  B0000A5A0K    [1, 3]        3  Hmmm.  BT seems to have drifted right into car...  11 24, 2004  A363QRXP83K8ZE  N. P. Stathoulopoulos \"nick9155\"                                     Vroooooom      1101254400  Hmmm.  BT seems to have drifted right into car...          3                0\n",
       "1342   B000000NN1  [14, 15]        5  It is a well-know fact that Yes-frontman, Jon ...  12 21, 2002  A1IANEBSMVGHS9       Manny Hernandez \"@askmanny\"  The music he could have recorded with Yes...      1040428800  It is a well-know fact that Yes-frontman, Jon ...          5                1\n",
       "11722  B000002GF0    [0, 1]        5  A great disc from the Brothers Johnson.  My fa...  12 20, 2006  A36NUDST4Y5JBA        D. S. HARDEN \"a movie fan\"                            Light up the Night      1166572800  A great disc from the Brothers Johnson.  My fa...          5                1\n",
       "41146  B000089RVR    [3, 3]        4  Dido's voice has this ethereal feminity to it ...   05 6, 2007  A18MBO1U4DPY20                   Harkanwar Anand          Make up your heart to break it again      1178409600  Dido's voice has this ethereal feminity to it ...          4                1\n",
       "23289  B000003N94   [9, 21]        2  John Coltrane may be the most influential and ...  11 25, 2000    A6FIAB28IS79                      Samuel Chell                           Head for the exits.       975110400  John Coltrane may be the most influential and ...          2                0\n",
       "55434  B000XXWK9I    [2, 3]        3  Rick Ross was never great but he had the poten...  03 25, 2008   A8SCX6VUTE05H              Nuisance \"the rebel\"   Better than his first album but not by much      1206403200  Rick Ross was never great but he had the poten...          3                0"
      ]
     },
     "execution_count": 1043,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1044,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_timestamp(displaytext):    \n",
    "    import sys\n",
    "    import datetime\n",
    "    datetime_now = str(datetime.datetime.now())\n",
    "    print(\"{:19.19}: In: {} {} \".format(datetime_now, sys._getframe(1).f_code.co_name, displaytext))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1045,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "asin                 0\n",
       "helpful              0\n",
       "overall              0\n",
       "reviewText           0\n",
       "reviewTime           0\n",
       "reviewerID           0\n",
       "reviewerName       177\n",
       "summary              0\n",
       "unixReviewTime       0\n",
       "message              0\n",
       "sentiment            0\n",
       "sentiment_label      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 1045,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1046,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 4, 3, 1, 2])"
      ]
     },
     "execution_count": 1046,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sentiment.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1047,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the current time of start is 2019-06-09 15:16:36.861644\n"
     ]
    }
   ],
   "source": [
    "# Definine outcome and predictors.\n",
    "print(\"the current time of start is {}\".format(str(datetime.datetime.now())))\n",
    "\n",
    "y = df['sentiment']\n",
    "X = df['message']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1048,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    It's hard to believe \"Memory of Trees\" came ou...\n",
       "1    A clasically-styled and introverted album, Mem...\n",
       "2    I never thought Enya would reach the sublime h...\n",
       "3    This is the third review of an irish album I w...\n",
       "4    Enya, despite being a successful recording art...\n",
       "Name: message, dtype: object"
      ]
     },
     "execution_count": 1048,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1049,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printFormatted(string):\n",
    "    display(Markdown(string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1050,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_them():\n",
    "    for column in X_train.columns:\n",
    "        plt.hist(X_train[column]*100, bins=40)\n",
    "        plt.xlabel(column)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1051,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rfc_and_feature_importances(leaf_values):    # Here we are using Gradient Boosting classifier method to determine the top 30 features.\n",
    "# train and then run RFC\n",
    "    \n",
    "    params = {'n_estimators': 500\n",
    "              ,'max_depth'    : 2\n",
    "             }\n",
    "\n",
    "    rfc = ensemble.RandomForestClassifier(**params)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=rfc_test_size, train_size=rfc_train_size)\n",
    "    \n",
    "    ## Fit the model on your training data.\n",
    "    rfc.fit(X_train, y_train) \n",
    "    \n",
    "    ## And score it on your testing data.\n",
    "    rfc.score(X_test, y_test)\n",
    "\n",
    "    feature_importance = rfc.feature_importances_\n",
    "\n",
    "    # Make importances relative to max importance.\n",
    "    feature_importance = 100.0 * (feature_importance / feature_importance.max())\n",
    "    sorted_idx = np.argsort(feature_importance)\n",
    "    cols=X.columns[sorted_idx].tolist() \n",
    "    cols=cols[::-1]\n",
    "    pos = np.arange(sorted_idx.shape[0]) + .5\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.barh(pos, feature_importance[sorted_idx], align='center')\n",
    "    plt.yticks(pos, X.columns[sorted_idx])\n",
    "    plt.xlabel('Relative Importance')\n",
    "    plt.title('Variable Importance')\n",
    "    plt.show()\n",
    "    print(\"We are returning these columns {}\".format(cols))\n",
    "    return cols # return it sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1052,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rf_and_feature_importances(rf_type, leaf_values):    # Here we are using Gradient Boosting classifier method to determine the top 30 features.\n",
    "# train and then run RFC\n",
    "    \n",
    "    params = {'n_estimators': 500\n",
    "              ,'max_depth'    : 2\n",
    "             }\n",
    "\n",
    "    if rf_type == 'classifier':\n",
    "        rf = ensemble.RandomForestClassifier(**params)\n",
    "    elif rf_type == 'regression':\n",
    "        rf = ensemble.RandomForestRegressir(**params)\n",
    "        \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=rfc_test_size, train_size=rfc_train_size)\n",
    "    \n",
    "    ## Fit the model on your training data.\n",
    "    rf.fit(X_train, y_train) \n",
    "    \n",
    "    ## And score it on your testing data.\n",
    "    rf.score(X_test, y_test)\n",
    "\n",
    "    feature_importance = rfc.feature_importances_\n",
    "\n",
    "    # Make importances relative to max importance.\n",
    "    feature_importance = 100.0 * (feature_importance / feature_importance.max())\n",
    "    sorted_idx = np.argsort(feature_importance)\n",
    "    cols=X.columns[sorted_idx].tolist() \n",
    "    cols=cols[::-1]\n",
    "    pos = np.arange(sorted_idx.shape[0]) + .5\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.barh(pos, feature_importance[sorted_idx], align='center')\n",
    "    plt.yticks(pos, X.columns[sorted_idx])\n",
    "    plt.xlabel('Relative Importance')\n",
    "    plt.title('Variable Importance')\n",
    "    plt.show()\n",
    "    print(\"We are returning these columns {}\".format(cols))\n",
    "    return cols # return it sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1053,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def run_feature_importances(type):\n",
    "# Here we will return the feature importances\n",
    "    all_feature_important_columns = []\n",
    " \n",
    "    for i in range(1,max_iterations):\n",
    "        print_timestamp('running rfc iteration {} features importance for {} times'.format(i,max_iterations))\n",
    "        columns2 = rf_and_feature_importances(type, i)\n",
    "#         columns2.extend('{}'.format(i))\n",
    "        all_feature_important_columns = all_feature_important_columns + columns2\n",
    "    #     print(\"all_feature_import_columns={}\".format(all_feature_important_columns))\n",
    "\n",
    "    print(\"\\nBOD:\\nall_feature_important_columns = {}\\nEOD\".format(sorted(all_feature_important_columns)))\n",
    "    for feature in set(all_feature_important_columns):\n",
    "        print(\"the number of occurrences of feature {} in all_feature_important_columns is {}\".format(feature, all_feature_important_columns.count(feature)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1054,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_correlation_matrix():\n",
    "    \n",
    "    print_timestamp('Begin')\n",
    "    \n",
    "    # Setup the correlation matrix.\n",
    "    corrmat = X.corr()\n",
    "    print(corrmat)\n",
    "\n",
    "    # Set up the subplots\n",
    "    f, ax = plt.subplots(figsize=(12, 9))\n",
    "\n",
    "    # Let's draw the heatmap using seaborn.\n",
    "    sns.heatmap(corrmat, vmax=.6, square=True)\n",
    "    plt.show()\n",
    "    \n",
    "    print_timestamp('End')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1055,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_size = 0.8, X_train is 51764, and y_train is 51764\n",
      "test_size  = 0.2, X_test  is 51764, and y_test is 12942\n"
     ]
    }
   ],
   "source": [
    "# Let's fit it with the RFC training set\n",
    "if (len(X) > 0):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, train_size=train_size, random_state=0)\n",
    "    print(\"train_size = {}, X_train is {}, and y_train is {}\".format(train_size, len(X_train), len(y_train)))\n",
    "    print(\"test_size  = {}, X_test  is {}, and y_test is {}\".format(test_size, len(y_train), len(y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1056,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_rf(func):\n",
    "\n",
    "    print_timestamp('Begin fit_train_rf')\n",
    "    \n",
    "    ## Fit the model on your training data.\n",
    "    func.fit(X_train, y_train) \n",
    "    \n",
    "    #Let's run cross validate score with the training data set\n",
    "    cross_val_score(func, X_train, y_train, cv=5)\n",
    "        \n",
    "    ## Let's score it with the training data set\n",
    "    fun .score(X_train, y_train)\n",
    "    \n",
    "    ## Let's score it with the test data set\n",
    "    func.score(X_test, y_test)\n",
    "    \n",
    "    print_timestamp('End run_rf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1057,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_rf(func):\n",
    "    print_timestamp('Begin run_rfr part 1')\n",
    "    \n",
    "    ## Fit the model on your training data.\n",
    "    func.fit(X_train, y_train) \n",
    "    \n",
    "    #Let's run cross validate score with the training data set\n",
    "    cross_val_score(func, X_train, y_train, cv=5)\n",
    "    \n",
    "#     ## Let's score it with the training data set\n",
    "#     rfr.score(X_train, y_train)\n",
    "    \n",
    "#     ## Let's score it with the test data set\n",
    "#     rfr.score(X_test, y_test)\n",
    "    \n",
    "    print_timestamp('End run_rfr part 1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1058,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_rf == True:\n",
    "    print_timestamp('Begin run_rfr part 2')\n",
    "    \n",
    "#     ## Fit the model on your training data.\n",
    "#     rfr.fit(X_train, y_train) \n",
    "    \n",
    "#     #Let's run cross validate score with the training data set\n",
    "#     cross_val_score(rfr, X_train, y_train, cv=5)\n",
    "    \n",
    "    ## Let's score it with the training data set\n",
    "    rfr.score(X_train, y_train)\n",
    "    \n",
    "    ## Let's score it with the test data set\n",
    "    rfr.score(X_test, y_test)\n",
    "    \n",
    "    print_timestamp('End run_rfr part 2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try predicting with gradient boosting classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1059,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_gradient_boosting():\n",
    "\n",
    "    print_timestamp('Begin')\n",
    "    \n",
    "    clf = ensemble.GradientBoostingClassifier(**params)\n",
    "\n",
    "    #Let's run cross validate score with the training data set\n",
    "    cross_val_score(clf, X_train, y_train, cv=5)\n",
    "\n",
    "    loss_function = 'deviance' # could be exponential\n",
    "    depth_value = 8\n",
    "    params = {'n_estimators': 500,\n",
    "              'max_depth': 8,\n",
    "              'loss': loss_function,\n",
    "              'max_leaf_nodes': depth_value, # 8 worked best...\n",
    "              'min_samples_leaf': depth_value * 3\n",
    "              ,'random_state' : random_state\n",
    "             }\n",
    "\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    predict_train = clf.predict(X_train)\n",
    "    predict_test = clf.predict(X_test)\n",
    "    \n",
    "    print_timestamp('End')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1060,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_svc():\n",
    "\n",
    "    print_timestamp('Begin')\n",
    "    \n",
    "    # Let's do a linear Support Vector Classifier\n",
    "    print_timestamp('Running SVC(kernel=linear')\n",
    "    svm = SVC(kernel = 'linear')\n",
    "    \n",
    "    # Let's fit the training model\n",
    "    print_timestamp('Running svm.fit')\n",
    "    svm.fit(X_train, y_train)\n",
    "    \n",
    "    # Let's score the training set\n",
    "    print_timestamp('Running svm.score for the training set')\n",
    "    svm.score(X_train, y_train)\n",
    "    \n",
    "    # Let's score the test set\n",
    "    print_timestamp('Running svm.fit for the test set')\n",
    "    svm.score(X_test, y_test)\n",
    "\n",
    "    print_timestamp('End')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1061,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_logistic_regression():\n",
    "    print_timestamp('Begin')\n",
    "\n",
    "    lr = LogisticRegression(C=1e20, solver='lbfgs', max_iter=1000)\n",
    "\n",
    "    print_timestamp('Running lr.fit for the training set')\n",
    "    lr.fit(X_train, y_train)\n",
    "    \n",
    "    print_timestamp('Running lr.fit for the training set')\n",
    "    print('\\nR-squared simple model training set yields:')\n",
    "    print(lr.score(X_train, y_train))\n",
    "    print(\"here comes the test set\")\n",
    "    print(lr.score(X_test, y_test))\n",
    "    \n",
    "    print_timestamp('End')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1062,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_linear_regression():\n",
    "\n",
    "    print_timestamp('Begin run_linear_regression')\n",
    "    \n",
    "    regr = linear_model.LinearRegression()\n",
    "\n",
    "    print_timestamp('Running regr.fit for the training set')\n",
    "    regr.fit(X_train, y_train)\n",
    "    \n",
    "    print(\"\\nCoeffecients: \\n\", regr.coef_)\n",
    "    print(\"\\nIntercept: \\n\", regr.intercept_)\n",
    "    print(\"\\nR-squared for training data set:\")\n",
    "    print(regr.score(X_train, y_train))\n",
    "    \n",
    "    print(\"\\nR-squared for test data set:\")\n",
    "    print(regr.score(X_test, y_test))\n",
    "    \n",
    "    print_timestamp('End run_linear_regression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1063,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_bnb():\n",
    "    print_timestamp('Begin run_bnb')\n",
    "\n",
    "    bnb = BernoulliNB()\n",
    "\n",
    "    print_timestamp('Running bnb.fit for the training set')\n",
    "    bnb.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = bnb.predict(d1, t1)\n",
    "    \n",
    "    print_timestamp('Running lr.fit for the training set')\n",
    "    print('\\nR-squared simple model training set yields:')\n",
    "    print(lr.score(X_train, y_train))\n",
    "    print(\"here comes the test set\")\n",
    "    print(lr.score(X_test, y_test))\n",
    "    \n",
    "    print_timestamp('End')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1064,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_Bernoulli_supervised_learning(d1, t1):\n",
    "# Our data is binary / boolean, so we're importing the Bernoulli classifier.\n",
    "\n",
    "    # Instantiate our model and store it in a new variable.\n",
    "    bnb = BernoulliNB()\n",
    "\n",
    "    # Fit our model to the data.\n",
    "#     bnb.fit(d1, target)\n",
    "    bnb.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "    # Classify, storing the result in a new variable.\n",
    "    y_pred = bnb.predict(X_train)\n",
    "\n",
    "    # Display our results.\n",
    "    return_message = \"Number of mislabeled points out of a total {} points : {}\".format(\n",
    "        d1.shape[0],\n",
    "        (t1 != y_pred).sum()\n",
    "    )\n",
    "\n",
    "    return (return_message, d1.shape[0], (t1 != y_pred).sum(), (t1 != y_pred).sum()/d1.shape[0] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1065,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1066,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_encoding(fn):\n",
    "    import chardet\n",
    "        \n",
    "    with open(fn, 'rb') as f:\n",
    "        content = f.read()\n",
    "\n",
    "    charset = chardet.detect(content)\n",
    "    # {'encoding': 'EUC-JP', 'confidence': 0.99}\n",
    "    #print(\"character set = {}\".format(charset['encoding']))\n",
    "    \n",
    "    return charset['encoding']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1067,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's process the data)\n",
    "def open_and_load_file (filename, columnnames):\n",
    "    file_encoding = get_file_encoding(filename)\n",
    "    df = pd.read_csv(filename, delimiter= '\\t', header=None, encoding=file_encoding) \n",
    "    df.columns = columnnames\n",
    "    return df, file_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1068,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_Bernoulli_supervised_learning(d1, t1):\n",
    "# Our data is binary / boolean, so we're importing the Bernoulli classifier.\n",
    "    from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "    # Instantiate our model and store it in a new variable.\n",
    "    bnb = BernoulliNB()\n",
    "\n",
    "    # Fit our model to the data.\n",
    "    bnb.fit(d1, target)\n",
    "\n",
    "    # Classify, storing the result in a new variable.\n",
    "    y_pred = bnb.predict(d1)\n",
    "\n",
    "    # Display our results.\n",
    "    return_message = \"Number of mislabeled points out of a total {} points : {}\".format(\n",
    "        d1.shape[0],\n",
    "        (t1 != y_pred).sum()\n",
    "    )\n",
    "\n",
    "    return (return_message, d1.shape[0], (t1 != y_pred).sum(), (t1 != y_pred).sum()/d1.shape[0] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1069,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_the_keywords():\n",
    "    for key in keywords:\n",
    "        # Add spaces around the key so that we are getting theword,\n",
    "        # not just pattern matching\n",
    "        sentiment_raw[str(key)] = sentiment_raw.message.str.contains(' ' + str(key) + ' ', case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1070,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_the_heatmap(subplotno):\n",
    "    plt.subplot(3,1,subplotno) \n",
    "    sns.heatmap(sentiment_raw.corr(), cmap=\"Blues\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1071,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_data_and_target():\n",
    "    global data \n",
    "    data = sentiment_raw[keywords]\n",
    "    # data.sample(5)\n",
    "    \n",
    "    global target \n",
    "    target = sentiment_raw['sentiment']\n",
    "    # target.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1072,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorizer_nb():\n",
    "\n",
    "    print_timestamp(BegTimeStamp)\n",
    "    \n",
    "    # 1. import and instantiate CountVectorizer (with the default parameters)\n",
    "\n",
    "    # 2. instantiate CountVectorizer (vectorizer)\n",
    "\n",
    "    X = df.message\n",
    "    y = df.sentiment_label\n",
    "\n",
    "    # split X and y into training and testing sets\n",
    "    # by default, it splits 75% training and 25% test\n",
    "    # random_state=1 for reproducibility\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n",
    "\n",
    "    # 3. fit\n",
    "    print(\"run_CountVectorizer={}, and run_TfidfVectorizer={}\".format(run_CountVectorizer,\n",
    "                                                                      run_TfidfVectorizer))\n",
    "    if run_CountVectorizer == True:\n",
    "        print(\"We are running with CountVectorizer\")\n",
    "        vectorizer = CountVectorizer()\n",
    "        vectorizer.fit(X_train)\n",
    "        vectorizer_method = 'CountVectorizer'\n",
    "    elif run_TfidfVectorizer == True:\n",
    "        print(\"We are running with TfidfVectorizer\")\n",
    "        vectorizer = TfidfVectorizer()\n",
    "        vectorizer.fit_transform(X_train)\n",
    "        vectorizer_method = 'TfidfVectorizer'\n",
    "    \n",
    "#     vectorizer_fit(X_train)\n",
    "\n",
    "    # 4. transform training data\n",
    "    X_train_dtm = vectorizer.transform(X_train)\n",
    "\n",
    "    # equivalently: combine fit and transform into a single step\n",
    "    # this is faster and what most people would do\n",
    "    X_train_dtm = vectorizer.fit_transform(X_train)\n",
    "\n",
    "    # 4. transform testing data (using fitted vocabulary) into a document-term matrix\n",
    "    X_test_dtm = vectorizer.transform(X_test)\n",
    "    X_test_dtm\n",
    "\n",
    "    # you can see that the number of columns, 7456, is the same as what we have learned above in X_train_dtm\n",
    "\n",
    "    # 1. import\n",
    "\n",
    "    # 2. instantiate a Multinomial Naive Bayes model\n",
    "    nb = MultinomialNB()\n",
    "\n",
    "    # 3. train the model \n",
    "    # using X_train_dtm (timing it with an IPython \"magic command\")\n",
    "\n",
    "    nb.fit(X_train_dtm, y_train)\n",
    "    \n",
    "    # 4. make class predictions for X_test_dtm\n",
    "    y_pred_class = nb.predict(X_test_dtm)\n",
    "\n",
    "    # calculate accuracy of class predictions\n",
    "\n",
    "    met_score = metrics.accuracy_score(y_test, y_pred_class)\n",
    "    printFormatted('###  With {} vectorizer, and Multinomial Naive Bayes,  the metrics accuracy score = {:.2%}'.format(vectorizer_method,\n",
    "                                                                                         met_score))\n",
    "    \n",
    "    print_timestamp(EndTimeStamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1073,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc = ensemble.RandomForestClassifier()\n",
    "rfr = ensemble.RandomForestRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1074,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-09 15:16:37: In: vectorizer_nb End\n",
      "\n",
      "\n",
      " \n",
      "run_CountVectorizer=True, and run_TfidfVectorizer=False\n",
      "We are running with CountVectorizer\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "###  With CountVectorizer vectorizer, and Multinomial Naive Bayes,  the metrics accuracy score = 82.98%"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-09 15:16:59: In: vectorizer_nb \n",
      "\n",
      "\n",
      "End \n"
     ]
    }
   ],
   "source": [
    "if flag_to_plot_them == True:\n",
    "    plot_them()\n",
    "    \n",
    "if flag_to_run_features_importance == True:\n",
    "    run_features_imporantance(rfc)\n",
    "    \n",
    "if flag_to_run_correlation_matrix == True:\n",
    "    run_correlation_matrix()\n",
    "    \n",
    "if flag_to_run_rfc == True:\n",
    "    run_rfc()\n",
    "        \n",
    "if flag_to_run_rfr == True:\n",
    "    run_rfr()\n",
    "    \n",
    "if flag_to_run_gradient_boosting  == True:\n",
    "    run_gradient_boosting()\n",
    "    \n",
    "if flag_to_run_linear_regression  == True:\n",
    "    run_linear_regression()\n",
    "    \n",
    "if flag_to_run_logistic_regression == True:\n",
    "    run_logistic_regression()\n",
    "    \n",
    "if flag_to_run_svc == True:\n",
    "    run_svc() \n",
    "    \n",
    "if flag_to_run_vectorizer_nb == True:\n",
    "    vectorizer_nb()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post Model Analysis:  \n",
    "\n",
    "#### As it turns out, the most effective model for this project was with CountVectorization, and MulitNomial Naive Bayes.  I attempted to use TfidfVectorization and MultiNomial Bayes, but it yielded a lower accuracy score of more the 1 percent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
