{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import nltk\n",
    "import spacy\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('gutenberg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Why, oh why are all of the corpuses lowercase after Removing stopwords in the lesson????"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Raw:\n",
      " [Alice's Adventures in Wonderland by Lewis Carroll 1865]\n",
      "\n",
      "CHAPTER I. Down the Rabbit-Hole\n",
      "\n",
      "Alice was\n"
     ]
    }
   ],
   "source": [
    "# import the data we just downloaded\n",
    "from nltk.corpus import gutenberg\n",
    "\n",
    "persuasion = gutenberg.raw('austen-persuasion.txt')\n",
    "alice = gutenberg.raw('carroll-alice.txt')\n",
    "\n",
    "# print the first 100 characters of Alice\n",
    "print('\\nRaw:\\n', alice[0:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Text Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title removed: \n",
      "\n",
      "CHAPTER I. Down the Rabbit-Hole\n",
      "\n",
      "Alice was beginning to get very tired of sitting by her sister on\n"
     ]
    }
   ],
   "source": [
    "# this pattern matches all text between square brackets\n",
    "pattern = \"[\\[].*?[\\]]\"\n",
    "persuasion = re.sub(pattern, \"\", persuasion)\n",
    "alice = re.sub(pattern, \"\", alice)\n",
    "\n",
    "# print the first 100 characters of Alice again\n",
    "print(\"Title removed:\", alice[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chapter headings removed from alice: \n",
      "\n",
      "\n",
      "\n",
      "Alice was beginning to get very tired of sitting by her sister on the\n",
      "bank, and of having nothing \n"
     ]
    }
   ],
   "source": [
    "# now we'll match and remove chapter headings\n",
    "persuasion = re.sub(r'Chapter \\d+', '', persuasion)\n",
    "alice = re.sub(r'CHAPTER .*', '', alice)\n",
    "\n",
    "# ok, what does it look like now\n",
    "print('Chapter headings removed from alice:', alice[0:102])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra whitespace removed:\n",
      " Alice was beginning to get very tired of sitting by her sister on the bank, and of having nothing to do\n"
     ]
    }
   ],
   "source": [
    "# remove newlines and other extra whitespaces by splitting and rejoining\n",
    "persuasion = ' '.join(persuasion.split())\n",
    "alice = ' '.join(alice.split())\n",
    "\n",
    "# almost done with cleanup?  let's check it out\n",
    "print('Extra whitespace removed:\\n', alice[0:103])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')\n",
    "\n",
    "# all the processing work is done below, sit it may take a while, Lou.\n",
    "\n",
    "alice_doc = nlp(alice)\n",
    "persuasion_doc = nlp(persuasion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the alice_doc is a <class 'spacy.tokens.doc.Doc'> object.\n",
      "It is 34408 tokens long.\n",
      "the first three tokens are: 'Alice was beginning'\n",
      "the type of each token is <class 'spacy.tokens.token.Token'>\n"
     ]
    }
   ],
   "source": [
    "# let's explore the objects that we've built...\n",
    "\n",
    "print(\"the alice_doc is a {} object.\".format(type(alice_doc)))\n",
    "print(\"It is {} tokens long.\".format(len(alice_doc)))\n",
    "print(\"the first three tokens are: '{}'\".format(alice_doc[:3]))\n",
    "print(\"the type of each token is {}\".format(type(alice_doc[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "alice_without_stopwords = [token for token in alice_doc if not token.is_stop]\n",
    "persuasion_without_stopwords = [token for token in persuasion_doc if not token.is_stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Alice, beginning, tired, sitting, sister, bank, ,, having, :, twice, peeped, book, sister, reading, ,, pictures, conversations, ,, ', use, book, ,, ', thought, Alice, ', pictures, conversation, ?, ']\n"
     ]
    }
   ],
   "source": [
    "print(alice_without_stopwords[0:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Alice [('said', 458), ('alice', 396), ('little', 126), ('like', 85), ('know', 84), ('went', 83), ('thought', 74), ('queen', 74), ('time', 71), ('king', 63)]\n",
      "Persuasion: [('anne', 496), ('captain', 303), ('mrs', 291), ('elliot', 289), ('mr', 254), ('wentworth', 217), ('lady', 216), ('good', 187), ('little', 176), ('charles', 166)]\n"
     ]
    }
   ],
   "source": [
    "# utility function to calculate how frequently words appear in the text\n",
    "def word_frequencies(text):\n",
    "    # build a list of words\n",
    "    # strip out punctuation\n",
    "    words = []\n",
    "    for token in text:\n",
    "        if not token.is_punct:\n",
    "            words.append(token.text) ## I just added lower() onto this function, and now all text items are lower!\n",
    "    \n",
    "    # build and return a Counter object containing the word counts\n",
    "    return Counter(words)\n",
    "\n",
    "# instantiate out list of most common words.\n",
    "alice_word_freq = word_frequencies(alice_without_stopwords).most_common(10)\n",
    "persuasion_word_freq = word_frequencies(persuasion_without_stopwords).most_common(10)\n",
    "print('\\nAlice', alice_word_freq)\n",
    "print('Persuasion:', persuasion_word_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Alice: [('say', 476), ('Alice', 394), ('think', 130), ('go', 130), ('little', 124), ('look', 105), ('know', 103), ('come', 96), ('like', 92), ('begin', 91)]\n",
      "Persuasion: [('Anne', 496), ('Captain', 297), ('Mrs', 291), ('Elliot', 288), ('think', 258), ('Mr', 254), ('know', 252), ('good', 222), ('Wentworth', 215), ('Lady', 191)]\n"
     ]
    }
   ],
   "source": [
    "def lemma_frequencies(text):\n",
    "    # build a list of lemas\n",
    "    # strip out punctuation\n",
    "    lemmas = []\n",
    "    for token in text:\n",
    "        if not token.is_punct:\n",
    "            lemmas.append(token.lemma_)\n",
    "            \n",
    "    # build and return a Counter object containing lemma counts\n",
    "    return Counter(lemmas)\n",
    "\n",
    "# instantiate out list of most common lemmas\n",
    "alice_lemma_freq = lemma_frequencies(alice_without_stopwords).most_common(10)\n",
    "persuasion_lemma_freq = lemma_frequencies(persuasion_without_stopwords).most_common(10)\n",
    "\n",
    "print('\\nAlice:' ,alice_lemma_freq)\n",
    "print('Persuasion:' ,persuasion_lemma_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique to Alice: {'come', 'say', 'little', 'look', 'like', 'begin', 'go', 'Alice'}\n",
      "Unique to Persuasion: {'Elliot', 'Wentworth', 'Lady', 'Captain', 'Mr', 'good', 'Mrs', 'Anne'}\n"
     ]
    }
   ],
   "source": [
    "alice_lemma_common = [pair[0] for pair in alice_lemma_freq]\n",
    "persuasion_lemma_common = [pair[0] for pair in persuasion_lemma_freq]\n",
    "\n",
    "print('Unique to Alice:', set(alice_lemma_common) - set(persuasion_lemma_common))\n",
    "print('Unique to Persuasion:', set(persuasion_lemma_common) - set(alice_lemma_common))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alice in Wonderland has 1989 sentences.\n",
      "Here is an example: \n",
      "There was nothing so VERY remarkable in that; nor did Alice think it so VERY much out of the way to hear the Rabbit say to itself, 'Oh dear!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# initial exploration of sentences\n",
    "sentences = list(alice_doc.sents)\n",
    "print(\"Alice in Wonderland has {} sentences.\".format(len(sentences)))\n",
    "\n",
    "example_sentence = sentences[2]\n",
    "print(\"Here is an example: \\n{}\\n\".format(example_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this sentence is [There, was, nothing, so, VERY, remarkable, in, that, nor, did, Alice, think, it, so, VERY, much, out, of, the, way, to, hear, the, Rabbit, say, to, itself, Oh, dear]\n",
      "There are 29 words in this sentence, and 25 of them are unique\n"
     ]
    }
   ],
   "source": [
    "# look at some metrics around this sentence\n",
    "\n",
    "example_words = [token for token in example_sentence if not token.is_punct]\n",
    "unique_words = set([token.text for token in example_words])\n",
    "\n",
    "print(\"this sentence is {}\".format(example_words))\n",
    "print((\"There are {} words in this sentence, and {} of them are unique\".format(len(example_words), len(unique_words))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
