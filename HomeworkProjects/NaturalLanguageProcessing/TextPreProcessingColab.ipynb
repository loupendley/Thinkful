{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "TextPreProcessing.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/loupendley/Thinkful/blob/master/HomeworkProjects/NaturalLanguageProcessing/TextPreProcessingColab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90Hch0kNdJGB",
        "colab_type": "text"
      },
      "source": [
        "### **PreProcessing for Cornell Movie Dialogs Corpus Data**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VsvVOTGcA2JF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import Counter\n",
        "import nltk\n",
        "import spacy\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FDuW7m3TdghL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def file_stuff(path, filename):\n",
        "    \n",
        "    # zip_file = ZipFile(zipfil\n",
        "    print(\"path = {}, and filename = {}\".format(path, filename))\n",
        "    fullfilename = \"{}\".format(path+'/'+filename)\n",
        "    print(\"fullfilename = {}\".format(path+'/'+filename))\n",
        "    df = pd.read_json(fullfilename,encoding = \"utf-8\")\n",
        "    # df = pd.read_csv(zip_file.open(filename), dtype=dtypes, usecols=usecols)\n",
        "\n",
        "    print(\"There are {} rows in this file.\".format(df.shape[0]))\n",
        "\n",
        "    # patients_df = pd.read_json('E:/datasets/patients.json')\n",
        "    \n",
        "    return df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6VfL_elj9sa3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "outputId": "efa66fe8-1cd9-47ea-fd1c-ea9ea65cf35d"
      },
      "source": [
        "# import os\n",
        "# path = os.getcwd()\n",
        "\n",
        "# print(path)\n",
        "\n",
        "# arr = os.listdir('.')\n",
        "# print(arr)\n",
        "# dtypes = 'string'\n",
        "# usecols = 'dialogs'\n",
        "dialogs = file_stuff(\".\", 'dialogs.json')\n",
        "dialogs.dialogs.head(20)\n",
        "# print(\"dialogs.dialogs is a {} datatype\".format(type(dialogs.dialogs[0])))"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "path = ., and filename = dialogs.json\n",
            "fullfilename = ./dialogs.json\n",
            "There are 304446 rows in this file.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0     Can we make this quick?  Roxanne Korrine and A...\n",
              "1     Well, I thought we'd start with pronunciation,...\n",
              "2     Not the hacking and gagging and spitting part....\n",
              "3     Okay... then how 'bout we try out some French ...\n",
              "4     You're asking me out.  That's so cute. What's ...\n",
              "5                                            Forget it.\n",
              "6     No, no, it's my fault -- we didn't have a prop...\n",
              "7                                              Cameron.\n",
              "8     The thing is, Cameron -- I'm at the mercy of a...\n",
              "9        Seems like she could get a date easy enough...\n",
              "10                                                 Why?\n",
              "11    Unsolved mystery.  She used to be really popul...\n",
              "12                                      That's a shame.\n",
              "13       Gosh, if only we could find Kat a boyfriend...\n",
              "14                            Let me see what I can do.\n",
              "15                       C'esc ma tete. This is my head\n",
              "16             Right.  See?  You're ready for the quiz.\n",
              "17    I don't want to know how to say that though.  ...\n",
              "18                 That's because it's such a nice one.\n",
              "19                                       Forget French.\n",
              "Name: dialogs, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nJSHjeHVA2JJ",
        "colab_type": "code",
        "outputId": "1b150235-cbd1-49ae-9ac8-4a504d9cb7c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# nltk.download('gutenberg')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/gutenberg.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bVp9ZUCEA2JL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Why, oh why are all of the corpuses lowercase after Removing stopwords in the lesson????"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ZGR5rCgA2JN",
        "colab_type": "code",
        "outputId": "09185e80-f237-49a9-f4cb-1504bae74773",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "# # import the data we just downloaded\n",
        "# from nltk.corpus import gutenberg\n",
        "\n",
        "# persuasion = gutenberg.raw('austen-persuasion.txt')\n",
        "# alice = gutenberg.raw('carroll-alice.txt')\n",
        "\n",
        "# # print the first 100 characters of Alice\n",
        "# print('\\nRaw:\\n', alice[0:100])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Raw:\n",
            " [Alice's Adventures in Wonderland by Lewis Carroll 1865]\n",
            "\n",
            "CHAPTER I. Down the Rabbit-Hole\n",
            "\n",
            "Alice was\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iFiskT1oA2JQ",
        "colab_type": "text"
      },
      "source": [
        "## Basic Text Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fBlqm58YA2JR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # this pattern matches all text between square brackets\n",
        "# pattern = \"[\\[].*?[\\]]\"\n",
        "# persuasion = re.sub(pattern, \"\", persuasion)\n",
        "# alice = re.sub(pattern, \"\", alice)\n",
        "\n",
        "# # print the first 100 characters of Alice again\n",
        "# print(\"Title removed:\", alice[0:100])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SZPrrKW7A2JT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # now we'll match and remove chapter headings\n",
        "# persuasion = re.sub(r'Chapter \\d+', '', persuasion)\n",
        "# alice = re.sub(r'CHAPTER .*', '', alice)\n",
        "\n",
        "# # ok, what does it look like now\n",
        "# print('Chapter headings removed from alice:', alice[0:102])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0tlemc6NA2JW",
        "colab_type": "code",
        "outputId": "e091a8c5-bb2a-4ef6-a34e-7b57a4bef51a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# remove newlines and other extra whitespaces by splitting and rejoining\n",
        "# persuasion = ' '.join(persuasion.split())\n",
        "# alice = ' '.join(alice.split())\n",
        "dialogs3 = ' '.join(dialogs.dialogs.to_string())\n",
        "\n",
        "# # almost done with cleanup?  let's check it out\n",
        "# print('Extra whitespace removed:\\n', alice[0:103])\n",
        "# print(\"dialogs.dialogs[0] = {}\".format(dialogs.dialogs[0]))\n",
        "# print(\"type of dialogs.dialogs[0] is {}\".format(type(dialogs.dialogs[0])))\n",
        "print(\"length of dialogs3 is {}\".format(len(dialogs3)))\n",
        "# print(\"dialogs3 sample = {}\".format(dialogs3))\n",
        "print(\"dialogs3 is a {} datatype.\".format(type(dialogs3)))"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "length of dialogs3 is 36533517\n",
            "dialogs3 is a <class 'str'> datatype.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3HkLU2i_A2JZ",
        "colab_type": "text"
      },
      "source": [
        "## Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z1fRWsZcA2JZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 354
        },
        "outputId": "4b92ead0-864d-413b-ee36-aaac979c5b82"
      },
      "source": [
        "nlp = spacy.load('en')\n",
        "\n",
        "# all the processing work is done below, sit it may take a while, Lou.\n",
        "\n",
        "# alice_doc = nlp(alice)\n",
        "# persuasion_doc = nlp(persuasion)\n",
        "\n",
        "dialogs = nlp(dialogs3)\n",
        "print(\"length of dialogs_dialogs = {}\".format(len(dialogs)))"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-55-278a2cf739f2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# persuasion_doc = nlp(persuasion)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mdialogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdialogs3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"length of dialogs_dialogs = {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdialogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/spacy/language.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[1;32m    427\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m             raise ValueError(\n\u001b[0;32m--> 429\u001b[0;31m                 \u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE088\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    430\u001b[0m             )\n\u001b[1;32m    431\u001b[0m         \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_doc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: [E088] Text of length 36533517 exceeds maximum of 1000000. The v2.x parser and NER models require roughly 1GB of temporary memory per 100,000 characters in the input. This means long texts may cause memory allocation errors. If you're not using the parser or NER, it's probably safe to increase the `nlp.max_length` limit. The limit is in number of characters, so you can check whether your inputs are too long by checking `len(text)`."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-x2BmyvyA2Jc",
        "colab_type": "code",
        "outputId": "ee39d38d-5a10-4273-e553-122d2b7bbb6e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "# let's explore the objects that we've built...\n",
        "\n",
        "print(\"the alice_doc is a {} object.\".format(type(alice_doc)))\n",
        "print(\"It is {} tokens long.\".format(len(alice_doc)))\n",
        "print(\"the first three tokens are: '{}'\".format(alice_doc[:3]))\n",
        "print(\"the type of each token is {}\".format(type(alice_doc[0])))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "the alice_doc is a <class 'spacy.tokens.doc.Doc'> object.\n",
            "It is 34408 tokens long.\n",
            "the first three tokens are: 'Alice was beginning'\n",
            "the type of each token is <class 'spacy.tokens.token.Token'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-RUE-s7A2Je",
        "colab_type": "text"
      },
      "source": [
        "## Removing stopwords"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JH5bPzI1A2Je",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "alice_without_stopwords = [token for token in alice_doc if not token.is_stop]\n",
        "persuasion_without_stopwords = [token for token in persuasion_doc if not token.is_stop]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9G702kWCA2Jg",
        "colab_type": "code",
        "outputId": "0b802b4e-34b2-4edd-8913-07377775b1e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print(alice_without_stopwords[0:30])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Alice, beginning, tired, sitting, sister, bank, ,, having, :, twice, peeped, book, sister, reading, ,, pictures, conversations, ,, ', use, book, ,, ', thought, Alice, ', pictures, conversation, ?, ']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_HHGp32NA2Jj",
        "colab_type": "code",
        "outputId": "a4880fca-8394-4161-ff41-dde43f0f57f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "# utility function to calculate how frequently words appear in the text\n",
        "def word_frequencies(text):\n",
        "    # build a list of words\n",
        "    # strip out punctuation\n",
        "    words = []\n",
        "    for token in text:\n",
        "        if not token.is_punct:\n",
        "            words.append(token.text) ## I just added lower() onto this function, and now all text items are lower!\n",
        "    \n",
        "    # build and return a Counter object containing the word counts\n",
        "    return Counter(words)\n",
        "\n",
        "# instantiate out list of most common words.\n",
        "alice_word_freq = word_frequencies(alice_without_stopwords).most_common(10)\n",
        "persuasion_word_freq = word_frequencies(persuasion_without_stopwords).most_common(10)\n",
        "print('\\nAlice', alice_word_freq)\n",
        "print('Persuasion:', persuasion_word_freq)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Alice [('said', 453), ('Alice', 394), ('little', 124), ('like', 84), ('went', 83), ('know', 83), ('thought', 74), ('Queen', 73), ('time', 68), ('King', 61)]\n",
            "Persuasion: [('Anne', 496), ('Captain', 297), ('Mrs', 291), ('Elliot', 288), ('Mr', 254), ('Wentworth', 217), ('Lady', 191), ('good', 181), ('little', 175), ('Charles', 166)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZLu7gMhHA2Jm",
        "colab_type": "text"
      },
      "source": [
        "## Lemmatization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xMXLfjFoA2Jm",
        "colab_type": "code",
        "outputId": "67f0831e-5e3f-474f-f3ef-eca0b2dfb7b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "def lemma_frequencies(text):\n",
        "    # build a list of lemas\n",
        "    # strip out punctuation\n",
        "    lemmas = []\n",
        "    for token in text:\n",
        "        if not token.is_punct:\n",
        "            lemmas.append(token.lemma_)\n",
        "            \n",
        "    # build and return a Counter object containing lemma counts\n",
        "    return Counter(lemmas)\n",
        "\n",
        "# instantiate out list of most common lemmas\n",
        "alice_lemma_freq = lemma_frequencies(alice_without_stopwords).most_common(10)\n",
        "persuasion_lemma_freq = lemma_frequencies(persuasion_without_stopwords).most_common(10)\n",
        "\n",
        "print('\\nAlice:' ,alice_lemma_freq)\n",
        "print('Persuasion:' ,persuasion_lemma_freq)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Alice: [('say', 476), ('Alice', 394), ('think', 130), ('go', 130), ('little', 124), ('look', 105), ('know', 103), ('come', 96), ('like', 92), ('begin', 91)]\n",
            "Persuasion: [('Anne', 496), ('Captain', 297), ('Mrs', 291), ('Elliot', 288), ('think', 258), ('Mr', 254), ('know', 252), ('good', 222), ('Wentworth', 215), ('Lady', 191)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m4x1YR7rA2Jo",
        "colab_type": "code",
        "outputId": "bf7d9b30-f346-460d-af65-a8390d6ed9bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "alice_lemma_common = [pair[0] for pair in alice_lemma_freq]\n",
        "persuasion_lemma_common = [pair[0] for pair in persuasion_lemma_freq]\n",
        "\n",
        "print('Unique to Alice:', set(alice_lemma_common) - set(persuasion_lemma_common))\n",
        "print('Unique to Persuasion:', set(persuasion_lemma_common) - set(alice_lemma_common))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Unique to Alice: {'Alice', 'little', 'say', 'like', 'look', 'come', 'go', 'begin'}\n",
            "Unique to Persuasion: {'Lady', 'Elliot', 'Mrs', 'good', 'Mr', 'Wentworth', 'Captain', 'Anne'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rpiYzg7VA2Jr",
        "colab_type": "text"
      },
      "source": [
        "## Sentences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eXLH9MnUA2Jr",
        "colab_type": "code",
        "outputId": "84a63268-08b0-4949-f48e-231468782669",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "# initial exploration of sentences\n",
        "sentences = list(alice_doc.sents)\n",
        "print(\"Alice in Wonderland has {} sentences.\".format(len(sentences)))\n",
        "\n",
        "example_sentence = sentences[2]\n",
        "print(\"Here is an example: \\n{}\\n\".format(example_sentence))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Alice in Wonderland has 1989 sentences.\n",
            "Here is an example: \n",
            "There was nothing so VERY remarkable in that; nor did Alice think it so VERY much out of the way to hear the Rabbit say to itself, 'Oh dear!\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ppXfEeNA2Jv",
        "colab_type": "code",
        "outputId": "d9bf2965-66ba-4a63-de2d-c3e1a18bb48d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "# look at some metrics around this sentence\n",
        "\n",
        "example_words = [token for token in example_sentence if not token.is_punct]\n",
        "unique_words = set([token.text for token in example_words])\n",
        "\n",
        "print(\"this sentence is {}\".format(example_words))\n",
        "print((\"There are {} words in this sentence, and {} of them are unique\".format(len(example_words), len(unique_words))))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "this sentence is [There, was, nothing, so, VERY, remarkable, in, that, nor, did, Alice, think, it, so, VERY, much, out, of, the, way, to, hear, the, Rabbit, say, to, itself, Oh, dear]\n",
            "There are 29 words in this sentence, and 25 of them are unique\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xmzLAf2jA2Jx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}