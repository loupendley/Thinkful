{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "TextPreProcessing.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/loupendley/Thinkful/blob/master/HomeworkProjects/NaturalLanguageProcessing/TextPreProcessingColab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90Hch0kNdJGB",
        "colab_type": "text"
      },
      "source": [
        "### **PreProcessing for Cornell Movie Dialogs Corpus Data**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VsvVOTGcA2JF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import Counter\n",
        "import nltk\n",
        "import spacy\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FDuW7m3TdghL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def file_stuff(path, filename, usecols=None, dtypes=None, nrows=None):\n",
        "    \n",
        "    # zip_file = ZipFile(zipfil\n",
        "    print(\"path = {}, and filename = {}\".format(path, filename))\n",
        "    fullfilename = \"{}\".format(path+'/'+filename)\n",
        "    print(\"fullfilename = {}\".format(path+'/'+filename))\n",
        "    # df = pd.read_json(fullfilename,encoding = \"utf-8\")\n",
        "    df = pd.read_csv(filename, dtype=dtypes, usecols=usecols, nrows=nrows, header=0, sep='|')\n",
        "\n",
        "    print(\"There are {} rows in this file.\".format(df.shape[0]))\n",
        "\n",
        "    # patients_df = pd.read_json('E:/datasets/patients.json')\n",
        "    \n",
        "    return df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6VfL_elj9sa3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 531
        },
        "outputId": "339d32fd-c43d-40a0-d9d5-2c11d7e77506"
      },
      "source": [
        "import os\n",
        "path = os.getcwd()\n",
        "\n",
        "print(path)\n",
        "\n",
        "arr = os.listdir('.')\n",
        "print(arr)\n",
        "# dtypes = 'string'\n",
        "# usecols = 'dialogs'\n",
        "usecols = ['index', 'dialogs']\n",
        "dialogs = file_stuff(\".\", 'dialogs_pipe_delimited.csv', usecols=usecols, nrows=1000)\n",
        "dialogs.dialogs.head(20)\n",
        "# print(\"dialogs.dialogs is a {} datatype\".format(type(dialogs.dialogs[0])))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "['.config', 'sample_data']\n",
            "path = ., and filename = dialogs_pipe_delimited.csv\n",
            "fullfilename = ./dialogs_pipe_delimited.csv\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-3f719c857adf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# usecols = 'dialogs'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0musecols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'index'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'dialogs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mdialogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile_stuff\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'dialogs_pipe_delimited.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0musecols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mdialogs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdialogs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# print(\"dialogs.dialogs is a {} datatype\".format(type(dialogs.dialogs[0])))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-e395024e81be>\u001b[0m in \u001b[0;36mfile_stuff\u001b[0;34m(path, filename, usecols, dtypes, nrows)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"fullfilename = {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m# df = pd.read_json(fullfilename,encoding = \"utf-8\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtypes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0musecols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'|'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"There are {} rows in this file.\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    674\u001b[0m         )\n\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 448\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    878\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1112\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1113\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1114\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1115\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1116\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1889\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1891\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1892\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1893\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File dialogs_pipe_delimited.csv does not exist: 'dialogs_pipe_delimited.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nJSHjeHVA2JJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# nltk.download('gutenberg')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bVp9ZUCEA2JL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Why, oh why are all of the corpuses lowercase after Removing stopwords in the lesson????"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ZGR5rCgA2JN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # import the data we just downloaded\n",
        "# from nltk.corpus import gutenberg\n",
        "\n",
        "# persuasion = gutenberg.raw('austen-persuasion.txt')\n",
        "# alice = gutenberg.raw('carroll-alice.txt')\n",
        "\n",
        "# # print the first 100 characters of Alice\n",
        "# print('\\nRaw:\\n', alice[0:100])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iFiskT1oA2JQ",
        "colab_type": "text"
      },
      "source": [
        "## Basic Text Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fBlqm58YA2JR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # this pattern matches all text between square brackets\n",
        "# pattern = \"[\\[].*?[\\]]\"\n",
        "# persuasion = re.sub(pattern, \"\", persuasion)\n",
        "# alice = re.sub(pattern, \"\", alice)\n",
        "\n",
        "# # print the first 100 characters of Alice again\n",
        "# print(\"Title removed:\", alice[0:100])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SZPrrKW7A2JT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # now we'll match and remove chapter headings\n",
        "# persuasion = re.sub(r'Chapter \\d+', '', persuasion)\n",
        "# alice = re.sub(r'CHAPTER .*', '', alice)\n",
        "\n",
        "# # ok, what does it look like now\n",
        "# print('Chapter headings removed from alice:', alice[0:102])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0tlemc6NA2JW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# remove newlines and other extra whitespaces by splitting and rejoining\n",
        "# persuasion = ' '.join(persuasion.split())\n",
        "# alice = ' '.join(alice.split())\n",
        "dialogs3 = ' '.join(dialogs.dialogs.to_string())\n",
        "\n",
        "# # almost done with cleanup?  let's check it out\n",
        "# print('Extra whitespace removed:\\n', alice[0:103])\n",
        "# print(\"dialogs.dialogs[0] = {}\".format(dialogs.dialogs[0]))\n",
        "# print(\"type of dialogs.dialogs[0] is {}\".format(type(dialogs.dialogs[0])))\n",
        "print(\"length of dialogs3 is {}\".format(len(dialogs3)))\n",
        "# print(\"dialogs3 sample = {}\".format(dialogs3))\n",
        "print(\"dialogs3 is a {} datatype.\".format(type(dialogs3)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3HkLU2i_A2JZ",
        "colab_type": "text"
      },
      "source": [
        "## Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z1fRWsZcA2JZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nlp = spacy.load('en', disable=['parser', 'ner'])\n",
        "nlp.add_pipe(nlp.create_pipe('sentencizer'))\n",
        "nlp.max_length = 20000000\n",
        "\n",
        "# all the processing work is done below, sit it may take a while, Lou.\n",
        "\n",
        "# alice_doc = nlp(alice)\n",
        "# persuasion_doc = nlp(persuasion)\n",
        "\n",
        "dialogs_doc = nlp(dialogs3)\n",
        "print(\"length of dialogs_dialogs = {}\".format(len(dialogs)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-x2BmyvyA2Jc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# let's explore the objects that we've built...\n",
        "\n",
        "print(\"the dialogs_doc is a {} object.\".format(type(dialogs_doc)))\n",
        "print(\"It is {} tokens long.\".format(len(dialogs_doc)))\n",
        "print(\"the first three tokens are: '{}'\".format(dialogs_doc[300]))\n",
        "print(\"the type of each token is {}\".format(type(dialogs_doc[0])))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-RUE-s7A2Je",
        "colab_type": "text"
      },
      "source": [
        "## Removing stopwords"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JH5bPzI1A2Je",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# alice_without_stopwords = [token for token in alice_doc if not token.is_stop]\n",
        "# persuasion_without_stopwords = [token for token in persuasion_doc if not token.is_stop]\n",
        "dialogs_doc_without_stopwords = [token for token in dialogs_doc if not token.is_stop]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9G702kWCA2Jg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(dialogs_doc_without_stopwords[0:30])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_HHGp32NA2Jj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# utility function to calculate how frequently words appear in the text\n",
        "def word_frequencies(text):\n",
        "    # build a list of words\n",
        "    # strip out punctuation\n",
        "    words = []\n",
        "    for token in text:\n",
        "        if not token.is_punct:\n",
        "            words.append(token.text) ## I just added lower() onto this function, and now all text items are lower!\n",
        "    \n",
        "    # build and return a Counter object containing the word counts\n",
        "    return Counter(words)\n",
        "\n",
        "# instantiate out list of most common words.\n",
        "alice_word_freq = word_frequencies(alice_without_stopwords).most_common(10)\n",
        "persuasion_word_freq = word_frequencies(persuasion_without_stopwords).most_common(10)\n",
        "print('\\nAlice', alice_word_freq)\n",
        "print('Persuasion:', persuasion_word_freq)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZLu7gMhHA2Jm",
        "colab_type": "text"
      },
      "source": [
        "## Lemmatization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xMXLfjFoA2Jm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def lemma_frequencies(text):\n",
        "    # build a list of lemas\n",
        "    # strip out punctuation\n",
        "    lemmas = []\n",
        "    for token in text:\n",
        "        if not token.is_punct:\n",
        "            lemmas.append(token.lemma_)\n",
        "            \n",
        "    # build and return a Counter object containing lemma counts\n",
        "    return Counter(lemmas)\n",
        "\n",
        "# instantiate out list of most common lemmas\n",
        "alice_lemma_freq = lemma_frequencies(alice_without_stopwords).most_common(10)\n",
        "persuasion_lemma_freq = lemma_frequencies(persuasion_without_stopwords).most_common(10)\n",
        "\n",
        "print('\\nAlice:' ,alice_lemma_freq)\n",
        "print('Persuasion:' ,persuasion_lemma_freq)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m4x1YR7rA2Jo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "alice_lemma_common = [pair[0] for pair in alice_lemma_freq]\n",
        "persuasion_lemma_common = [pair[0] for pair in persuasion_lemma_freq]\n",
        "\n",
        "print('Unique to Alice:', set(alice_lemma_common) - set(persuasion_lemma_common))\n",
        "print('Unique to Persuasion:', set(persuasion_lemma_common) - set(alice_lemma_common))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rpiYzg7VA2Jr",
        "colab_type": "text"
      },
      "source": [
        "## Sentences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eXLH9MnUA2Jr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# initial exploration of sentences\n",
        "sentences = list(alice_doc.sents)\n",
        "print(\"Alice in Wonderland has {} sentences.\".format(len(sentences)))\n",
        "\n",
        "example_sentence = sentences[2]\n",
        "print(\"Here is an example: \\n{}\\n\".format(example_sentence))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ppXfEeNA2Jv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# look at some metrics around this sentence\n",
        "\n",
        "example_words = [token for token in example_sentence if not token.is_punct]\n",
        "unique_words = set([token.text for token in example_words])\n",
        "\n",
        "print(\"this sentence is {}\".format(example_words))\n",
        "print((\"There are {} words in this sentence, and {} of them are unique\".format(len(example_words), len(unique_words))))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xmzLAf2jA2Jx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}